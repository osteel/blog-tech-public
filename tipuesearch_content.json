{"pages":[{"title":"About","text":"Demat ! I'm Yannick, a senior backend developer based in Brighton, UK. I hope you enjoy my blog! You can also find me on the following platforms: GitHub Twitter LinkedIn Instagram Want to get in touch? Drop me a line","tags":"pages","url":"https://tech.osteel.me/pages/about"},{"title":"Vim is much cooler than you think","text":"Source I've been using Vim for 5 years... Mainly because I don't know how to exit it If you were to ask people about their experience of Vim, this is what their answer would look like – a joke about quitting the program, and a sigh of relief when Nano is present on the system. Besides the fact that Nano is at least equally weird to exit to me, Vim's esoteric reputation is so widespread that the public mostly stays away from it, largely unaware of its potential. Programmers, in particular, are missing out. I am not advocating for the use of Vim as a full-fledged IDE here. I am talking about bringing Vim to your favourite code editor, as an extra layer for which there's probably an extension already. VSCode , Sublime Text , PHPStorm and Atom all have Vim integrations, for instance. If you use a different program, open its extension manager now and search for \"Vim\" – I'm sure you'll get some results. The point is that you don't have to learn a new editor from scratch – you can keep your favourite software and sprinkle it with some Vim magic. Vim doesn't have to replace the way you interact with text entirely, either. In most cases, developers will be able to incorporate Vim commands into their coding practice progressively, alongside native ones. They will have the option to keep what they like and leave out the rest. TL;DR Vim doesn't have to be an obscure, elitist text editor; It's got plenty of features that can greatly speed up your coding; Most modern IDEs and code editors have decent Vim integrations; This post intends to show you the ropes and convince you to give it a try. Contents TL;DR Contents Vim or Vi? Normal mode vs insert mode Moving around Copying, cutting and pasting Editing Searching, saving and exiting Undoing and redoing Combining Vim shortcuts with native features Conclusion Resources Vim or Vi? Let me start with a little bit of disambiguation. You may have come across both Vim and Vi on your developer journey, and be unsure as to what the difference is. Simply put, Vi is a standard, and Vim an implementation of that standard. In fact, Vim is the standard's most popular implementation, coming with most Linux distributions. This is also why the vast majority of IDE extensions will refer to Vim instead of Vi. Normal mode vs insert mode This is usually the first thing that trips up new Vim users – the fact that the insert mode is not the default mode. They will place the cursor where they want to make changes, start to type some text, and then... Nothing. Worse yet, they accidentally trigger a Vim command that makes the text behave seemingly randomly, producing weird outcomes which they promptly try to revert. When they make things worse instead, they attempt to prevent further damage by closing the program but can't seem to find a way out, wondering how the hell they ended up trapped in this vicious escape game. From the \"Saw\" film franchise The most tenacious individuals will eventually figure out that they can make changes by pressing i , and then save their work and get out of there unharmed by pressing the escape key followed by :wq . The vast majority of people won't push their exploration further, confident that this is all the knowledge they need. And as far as quickly editing a file on a server goes, they're not wrong. But Vim can be used for much more than this. To get to that conclusion though, we need to explore some of its many features, starting with the different ways at our disposal to enter the insert mode. Here's the first one, which we've mentioned already: pressing i (for insert ) will allow the user to makes changes at the current position: To be more specific, it will place the cursor before the current character. If you wanted to place the cursor after instead, you'd go for a (for append ): You can also insert some text at the very beginning of a line with I (capital i): Or at the end of a line with A (capital a): You can also create a new line below the current one by pressing o (for open , although that one feels a bit like a stretch, but whatever): And if you want to add a line above instead, that's O (capital o): That's it for the basics of entering the insert mode. As mentioned earlier, press the escape key to return to the normal mode. While you may not be super impressed yet, in the long run the repeated use of these shortcuts can make for an improved user experience already. That said, most of them are only useful once the cursor is already close to where you want to make changes, which is not necessarily the case when you've just opened the file. Thankfully, Vim has a few ways to get us where we need to be – fast. Moving around Vim is first and foremost the art of moving around without using the mouse. That doesn't mean you shouldn't use the mouse – I use it all the time. That means you can not use the mouse if you don't want to. Now if you've been doing this for a while, you've probably figured out a few ways besides the arrow keys to move around the code, like shifting the cursor's position word by word instead of character by character, or getting quickly to the beginning or the end of a line, or that of an entire file. Let's see how we can do the same with Vim. The first thing to remember is that you need to be in normal mode to use the following commands. Next, you might have heard that you're supposed to use the h , j , k , and l keys to move left, up, down, and right respectively. I don't. In practice, I find these shortcuts extremely inconvenient, so I just use the arrow keys instead. No hard feelings. With that out of the way, here is how you can move the cursor word by word using Vim – by pressing w (for word ): Note that it will get you to the beginning of the following word. If you wish to get to the end instead, press e (for end ): To go backwards, press b : To go to the very beginning of the line (the hard beginning), press 0 : To go to the end, press $ (which appears as the shift key followed by the number 4 on the gif – sorry about that): To get to the top of a file, press gg : And to the last line, press G (capital g): That should cover what you can normally do with a regular editor – let's now see how we can take things further with Vim. I demonstrated earlier how to go to the hard beginning of a line (with 0 ), which implies the existence of a soft beginning. The soft beginning is the first non-blank character of a line, to which you can get by pressing &#94; (which appears as the shift key followed by the number 6 on the gif): The &#94; and $ characters are likely familiar – they usually mark the beginning and end of a regular expression. It is also possible to go to a specific line number, which is useful to find where an exception was thrown, for instance. This is achieved by pressing #G , where # is the line number: Another couple of Vim shortcuts I use all the time are f and t . The former stands for find and allows us to place the cursor on the first matching character: t (for till ) does the same thing, but places the cursor before the first matching character instead: They also have their backwards counterparts – F (capital f) and T (capital t) – which I invite you to try by yourself. There are many other available shortcuts but these will do for this article. What I'd like to draw your attention to now is the fact that most of these commands can be combined with a number, the same way we associated G with a number to get to a specific line earlier. You can for instance skip four words by typing 4e (remember that e gets you to the end of a word): This also works for moving the cursor to the nth matching character. You could for instance type 3F, to move backwards to the third comma: Try this with some of the other shortcuts – it's pretty cool. This is just a glimpse of the way Vim allows us to combine different commands to achieve specific outcomes. Some might look hard to pick up at first, but with a little bit of practice, they all come naturally. Knowing how to move around with Vim is extremely important because it is the key to performing complex edits. But this is also useful for more common operations – let's see how. Copying, cutting and pasting When it comes to copying and cutting, Vim has its own clipboard, separate from the operating system's. It's called the unnamed register and allows for some advanced operations that I won't get into here (mostly because I don't know how to use them myself). In other words, if you copy a value the regular way (with cmd+c on a Mac, for instance) and then copy another value using the Vim command, the latter doesn't overwrite the former – they're stored in different places. So how do we copy stuff? First, make sure you're in normal mode. Then, select the bit of text you're interested in. Finally, press y to copy the text into the register. That's it. The y command stands for yanking . Now I don't know why they called it that, but I find it kind of funny. Anyway, once you've done that place the cursor where you want to paste the text and press p : That will paste the text after the cursor. If you'd rather paste it before, use P (capital p): To paste over some text instead, select it first and then press p : How about cutting some text? Select the portion of interest, then press d (for delete ): That's right, there's no difference between cutting and deleting, so long as you don't paste the corresponding text. If that's what you want to do though, the same shortcuts apply. For instance p : Now the above is great but doesn't bring anything new to the table. Then again, things get more interesting when we start combining commands. I won't always show examples for both yanking and deleting below – you can assume that what works for one also works for the other. First, we can copy or delete/cut an entire line by pressing the corresponding key twice. E.g. to copy the entire line, press yy (and then p to paste below): To paste above instead, press P (capital p – same as before): How about copying or deleting multiple lines? Just squeeze a number in there – e.g. to delete two lines, type d2d : Want to delete until a specific line number? That's d#G , where # is the line number: This is another way to use the G shortcut we've seen earlier, in conjunction with yet another command. Now, remember the f , F , t and T commands to place the cursor at or before a specific character forwards and backwards? They work with copying and cutting, too. Here, we copy up to and including the full stop, and paste before the cursor with P : But that's not all. One of my favourite features is the ability to yank and delete some text inside boundaries. Say we want to delete a section that's between double quotes. We can do so with di\" (the double quotes appear as the shift key followed by a single quote on the gif): In the above command, i is for inner . This also works with parentheses, curly and square brackets, greater than and less than symbols – anything marking a beginning and an end. It applies to words, too: In the above example, diw stands for delete inner word . You can also paste content inside boundaries (then again, the double quotes appear as the shift key followed by a single quote on the gif): The trick here is to use Vim's visual mode , which is different from the normal and insert modes (two other modes also exist – command and replace – but we'll stick to those three in this article). The visual mode is essentially a selection mode, which we briefly saw earlier when we covered yanking . To enter it, make sure you're in normal mode and press v . From there, the same shortcuts used to move around the text also apply to selecting it. In the example above, we've selected the text between double to paste some other text over it using the vi\"p sequence, which roughly translates to visual inner double quotes and paste . Note that whenever you select some text using the mouse instead of the keyboard while in normal mode, you automatically enter the visual mode. Editing There are several ways to go about editing, but in practice I only use a single command which I combine with others. Starting in normal mode, select a bunch of text and press c (for change ): This is basically a shortcut for di – delete and insert . Like the other commands, it can be associated with different shortcuts to achieve various outcomes. You can for instance change the current word with ciw (for change inner word ): Or replace the next three words with c3w (for change 3 words ): Or until the next exclamation mark (which appears as the shift key followed by the number 1 on the gif): In the above example, ct! stands for change till exclamation mark . You can also replace the entire line with cc : Or change till the end of the line with C (capital c): There is more to editing, but as the above covers 99% of what I use daily, I'll leave it at that. Should you be interested though, you'll find more information in the resources listed at the end of this article. Searching, saving and exiting As Vim is originally a text editor in its own right, it also has commands for searching/replacing, saving and – infamously – exiting. But since the premise of this article is to use it within your editor of choice, you're probably comfortable with the way those are handled already. While nothing prevents you from using the Vim way instead, you wouldn't miss out too much by sticking to what you already know. Then again, if you want to know more the resources listed further down will help. Undoing and redoing While undoing and redoing are also standard IDE features, you might want to learn the Vim shortcuts for these because they don't necessarily mean the same thing. When you undo something in Vim, you revert everything you've done since the last time you were in normal mode. Conversely, when you redo something, you restore the changes until the point you switched back to normal mode. Your code editor may not have the same notions of previous and next changes, which is why you should probably learn the Vim way for these. Thankfully, it isn't complicated at all. You can undo some changes by pressing u : Note that I pressed the escape key to return to normal mode first. You can redo the changes by pressing &#94;r (the control key followed by lower case r): Another useful thing to know is that when you start typing a Vim sequence and realise you've got it wrong, pressing the escape key will cancel it. Combining Vim shortcuts with native features This is the last thing I wanted to broach today. In the introduction to this article, I mentioned that developers would be able to use Vim features alongside native IDE ones. Here is an example of this, taken from my own practice. I really like VSCode's way of handling multiple cursors, and while it's possible to achieve a somewhat similar result with native Vim features, I use VSCode's implementation all the time ( cmd+d on a Mac): What I often do is I copy a value in the regular clipboard, select the instances of a bit of text I want to replace using VSCode's multi-cursor feature, then press c and paste the text the regular way (with cmd+v ): This is a weird combo that ultimately only uses a single Vim command ( c ), but one that works for me. The point is that nothing forces you to embrace Vim fully. Try different approaches, see what works for you and ignore the rest. Conclusion It may have occurred to you in light of this article that Vim is pretty much its own language. You've got to learn some vocabulary first, then some grammar and make sure you've got the syntax right. Eventually, you start forming whole sentences allowing you to achieve increasingly complex results, in a way that almost feels natural. There's undoubtedly a learning curve to Vim, but it's a small price to pay to leverage its potential in my opinion. Besides, this initial hurdle is further mitigated by its integration into popular code editors, allowing users to experiment within familiar surroundings. Vim has many features to offer, and while this article is a collection of rather basic examples they can get you a long way already. The resources listed in the next section will give you some pointers if you want to explore the topic further. I find this cheat sheet to be particularly helpful, for instance. In any case, Vim has been around for a long time and is used by many people; as a result, there's a vast amount of readily accessible knowledge out there. I hope this post will encourage you to give it a go, and if you're a Vim veteran already, I'd love to know about your favourite commands in the comments. Resources Vim.org Vim Adventures Learn Vim (the Smart Way) Vim cheat sheet Vi vs Vim VSCode extension Sublime Text extension PHPStorm extension Atom extension You may have recognised some quotes from Breaking Bad Gifs were made with Keycastr , QuickTime , FFmpeg and Gifsicle ( tutorial )","tags":"Productivity","url":"https://tech.osteel.me/posts/vim-is-much-cooler-than-you-think"},{"title":"A complete guide to Laravel Sail","text":"Truman continues to steer his wrecked sailboat towards the infinitely receding horizon. All is calm until we see the bow of the boat suddenly strike a huge, blue wall, knocking Truman off his feet. Truman recovers and clambers across the deck to the bow of the boat. Looming above him out of the sea is a cyclorama of colossal dimensions. The sky he has been sailing towards is nothing but a painted backdrop. (Andrew M. Niccol, The Truman Show ) On December 8 2020, Taylor Otwell announced the launch of Laravel Sail , a development environment based on Docker, along with a large overhaul of Laravel's documentation: 📸 If you missed my stream walking through Laravel Sail and chatting about some of the documentation improvements, you can watch it here! Stream really starts at the timestamp in this link: https://t.co/U0otoNHg8U 📸 — Taylor Otwell ⛵️ (@taylorotwell) December 8, 2020 The announcement caused a wave of excitement across the community, as a lot of people identified the new environment as a way to finally get into Docker; but it also left some confusion in its wake, as Sail introduces an approach to development that is quite different from its predecessors and isn't exactly a guide to becoming a Docker expert. This post is about what to expect from Laravel Sail, how it works and how to make the most of it; it is also a plea to developers to break away from it, in favour of their own, tailored solution. But before we get there, we need to take a look under the deck, starting with a high-level explanation of what Sail is. In this post In this post What is Laravel Sail? How does it compare to its predecessors? How does it work? The docker-compose.yml file The mysql service The redis service The laravel.test service The meilisearch, mailhog and selenium services The sail script Extending Laravel Sail Installing extra extensions Adding new services Custom sail commands What's wrong with Laravel Sail anyway? The whale in the cabin You don't need Laravel Sail Conclusion Resources What is Laravel Sail? Sail is Laravel's latest development environment. It is the most recent addition to a long list featuring official solutions like Homestead and Valet on the one hand, and community efforts like Laragon , Laradock , Takeout and Vessel on the other (according to the GitHub repository , Sail is largely inspired by the latter). Laravel Sail is based on Docker , a technology leveraging containers to essentially package up applications so they can run quickly and easily on any operating system. The future of Sail appears to be bright, as the Laravel documentation already features it as the preferred way to instal and run Laravel projects locally, a spot that Homestead and Valet occupied for years. How does it compare to its predecessors? As a refresher, Homestead is a Vagrant box (a virtual machine) pre-packaged with everything most Laravel applications need, including essential components like PHP, MySQL and a web server (Nginx), but also less-often used technologies like PostgreSQL, Redis or Memcached. Valet, on the other hand, is a lightweight environment for macOS focused on performance, relying on a local installation of PHP instead of a virtual machine, and intended to be used along with other services like DBngin or Takeout to manage other dependencies like databases. While Homestead and Valet look quite different on paper, they promote the same general approach to local development, which is also shared by most of the aforementioned solutions: they try to be one-size-fits-all environments for Laravel projects and to manage them all under one roof. Sail's approach is different, in that the development environment's description is included with the rest of the codebase. Instead of relying on the presence of a third-party solution on the developer's machine, the project comes with a set of instructions for Docker to pick up and build the corresponding environment. The application comes with batteries included, only requiring a single command to spin up its development environment, regardless of the developer's operating system so long as Docker is installed on it. It also introduces the notion of a bespoke development environment for the application, which, in my opinion, is Laravel Sail's real kicker. While this approach is a major departure from traditional solutions, Sail still bears some resemblance to them around the tools it comes with, some of which are essential, others not. Let's review the most important ones and the way they're implemented. How does it work? From here on, it will be easier to follow along with a fresh installation of Laravel, although the files I refer to come with links to the official GitHub repository . If you've got a little bit of time, go follow the instructions for your operating system now and come back here when you're done. While Sail allows us to pick the services we're interested in when creating a new Laravel application, by default it is composed of three main components: PHP, MySQL and Redis. As per the documentation , the whole setup gravitates around two files: docker-compose.yml [ link ] (which you will find at the project's root after a fresh installation) and the sail script [ link ] (found under vendor/bin ). The docker-compose.yml file As mentioned earlier, Laravel Sail is based on Docker, which is a technology leveraging containers. As a rule of thumb, each container should only run one process; roughly translated, that means that each container should only run a single piece of software. If we apply this rule to the above setup, we'll need one container for PHP, another one for MySQL, and a third one for Redis. These containers make up your application, and they need to be orchestrated for it to function properly. There are several ways to do this, but Laravel Sail relies on Docker Compose to do the job, which is the easiest and most used solution for local setups. Docker Compose expects us to describe the various components of our application in a docker-compose.yml file, in YAML format. If you open the one at the root of the project, you will see a version parameter at the top, under which there is a services section containing a list of components comprising the ones we've just mentioned: laravel.test , mysql and redis . I'll describe the mysql and redis services first, as they are simpler than laravel.test ; I'll then briefly cover the other, smaller ones that also come by default with a new instal. The mysql service As the name suggests, the mysql service handles the MySQL database: mysql : image : 'mysql:8.0' ports : - '${FORWARD_DB_PORT:-3306}:3306' environment : MYSQL_ROOT_PASSWORD : '${DB_PASSWORD}' MYSQL_DATABASE : '${DB_DATABASE}' MYSQL_USER : '${DB_USERNAME}' MYSQL_PASSWORD : '${DB_PASSWORD}' MYSQL_ALLOW_EMPTY_PASSWORD : 'yes' volumes : - 'sailmysql:/var/lib/mysql' networks : - sail healthcheck : test : [ \"CMD\" , \"mysqladmin\" , \"ping\" ] The image parameter indicates which image should be used for this container. An easy way to understand images and the difference with containers is to borrow from Object-Oriented Programming concepts: an image is akin to a class and a container to an instance of that class. Here, we specify that we want to use the tag 8.0 of the mysql image, corresponding to MySQL version 8.0. By default, images are downloaded from Docker Hub , which is the largest image registry. Have a look at the page for MySQL – most images come with simple documentation explaining how to use it. The ports key allows us to map local ports to container ports, following the local:container format. In the code snippet above, the value of the FORWARD_DB_PORT environment variable (or 3306 if that value is empty) is mapped to the container's 3306 port. This is mostly useful to connect third-party software to the database, like MySQL Workbench or Sequel Ace ; the setup would also work without it. environments is for defining environment variables for the container. Here, most of them receive the value of existing environment variables, which are loaded from the .env file at the root of the project – docker-compose.yml automatically detects and imports the content of this file. For instance, in the MYSQL_ROOT_PASSWORD: '${DB_PASSWORD}' line, the container's MYSQL_ROOT_PASSWORD environment variable will receive the value of DB_PASSWORD from the .env file. volumes is to declare some of the container's files or folders as volumes , either by mapping specific local files or folders to them or by letting Docker deal with it. Here, a single Docker-managed volume is defined: sailmysql . This type of volume must be declared in a separate volumes section, at the same level as services . We can find it at the bottom of the docker-compose.yml file: volumes : sailmysql : driver : local sailredis : driver : local sailmeilisearch : driver : local The sailmysql volume is mapped to the container's /var/lib/mysql folder, which is where the MySQL data is stored. This volume ensures that the data is persisted even when the container is destroyed, which is the case when we run the sail down command. The networks section allows us to specify which internal networks the container should be available on. Here, all services are connected to the same sail network, which is also defined at the bottom of docker-compose.yml , in the networks section above the volumes one: networks : sail : driver : bridge Finally, healthcheck is a way to indicate which conditions need to be true for the service to be ready , as opposed to just be started . I'll go back to this soon. The redis service The redis service is very similar to the mysql one: redis : image : 'redis:alpine' ports : - '${FORWARD_REDIS_PORT:-6379}:6379' volumes : - 'sailredis:/data' networks : - sail healthcheck : test : [ \"CMD\" , \"redis-cli\" , \"ping\" ] We pull the alpine tag of the official image for Redis ( Alpine is a lightweight Linux distribution ) and we define which port to forward; we then declare a volume to make the data persistent, connect the container to the sail network, and define the check to perform in order to consider the service ready. The laravel.test service The laravel.test service is more complex: laravel.test : build : context : ./vendor/laravel/sail/runtimes/8.0 dockerfile : Dockerfile args : WWWGROUP : '${WWWGROUP}' image : sail-8.0/app ports : - '${APP_PORT:-80}:80' environment : WWWUSER : '${WWWUSER}' LARAVEL_SAIL : 1 volumes : - '.:/var/www/html' networks : - sail depends_on : - mysql - redis - selenium For starters, the name is a bit confusing, but this service is the one handling PHP (i.e. the one serving the Laravel application). Next, it has a build key that we haven't seen before, which points to the Dockerfile [ link ] that is present under the vendor/laravel/sail/runtimes/8.0 folder. Dockerfiles are text documents containing instructions to build images. Instead of pulling and using an existing image from Docker Hub as-is, the Laravel team chose to describe their own in a Dockerfile. The first time we ran the sail up command, we built that image and created a container based on it. Open the Dockerfile and take a look at the first line: FROM ubuntu:20.04 This means that the tag 20.04 of the ubuntu image is used as a starting point for the custom image; the rest of the file is essentially a list of instructions to build upon it, installing everything a standard Laravel application needs. That includes PHP, various extensions, and other packages like Git or Supervisor, as well as Composer. The end of the file also deserves a quick explanation: COPY start-container /usr/local/bin/start-container COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf COPY php.ini /etc/php/8.0/cli/conf.d/99-sail.ini RUN chmod +x /usr/local/bin/start-container EXPOSE 8000 ENTRYPOINT [ \"start-container\" ] We can see that a bunch of local files are copied over to the container: the php.ini file [ link ] is some custom configuration for PHP; the supervisord.conf file [ link ] is a configuration file for Supervisor , a process manager here responsible for starting the PHP process; the start-container file [ link ] is a Bash script that will do a few things every time the container starts, because it is defined as the container's ENTRYPOINT . We can see that it's made executable by the RUN chmod +x instruction; finally, EXPOSE 8000 doesn't do anything, apart from informing the reader that this container listens on the specified port at runtime (which actually seems wrong here, since the application is served on port 80, not 8000). Other things are happening in this Dockerfile, but the above is the gist of it. Note that this one pertains to PHP 8.0, but Laravel Sail also comes with a 7.4 version you can point to from the laravel.test service in docker-compose.yml instead. The service also has a depends_on section containing the list of services whose containers should be ready prior to the Laravel application's. Since the latter references MySQL, Redis and Selenium, theirs should be started and ready first to avoid connection errors. This is where the health checks described earlier are useful: by default depends_on will wait for the specified services to be started , which doesn't necessarily mean they are ready . By specifying on which conditions these services are deemed ready, we ensure they are in the right state prior to starting the Laravel application. The rest of the settings should be familiar by now, so I'll skip them. The meilisearch , mailhog and selenium services These are the smaller services I referred to earlier; they are already documented here , here and here . The point is they work the same way as the other ones: they pull existing images from Docker Hub and use them as-is, with minimal configuration. The sail script If you followed Laravel's installation instructions for your operating system, you must have run the following command at some point: $ ./vendor/bin/sail up The sail file [ link ] that we call here is a Bash script essentially adding a more user-friendly layer on top of sometimes long-winded Docker commands. Let's open it now for closer inspection (don't worry if you're not familiar with Bash – it's pretty straightforward). We can ignore the whole first part of the file and focus on the big if statement that starts like this: if [ $# -gt 0 ] ; then # Source the \".env\" file so Laravel's environment variables are available... if [ -f ./.env ] ; then source ./.env fi # ... In plain English, the $# -gt 0 bit translates to \"if the number of arguments is greater than 0\", meaning whenever we call the sail script with arguments, the execution will enter that if statement. In other words, when we run the ./vendor/bin/sail up command, we call the sail script with the up argument, and the execution gets inside the big if statement where it looks for a condition matching the up argument. Since there is none, the script goes all the way down to the end of the big if , in the sort of catch-all else we can find there: # Pass unknown commands to the \"docker-compose\" binary... else docker compose \" $@ \" fi The comment already describes what's going on – the script passes the up argument on to the docker-compose binary. In other words, when we run ./vendor/bin/sail up we actually run docker compose up , which is the standard Docker Compose command to start the containers for the services listed in docker-compose.yml . This command downloads the corresponding images first if necessary, and builds the Laravel image based on the Dockerfile as we talked about earlier. Give it a try! Run ./vendor/bin/sail up then docker compose up – they do the same thing. Let's now look at a more complicated example, one involving Composer, which is among the packages installed by the application's Dockerfile. But before we do that, let's start Sail in detached mode to run the containers in the background: $ ./vendor/bin/sail up -d The sail script allows us to run Composer commands, e.g.: $ ./vendor/bin/sail composer --version The above calls the sail script with composer and --version as arguments, meaning the execution will enter that big if statement again. Let's search for the condition dealing with Composer: # ... # Proxy Composer commands to the \"composer\" binary on the application container... elif [ \" $1 \" == \"composer\" ] ; then shift 1 if [ \" $EXEC \" == \"yes\" ] ; then docker compose exec \\ -u sail \\ \" $APP_SERVICE \" \\ composer \" $@ \" else sail_is_not_running fi # ... The first line of the condition starts with shift , which is a Bash built-in that skips as many arguments as the number it is followed by. In this case, shift 1 skips the composer argument, making --version the new first argument. The program then makes sure that Sail is running, before executing a weird command split over four lines, which I break down below: docker compose exec \\ -u sail \\ \" $APP_SERVICE \" \\ composer \" $@ \" exec is the way Docker Compose allows us to execute commands on already running containers. -u is an option indicating which user we want to execute the command as, and $APP_SERVICE is the container on which we want to run it all. Here, its value is laravel.test , which is the service's name in docker-compose.yml as explained in a previous section. It is followed by the command we want to run once we're in the container, namely composer followed by all the script's arguments. These now only comprise --version , since we've skipped the first argument. In other words, when we run: $ ./vendor/bin/sail composer --version The command that is executed behind the scenes is the following: $ docker compose exec -u sail \"laravel.test\" composer \"--version\" It would be quite cumbersome to type this kind of command every single time; that's why the sail script provides shortcuts for them, making the user experience much smoother. Have a look at the rest of the smaller if statements inside the big one to see what else is covered – you'll see that roughly the same principle applies everywhere. There are a few other features available out of the box (like making local containers public ), but we've now covered the substance of what Laravel Sail currently offers. While this is a pretty good start already, it is somewhat limited, even for a basic application. The good news is that the Laravel team is aware of this, and built the environment with extension in mind: Since Sail is just Docker, you are free to customize nearly everything about it. ( The Laravel documentation ) Let's see what that means in practice. Extending Laravel Sail The code covered in this section is also available as a GitHub repository you can refer to at any moment. We're going to explore three ways to extend Laravel Sail, using MongoDB as a pretext; but before we proceed, let's make sure we get our hands on as many files as we can. The only thing we've got access to initially is the docker-compose.yml file [ link ], but we can publish more assets with the following command, which will create a new docker folder at the root of the project: $ ./vendor/bin/sail artisan sail:publish We'll get back to those in a minute; for the time being, let's try and instal the Laravel MongoDB package, which will make it easy to use MongoDB with our favourite framework: $ ./vendor/bin/sail composer require jenssegers/mongodb Unfortunately, Composer is complaining about some missing extension: mongodb/mongodb [ dev-master, 1 .8.0-RC1, ..., v1.8.x-dev ] require ext-mongodb &#94;1.8.1 -> it is missing from your system. Install or enable PHP ' s mongodb extension Let's fix this! Installing extra extensions Earlier in this post, we talked about the way Sail uses Dockerfiles to build images matching Laravel's requirements for both PHP 7.4 and PHP 8.0. These files were published with the command we ran at the beginning of this section – all we need to do to add extensions is to edit them and rebuild the corresponding images. Many extensions are available out of the box and we can list them with the following command: $ ./vendor/bin/sail php -m MongoDB is not part of them; to add it, open the docker/8.0/Dockerfile file and spot the RUN instruction installing the various packages: RUN apt-get update \\ && apt-get install -y gnupg gosu curl ca-certificates zip unzip git supervisor sqlite3 libcap2-bin \\ && mkdir -p ~/.gnupg \\ && echo \"disable-ipv6\" >> ~/.gnupg/dirmngr.conf \\ && apt-key adv --homedir ~/.gnupg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys E5267A6C \\ && apt-key adv --homedir ~/.gnupg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C300EE8C \\ && echo \"deb http://ppa.launchpad.net/ondrej/php/ubuntu focal main\" > /etc/apt/sources.list.d/ppa_ondrej_php.list \\ && apt-get update \\ && apt-get install -y php8.0-cli php8.0-dev \\ php8.0-pgsql php8.0-sqlite3 php8.0-gd \\ php8.0-curl php8.0-memcached \\ php8.0-imap php8.0-mysql php8.0-mbstring \\ php8.0-xml php8.0-zip php8.0-bcmath php8.0-soap \\ php8.0-intl php8.0-readline \\ php8.0-msgpack php8.0-igbinary php8.0-ldap \\ php8.0-redis \\ && php -r \"readfile('http://getcomposer.org/installer');\" | php -- --install-dir = /usr/bin/ --filename = composer \\ && curl -sL https://deb.nodesource.com/setup_15.x | bash - \\ && apt-get install -y nodejs \\ && apt-get -y autoremove \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/ It's easy to identify the block related to PHP extensions since they all start with php8.0 . Amend the end of the list so it looks like this: php8.0-redis php8.0-mongodb \\ You can see the detail of the available PHP extensions for Ubuntu 20.04 here . Save the file and run the following command: $ ./vendor/bin/sail build This will go through all the services listed in the docker-compose.yml file and build the corresponding images if they have changed, including the laravel.test service's, whose Dockerfile we've just updated. Once it's done, start the containers again: $ ./vendor/bin/sail up -d The command will detect that the image corresponding to the laravel.test service has changed, and recreate the container: That's it! The MongoDB extension for PHP is now installed and enabled. We've only done it for the PHP 8.0 image, but you can apply the same process to PHP 7.4's by updating the docker/7.4/Dockerfile file instead, with php7.4-mongodb as the extension name. We can now safely import the Laravel package: $ ./vendor/bin/sail composer require jenssegers/mongodb Next up: adding a Docker service for MongoDB. Adding new services MongoDB is essentially another database; as a result, the corresponding service will be very similar to the ones of MySQL and Redis. A quick search on Docker Hub reveals that there is an official image for it, which we are going to use. Its documentation contains an example configuration for Docker Compose, which we can copy and adjust to our needs. Open docker-compose.yml and add the following service at the bottom, after the other ones: mongo : image : 'mongo:4.4' restart : always environment : MONGO_INITDB_ROOT_USERNAME : '${DB_USERNAME}' MONGO_INITDB_ROOT_PASSWORD : '${DB_PASSWORD}' MONGO_INITDB_DATABASE : '${DB_DATABASE}' volumes : - 'sailmongo:/data/db' networks : - sail The changes I've made are the following: first, I specified the tag 4.4 of the mongo image. If you don't specify one, Docker Compose will pull the latest tag by default, which is not good practice since it will refer to different versions of MongoDB over time, as new releases are available. The introduction of breaking changes could create instability in your Docker setup, so it's better to target a specific version, matching the production one whenever possible. Then, I declared a MONGO_INITDB_DATABASE environment variable for the container to create a database with the corresponding name at start-up, and I matched the value of each environment variable to one coming from the .env file (we'll come back to those in a minute). I also added a volumes section, mounting a Docker-managed volume onto the container's /data/db folder. The same principle as MySQL and Redis here applies: if you don't persist the data on your local machine, it will be lost every time the MongoDB container is destroyed. In other words, as the MongoDB data is stored in the container's /data/db folder, we persist that folder locally using a volume. As this volume doesn't exist yet, we need to declare it at the bottom of docker-compose.yml , after the other ones: volumes : sailmysql : driver : local sailredis : driver : local sailmeilisearch : driver : local sailmongo : driver : local Finally, I added the networks section to ensure the service is on the same network as the others. We can now configure Laravel MongoDB as per the package's instructions . Open config/database.php and add the following database connection: 'mongodb' => [ 'driver' => 'mongodb', 'host' => env('DB_HOST'), 'port' => env('DB_PORT'), 'database' => env('DB_DATABASE'), 'username' => env('DB_USERNAME'), 'password' => env('DB_PASSWORD'), 'options' => [ 'database' => env('DB_AUTHENTICATION_DATABASE', 'admin'), ], ], Open the .env file at the root of the project and change the database values as follows: DB_CONNECTION = mongodb DB_HOST = mongo DB_PORT = 27017 DB_DATABASE = laravel_sail DB_USERNAME = root DB_PASSWORD = root The above makes MongoDB the main database connection; in a real case scenario, you might want to make it a secondary database like Redis, but for demonstration purposes, this will do. DB_HOST is the name of the MongoDB service from docker-compose.yml ; behind the scenes, Docker Compose resolves the service's name to the container's IP on the networks it manages (in our case, that's the single sail network defined at the end of docker-compose.yml ). DB_PORT is the port MongoDB is available on, which is 27017 by default, as per the image's description . We're ready for a test! Run the following command again: $ ./vendor/bin/sail up -d It will download MongoDB's image, create the new volume and start the new container, which will also create the laravel_sail database: Let's make sure of that by running Laravel's default migrations: $ ./vendor/bin/sail artisan migrate We can push the test further by updating the User model so it extends Laravel MongoDB's Authenticable model: <?php namespace App\\Models ; use Illuminate\\Contracts\\Auth\\MustVerifyEmail ; use Illuminate\\Database\\Eloquent\\Factories\\HasFactory ; use Illuminate\\Notifications\\Notifiable ; use Jenssegers\\Mongodb\\Auth\\User as Authenticatable ; class User extends Authenticatable { // ... Use Tinker to try and create a model: $ ./vendor/bin/sail tinker Psy Shell v0.10.5 (PHP 8.0.0 — cli) by Justin Hileman >>> \\App\\Models\\User::factory()->create(); Great! Our MongoDB integration is functional. We can keep interacting with it using Tinker and Eloquent, but oftentimes it is useful to have direct access to the database, through third-party software or via a command-line interface such as the Mongo shell . Let's add the latter to our setup. Custom sail commands The good news is the Mongo shell is already available, as long as we know the right formula to summon it. Here it is, along with some extra commands to log into the database and list the users (run the first command from the project's root): $ docker compose exec mongo mongo MongoDB shell version v4.4.2 connecting to: mongodb://127.0.0.1:27017/?compressors = disabled & gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"919072cf-817d-43a6-9ffb-c5e721eeefbc\" ) } MongoDB server version: 4 .4.2 Welcome to the MongoDB shell. For interactive help, type \"help\" . For more comprehensive documentation, see https://docs.mongodb.com/ Questions? Try the MongoDB Developer Community Forums https://community.mongodb.com > use admin switched to db admin > db.auth ( \"root\" , \"root\" ) 1 > use laravel_sail switched to db laravel_sail > db.users.find () The docker compose exec mongo mongo command should look familiar; earlier in the article, we looked at what the sail script does behind the scenes, which mostly consists of translating simple sail commands into more complex docker-compose ones. Here, we're telling the docker-compose binary to execute the mongo command on the mongo container. To be fair, this command isn't too bad and we could easily remember it; but for consistency, it would be nice to have a simpler sail equivalent, like the following: $ ./vendor/bin/sail mongo To achieve this we'd need to complete the sail script somehow, but as it is located inside the vendor folder – which is created by Composer – we cannot update it directly. We need a way to build upon it without modifying it, which I've summarised below: make a copy of the sail script at the root of the project; replace the content of its big if statement with custom conditions; if none of the custom conditions matches the current arguments, pass them on to the original sail script. If we take a closer look at the sail file with ls -al , we can see that it's a symbolic link to the vendor/laravel/sail/bin/sail file: Let's copy that file to the root of our project now: $ cp vendor/laravel/sail/bin/sail . Open the new copy and replace the content of its big if with the following, leaving the rest as-is: if [ $# -gt 0 ] ; then # Source the \".env\" file so Laravel's environment variables are available... if [ -f ./.env ] ; then source ./.env fi # Initiate a Mongo shell terminal session within the \"mongo\" container... if [ \" $1 \" == \"mongo\" ] ; then if [ \" $EXEC \" == \"yes\" ] ; then docker compose exec mongo mongo else sail_is_not_running fi # Pass unknown commands to the original \"sail\" script.. else ./vendor/bin/sail \" $@ \" fi fi In the above code, we removed all the if...else conditions inside the big if and added one of our own, which runs the command we used earlier to access the Mongo shell if the value of the script's first argument is mongo . If it's not, the execution will hit the last else statement and call the original sail script with all the arguments. You can try this out now – save the file and run the following command: $ ./sail mongo It should open a Mongo shell session in your terminal. Try another command, to make sure the original sail script is taking over when it's supposed to: $ ./sail artisan The Artisan menu should display. That's it! If you need more commands, you can add them as new if...else conditions inside the big if of the copy of the sail script at the root of the project. It works exactly the same way, except that you now need to run ./sail instead of ./vendor/bin/sail (or update your Bash alias if you created one as suggested in the documentation ). We are now running a fully functional instance of MongoDB as part of our Docker setup, nicely integrated with Laravel Sail. But MongoDB is a mere example here – you can do the same with pretty much any technology you'd like to use. Go take a look now! Most major actors have Docker images – official or community-maintained – with easy-to-follow instructions. In most cases, you'll have a local instance of the software running in minutes. There are probably many more things we could do to customise Laravel Sail, but the three methods described above should get you a long way already. At this stage, you may be thinking that Laravel's new environment has a lot going for it, maybe even more so than you initially thought. Yet, the point of this article is to avoid using it... So where am I going with this? Why not use Takeout? While reading this section, it may have occurred to you that existing solutions like Takeout support MongoDB out of the box. While using Sail in conjunction with some other technology is possible and would make up for some of its current limitations, relying on Takeout would reintroduce a third-party dependency to our setup. Instead of just using Docker, we would now expect everyone in the team to also instal and configure Takeout on their machine. Sail opens the way to having the whole development environment handled by a single docker-compose.yml file that is part of the codebase. My opinion is that if we go down that path, we should embrace it all the way and make Docker the only third-party dependency. That is not to say Takeout has no utility whatsoever; it makes a lot of sense to circumvent some performance issues that we will talk about later in this article. What's wrong with Laravel Sail anyway? If you made it this far, you're probably wondering what's wrong with Laravel Sail, now that it's clear how far we can push it. Let me break it to you right now: once you know and understand everything I've explained in the previous sections, you don't need Laravel Sail anymore. That's right – you can take that knowledge and walk away. But before I elaborate on this, let's review some actual pain points of Sail, even though I expect the Laravel team to address most of them sooner rather than later. The first one concerns the custom sail commands: while it's possible to extend the sail script as demonstrated earlier, the process is a bit ugly and somewhat hacky. Sail's maintainers could fix this with an explicit Bash extension point allowing users to add their own shortcuts, or by publishing the sail script along with the other files. Second, the Laravel application is served by PHP's development server. I won't go into too much detail here, but as mentioned before Supervisor manages the PHP process in the laravel.test container; this line is where Supervisor runs the php artisan serve command, which starts PHP's development server under the hood. The point here is that the environment doesn't use a proper web server (e.g. Nginx), which means we can't easily have local domain names, nor bring HTTPS to the setup. This may be fine for quick prototyping, but more elaborate development will most likely need those. The third issue is one I noticed while trying to clone and run a fresh instance of this article's repository for testing. While the process to create a new Laravel project based on Sail works well, I couldn't find proper instructions to instal and run an existing one. You can't run ./vendor/bin/sail up because the vendor folder doesn't exist yet. For this folder to be created, you need to run composer install ; but if your project relies on dependencies present on the Docker image but not on your local machine, composer install won't work. You can run composer install --ignore-platform-reqs instead, but that doesn't feel right. There should be a way to instal and run an existing project without relying on a local Composer instance and clunky commands. The last issue belongs to a separate category, as it relates to Docker overall and not Laravel Sail specifically. It should be carefully considered before going down the Docker road and deserves a section of its own. The whale in the cabin The one major caveat that appears to be absent from the conversation so far relates to performance. While this shouldn't affect Linux users, if you run Docker Desktop on your system you will most likely experience long loading times, especially on macOS (it seems that using WSL 2 on Windows can mitigate the slowness). You can see it for yourself right now: if you're using Docker Desktop and Sail is running, try and load the Laravel welcome page – you will probably notice a delay. I won't go into too much detail here, but the reason essentially comes from the host's underlying filesystem, which does not perform well around mounted local directories. As we've seen, this is how Laravel Sail gets the application's source code in the Laravel application's container, hence the slowness. This is where an approach like Takeout's makes sense, as instead of running PHP from a Docker container, they expect developers to run it on their local machine (e.g. via Valet ), all the while providing instances of services like MySQL or MongoDB, thus offering convenience without sacrificing performance. But from the moment you choose to run PHP via a Docker container (like Sail does), the added value of Takeout decreases, in my opinion. There are strategies to mitigate these performance issues, but the Laravel documentation mentions none of them, let alone the fact that performance might be an issue at all, which I find surprising. That being said, you might be comfortable enough with performance as it is; I, for one, have been OK with it for years, even though I use Docker Desktop on macOS. The bottom line is that this aspect should be carefully considered before moving your whole setup to a solution running PHP in a container, be it Laravel Sail or something else. But once you've made that decision, and whether or not the other issues are eventually addressed, the main idea of this article remains the same. You don't need Laravel Sail If you're considering building anything substantial using Laravel Sail as your development environment, sooner or later you will have to extend it. You'll find yourself fumbling around the Dockerfiles and eventually writing your own; having to add some services to docker-compose.yml ; and maybe throwing in a few custom Bash commands. Once you get there, there's one question you should ask yourself: What's stopping me from building my own setup? The answer is nothing . Once you feel comfortable extending Laravel Sail, you already have the knowledge required to build your own environment. Think about it: the docker-compose.yml file is not specific to Laravel Sail, that's just how Docker Compose works. The same goes for Dockerfiles – they are standard Docker stuff. The Bash layer? That's all there is to it – some Bash code, and as you can see, it's not that complicated. So why artificially restrain yourself within the constraints of Sail? And more importantly: why limit yourself to using Docker in the context of Laravel? Your application may start as a monolith, but it might not always be. Perhaps you've got a separate frontend, and you use Laravel as the API layer. In that case, you might want your development environment to manage them both; to run them simultaneously so they interact with each other like they do on a staging environment or in production. If your whole application is a monorepo , your Docker configuration and Bash script could be at the root of the project, and you could have your frontend and backend applications in separate subfolders, e.g. under an src folder. The corresponding tree view would look something like this: my-app/ ├── bash-script ├── docker-compose.yml └── src/ ├── backend/ │ └── Dockerfile └── frontend/ └── Dockerfile The docker-compose.yml file would declare two services – one for the backend and one for the frontend – both pointing to each's respective Dockerfile. If the backend and the frontend live in different repositories, you could create a third one, containing your Docker development environment exclusively. Just git-ignore the src folder and complete your Bash script so that it pulls both application repositories into it, using the same commands you would normally run by hand. Even if your project is a Laravel monolith, this kind of structure is already cleaner than mixing up development-related files with the rest of the source code. Moreover, if your application grows bigger and needs other components besides Laravel, you're already in a good position to support them. Once you've made the effort to understand Laravel Sail to extend it, nothing is stopping you from building your own development environments, whether or not Laravel is part of the equation . That's right, you can build bespoke Docker-based environments for anything. And if Laravel is part of the stack, nothing prevents you from reusing Sail's Dockerfiles if you're not comfortable writing your own yet; after all, they are already optimised for Laravel. Likewise, you can draw inspiration from Sail's docker-compose.yml file if that helps. Conclusion Don't get me wrong: Laravel Sail has a lot going for it, and I am glad to see such an established actor push forward the adoption of Docker for local development. We love our frameworks because they offer guidelines to achieve desired results in a way we know to be efficient and battle-tested, and it's only natural that they also seek to provide the environment that will allow their users to build upon them. But one thing that Sail incidentally shows us is that this doesn't have to be part of the framework's mandate anymore. Much like Truman's sailboat helps him overcome his fear of the sea and takes him to the edges of the artificial world he lives in, Sail reveals both the confines of Laravel and a way to escape from them. You may feel that Sail is more than enough for your needs today, or that you're not yet ready to go your own way. That's fine. But Laravel will always be limited by its monolithic nature, and as you grow as a developer, the day will come where your Laravel application will be but a single component of a larger system, for which Sail won't be enough anymore. Eventually, your small sailboat will bump into a painted backdrop. If you'd like to explore this further but feel like you need more guidance, I've published a series on the subject that should get you going. It requires no prior knowledge of Docker and covers web servers, HTTPS, domain names and many other things. It doesn't have all the answers but will get you to a place where you can find your own. What you do next is entirely up to you; just know that there's a whole world out there, waiting for you. Truman hesitates. Perhaps he cannot go through with it after all. The camera slowly zooms into Truman's face. TRUMAN: \"In case I don't see you – good afternoon, good evening and good night.\" He steps through the door and is gone. Resources Laravel Sail documentation Laravel Sail repository This article's repository Docker Hub Docker documentation Overview of Docker Compose Dockerfile reference What is a Container? Docker for local web development series","tags":"Laravel","url":"https://tech.osteel.me/posts/you-dont-need-laravel-sail"},{"title":"OpenAPI-backed API testing in PHP projects – a Laravel example","text":"Am I proud of this montage? You bet I am. OpenAPI is a specification intended to describe RESTful APIs in JSON and YAML, with the aim of being understandable by humans and machines alike. OpenAPI definitions are language-agnostic and can be used in a lot of different ways: An OpenAPI definition can be used by documentation generation tools to display the API, code generation tools to generate servers and clients in various programming languages, testing tools, and many other use cases. – The OpenAPI Specification In this article, we will see how to combine OpenAPI 3.0.x definitions with integration tests to validate whether an API behaves the way it's supposed to, using the OpenAPI HttpFoundation Testing package. We will do so in a fresh Laravel installation, for which we'll also generate a Swagger UI documentation using the L5 Swagger package. I will first elaborate a bit further on why this is useful, but if you're just here for the code, you're welcome to skip ahead and go to the Laravel example section straight away. The issue APIs are pretty common nowadays, and when we're lucky they come with some form of documentation that helps us find our way around the endpoints. These documentations come in many shapes and flavours (some tastier than others), but one thing they've got in common in that they need to be updated every time the API they describe changes. To many developers, maintaining an API's documentation feels like extra homework when they've already passed the exam; it's boring, sometimes tedious, and often unrewarding. Some strategies can help, like using annotations to keep the code and the documentation in one place; but those are often annoying to write still, and even the most willing developer is not immune to an oversight that won't necessarily be caught by coworkers. The usual outcome is that, one way or another, the documentation and the API become out of sync, leading to confused consumers. Another aspect of API maintenance is ensuring that no endpoint stops functioning the way it's supposed to; regressions will be introduced eventually, and without a proper testing strategy they might go unnoticed for a while. A way to avoid this is to implement integration tests that will automatically check that the API's behaviour is correct, and that recently introduced changes have not had unintended consequences. This is fine, but still doesn't provide any guarantee that the expectations set in the integration tests are exactly the same as the ones inferred from the documentation. If only there was a way to ensure that they perfectly reflect each other… A solution We are now assuming that we've got an API documentation and some integration tests, and we'd like to align their expectations somehow. The OpenAPI specification has become a popular choice to describe APIs over time, but whether we use it or not doesn't change the fact that the corresponding definitions need to be maintained; in other words, using OpenAPI does not automagically make the aforementioned issues go away. What sets OpenAPI apart, however, is that it's used as the base layer for a growing number of tools that make the specification useful far beyond the mere documenting side of things. One of these tools built for the PHP ecosystem and maintained by The PHP League is OpenAPI PSR-7 Message Validator , a package for validating HTTP requests and responses implementing the PSR-7 standard against OpenAPI definitions. The idea is essentially to take HTTP requests and responses, and make sure they match one of the operations described in an OpenAPI definition. Can you see where this is going? We could basically use this package to add an extra layer on top of our integration tests, that will take the API responses obtained in the tests and make sure they match the OpenAPI definitions describing our API. If they don't, the tests fail. This is what it looks like as a fancy diagram: The OpenAPI definition describes the API, and the tests use the OpenAPI definition to make sure the API actually behaves the way the definition says it does. All of a sudden, our OpenAPI definition becomes a reference for both our code and our tests, thus acting as the API's single source of truth. PSR-7 You might have noticed a small detail in the previous section: the OpenAPI PSR-7 Message Validator package only works for – it's in the name – PSR-7 messages. The issue here is that not all frameworks support this standard out of the box; as a matter of fact, a lot of them use Symfony's HttpFoundation component under the hood, whose requests and responses do not implement that standard by default. The Symfony folks have got us covered though, as they've developed a bridge that converts HttpFoundation objects to PSR-7 ones, as long as it's given a PSR-7 and PSR-17 factory to do so, for which they suggest to use Tobias Nyholm 's PSR-7 implementation . All of these pieces form a jigsaw puzzle that the OpenAPI HttpFoundation Testing package offers to assemble for us, allowing developers to back their integration tests with OpenAPI definitions in projects leveraging the HttpFoundation component. Let's see how to use it in a Laravel project, which falls into this category. A Laravel example The code contained in this section is also available as a GitHub repository . First, let's create a new Laravel 8 project, using Composer: $ composer create-project --prefer-dist laravel/laravel openapi-example \"8.*\" Enter the project's root folder and install a couple of dependencies: $ cd openapi-example $ composer require --dev osteel/openapi-httpfoundation-testing $ composer require darkaonline/l5-swagger The first one is the OpenAPI HttpFoundation Testing package mentioned earlier, that we install as a development dependency as it's intended to be used as part of our test suite. The second one is L5 Swagger , a popular package bringing Swagger PHP and Swagger UI to Laravel. We actually don't need Swagger PHP here, as it uses Doctrine annotations to generate OpenAPI definitions and we're going to write our own manually instead. We do need Swagger UI, however, and the package conveniently adapts it to work with Laravel. To make sure Swagger PHP doesn't overwrite the OpenAPI definition, let's set the following environment variable in the .env file at the root of the project: L5_SWAGGER_GENERATE_ALWAYS = false Create a file named api-docs.yaml in the storage/api-docs folder (which you need to create), and add the following content to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 openapi : 3.0.3 info : title : OpenAPI HttpFoundation Testing Laravel Example version : 1.0.0 servers : - url : http://localhost:8000/api paths : '/test' : get : responses : '200' : description : Ok content : application/json : schema : type : object required : - foo properties : foo : type : string example : bar This is a simple OpenAPI definition describing a single operation – a GET request on the /api/test endpoint, that should return a JSON object containing a required foo key. Let's check whether Swagger UI displays our OpenAPI definition correctly. Start PHP's development server with this artisan command, to be run from the project's root: $ php artisan serve Open localhost:8000/api/documentation in your browser and replace api-docs.json with api-docs.yaml in the navigation bar at the top (this is so Swagger UI loads up the YAML definition instead of the JSON one, as we haven't provided the latter). Hit the enter key or click Explore – our OpenAPI definition should now be rendered as a Swagger UI documentation: Expand the /test endpoint and try it out – it should fail with a 404 Not Found error, because we haven't implemented it yet. Let's fix that now. Open the routes/api.php file and replace the example route with this one: 1 2 3 Route::get('/test', function (Request $request) { return response()->json(['foo' => 'bar']); }); Go back to the Swagger UI tab and try the endpoint again – it should now return a successful response. Time to write a test! Open tests/Feature/ExampleTest.php and replace its content with this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 <?php namespace Tests\\Feature ; use Osteel\\OpenApi\\Testing\\ValidatorBuilder ; use Tests\\TestCase ; class ExampleTest extends TestCase { /** * A basic test example. * * @return void */ public function testBasicTest () { $response = $this -> get ( '/api/test' ); $validator = ValidatorBuilder :: fromYaml ( storage_path ( 'api-docs/api-docs.yaml' )) -> getValidator (); $result = $validator -> validate ( $response -> baseResponse , '/test' , 'get' ); $this -> assertTrue ( $result ); } } Let's unpack this a bit. For those unfamiliar with Laravel, $this->get() is a test method provided by the MakesHttpRequests trait that essentially performs a GET request on the provided endpoint, executing the request's lifecycle without leaving the application. It returns a response that is identical to the one we would obtain if we'd perform the same request from the outside. We then create a validator using the Osteel\\OpenApi\\Testing\\ValidatorBuilder class, to which we feed the YAML definition we wrote earlier via the fromYaml static method (the storage_path function is a helper returning the path to the storage folder, where we stored the definition). Had we had a JSON definition instead, we could have used the fromJson method; also, both methods accept YAML and JSON strings respectively, as well as files. The builder returns an instance of Osteel\\OpenApi\\Testing\\Validator , on which we call the get method, passing the path and the response as parameters ( $response is a Illuminate\\Testing\\TestResponse object here, which is a wrapper for the underlying HttpFoundation object, which can be retrieved through the baseResponse public property). The above is basically the equivalent of saying: I want to validate that this response conforms to the OpenAPI definition of a GET request on the /test path. It could also be written this way: $result = $validator->get($response->baseResponse, '/test'); That's because the validator has a shortcut method for each of the HTTP methods supported by OpenAPI ( GET , POST , PUT , PATCH , DELETE , HEAD , OPTIONS and TRACE ), to make it simpler to test responses for the corresponding operations. Note that the specified path must exactly match one of the OpenAPI definition's paths . You can now run the test, which should be successful: $ ./vendor/bin/phpunit tests/Feature Open routes/api.php again, and change the route for this one: 1 2 3 Route::get('/test', function (Request $request) { return response()->json(['baz' => 'bar']); }); Run the test again; it should now fail, because the response contains baz instead of foo , and the OpenAPI definition says the latter is expected. Our test is backed by OpenAPI! The above is obviously an oversimplified example for the sake of the demonstration, but in a real situation a good practice would be to overwrite the MakesHttpRequests trait's call method, so it performs both the test request and the OpenAPI validation. As a result, our test would now be a single line: $this->get('/api/test'); This could be implemented as a new MakesOpenApiRequests trait that would \"extend\" the MakesHttpRequests one, and that would first call the parent call method to get the response. It would then work out the path from the URI, and validate the response against the OpenAPI definition before returning it, for the calling test to perform any further assertions as needed. Conclusion While the above setup is a great step up in improving an API's robustness, it is no silver bullet; it requires that every single endpoint is covered with integration tests, which is not easily enforceable in an automated way, and ultimately still requires some discipline and vigilance from the developers. It may even feel a bit coercive to some at first, since as a result they are basically forced to maintain the documentation in order to write successful tests. The added value, however, is that said documentation is now guaranteed to be much more accurate, leading to happy consumers who will enjoy an API which is less likely to act erratically; this, in turn, should lead to less frustrated developers, who shall spend less time hunting down pesky discrepancies. All in all, making OpenAPI definitions the single source of truth for both the API documentation and the integration tests is in itself a strong incentive to keep them up to date; they naturally become a priority, where they used to be an afterthought. As for maintaining the OpenAPI definition itself, doing so manually can admittedly feel a bit daunting. Annotations are a solution, but I personally don't like them and prefer to maintain a YAML file directly. IDE extensions like this VSCode one make it much easier, but if you can't bear the sight of a YAML or JSON file, you can also use tools like Stoplight Studio to do it through a more user-friendly interface. And since we're talking about Stoplight*, this article about API Design-First vs Code First by Phil Sturgeon is a good starting point for API documentation in general, and might help you choose an approach to documenting that suits you. Resources The OpenAPI Specification The OpenAPI HttpFoundation Testing package Laravel example repository Swagger UI The L5 Swagger Laravel package OpenAPI.Tools The OpenAPI PSR-7 Message Validator package The HttpFoundation component PSR-7: HTTP message interfaces The PSR-7 Bridge PSR-7 implementation API Design-First vs Code First * I am not affiliated with them in any way","tags":"PHP","url":"https://tech.osteel.me/posts/openapi-backed-api-testing-in-php-projects-a-laravel-example"},{"title":"Dynamic GitHub profile README with Github Actions and PHP","text":"A few weeks ago, GitHub quietly released a feature that was quickly noticed by the community – profile READMEs. A profile README is a global README file for your GitHub profile, which you can set up by creating a public repository whose name is identical to your GitHub username. For instance, as my username is osteel , I created the osteel/osteel repository . A little box like this one should appear while you add your own: Once the repository is created, add a README file with a short description explaining how great you are, and your GitHub profile page will display its content by default: Neat and simple. As I was browsing examples for some inspiration, I stumbled upon Simon Willison's version , which features some dynamic content like recent work and blog publications. He explained how he used a combination of GitHub Actions and Python to achieve this in a blog post , and I decided to do something similar with PHP. The placeholder The first thing to do is to create a placeholder in the README file where the dynamic content will go. Since I wanted to automatically insert the latest publications of my blog, I used the following tags: <!-- posts --><!-- /posts --> You might recognise this format; since Markdown files also support HTML, I used some HTML comment tags to make sure they wouldn't show up on my profile page. The PHP script I can't remember the last time I wrote some PHP without a framework; as a result, I had to do a quick search just to get started with a basic PHP script and some Composer dependencies. Turn out it's quite simple! The first step is to initialise the project with the following command: $ composer init From there, I installed a lightweight library to parse my blog's RSS feed : $ composer require dg/rss-php I then added a posts.php file at the root of the project, with the following content: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <?php // Load Composer's autoload require_once __DIR__ . '/vendor/autoload.php' ; // Load the RSS feed $feed = Feed :: loadRss ( 'https://tech.osteel.me/feeds/rss.xml' ) -> toArray (); // Generate the list of blog posts $posts = '' ; foreach ( array_slice ( $feed [ 'item' ], 0 , 5 ) as $post ) { $date = date ( 'd/m/Y' , strtotime ( $post [ 'pubDate' ])); $posts .= sprintf ( \" \\n * **[%s]** [%s](%s \\\" %s \\\" )\" , $date , $post [ 'title' ], $post [ 'link' ], $post [ 'title' ]); } // Generate the new content $content = preg_replace ( '#<!-- posts -->.*<!-- /posts -->#s' , sprintf ( '<!-- posts -->%s<!-- /posts -->' , $posts ), file_get_contents ( 'README.md' ) ); // Overwrite the file file_put_contents ( 'README.md' , $content ); Nothing too complicated here – Composer's autoload is required at the top, allowing me to load the RSS parser to generate a list of blog posts as a string, in Markdown format. The existing content of the README file is then loaded into the $content variable, and the Markdown string is inserted between its <!-- posts --> and <!-- /posts --> tags with preg_replace . Finally, the file's entire content is replaced with the new one, using the file_put_contents function. The GitHub action GitHub Actions are a fairly recent addition to GitHub, allowing developers to automate various CI/CD tasks, like running test suites or deploying web services. They must be defined using YAML format in a .github/workflows folder at the root of the project, and contain a list of steps that they are to execute. Here's mine, which I named posts.yml : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 name : Update blog posts on : push : workflow_dispatch : schedule : - cron : '0 0 * * *' jobs : build : runs-on : ubuntu-latest steps : - name : Clone repository uses : actions/checkout@v2 - name : Install PHP uses : shivammathur/setup-php@v2 with : php-version : '7.4' - name : Install Composer dependencies run : composer install - name : Insert blog posts run : php posts.php - name : Push changes uses : stefanzweifel/git-auto-commit-action@v4 with : commit_message : Updated latest blog posts Again, nothing too complicated. We first give the action a name, and then define a list of events that should trigger it – pushing some code to the repository, a manual trigger from the interface ( workflow_dispatch ), or periodically like a cron job (here, every day at midnight). We then indicate that the action should run on an Ubuntu image, where it will: clone the repository ; instal PHP 7.4 ; instal the Composer dependencies; run the PHP script; commit and push the changes , if any. That's it! My GitHub profile is now automatically updated every time I publish a new article. Conclusion This was a quick experiment aiming at exploring GitHub Actions, which I expect to use more and more in the future. It was also fun to use PHP as a simple scripting language again, in a procedural way. I've voluntarily left out a few things in order to keep this article short and simple – please refer to the repository for implementation details. Resources Managing your profile README GitHub Actions This article's repository GitHub Action: Checkout V2 GitHub Action: Setup PHP GitHub Action: Git Auto Commit","tags":"PHP","url":"https://tech.osteel.me/posts/dynamic-github-profile-readme-with-github-actions-and-php"},{"title":"A simple Git pre-commit hook running PHP_CodeSniffer","text":"Git hooks are scripts that are run every time a specific Git-related event happens, like a commit or a checkout. They can be used to do a lot of different things; a common use case is to scan the code which is about to be committed in search for violations of a coding standard, so they are fixed before the code is pushed to the repository, which is way quicker than letting the build pipeline flag them (or worse, a peer reviewer, who'd rather spend their time focussing on more important issues). This is the kind of hook I was looking for for one of my projects, and since I couldn't find something simple enough I ended up writing my own. PSR-12 PHP has a few coding standards flying around, but for the past few years the ones from the PHP Framework Interop Group (PHP-FIG) seem to have gained the most traction. They currently offer three different levels: PSR-1 (Basic Coding Standard); PSR-2 (Coding Style Guide); PSR-12 (Extended Coding Style Guide). Each one builds on the previous one; PSR-12 was approved in September 2019 and supersedes PSR-2, which is now deprecated. PHP_CodeSniffer PHP_CodeSniffer is a tool composed of two different scripts: phpcs , which detects violations of a defined coding standard; phpcbf , which automatically fixes some of them. My goal was to trigger the first script at every commit, and to offer to run the second script if some violations were detected. Whether or not the second script is run, if some violations remain the commit does not happen, unless a specific option is used. Installation Download the pre-commit file from the project's GitHub repository , and put it under .git/hooks at the root of your project. Make sure it is executable: $ chmod +x pre-commit Install PHP_CodeSniffer following one of the suggested methods – I personally like to install it on a per-project basis as a Composer dependency, but it's really up to you: $ composer require --dev squizlabs/php_codesniffer Once you've done that, you can update the STANDARD variable at the top of the pre-commit file to change the coding standard, or leave PSR12 by default. The hook also assumes that PHP_CodeSniffer's bin folder is at the same level as the .git folder by default, but depending on the chosen installation that might not be the case. You can set a different location simply by updating the BIN variable. 1 2 3 4 #!/bin/bash STANDARD = \"PSR12\" BIN = \"./vendor/bin\" That's it! For other standards and options, please visit PHP_CodeSniffer's usage page . Use All you have to do now is commit some new code; PHP_CodeSniffer will analyse it, flag the violations if it finds any and ask you whether you want it to fix them for you: It will also automatically stage the files it updates, and if it cannot fix them all, it will ask you to address the remaining ones manually. If you want to commit anyway, all you have to do is to use the --no-verify option to skip the hook altogether: $ git commit --no-verify Other projects Of course Git hooks are nothing new, and there are plenty of open source projects leveraging them, for all sorts of languages and purposes . In my case I was looking for something simple since I only needed a single hook anyway, but once you've got multiple hooks, managing them can become a bit of a pain. I reviewed a number of tools and found this one that looked quite good, although I must admit I was a little confused by the concept of templates in introduces. This project also comes with a blog post explaining it in a bit more detail.","tags":"PHP","url":"https://tech.osteel.me/posts/a-simple-git-pre-commit-hook-running-php-codesniffer"},{"title":"Docker for local web development, conclusion: where to go from here","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here ⬅️ you are here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Congratulations! \"You said it wouldn't be too slow, but really, it is\" \"How do I deploy this?\" What's actually deployed Infrastructure as a Service Application files Application dependencies \"How about Kubernetes?\" It was all about the journey Special thanks Congratulations! If you've been following this series from the beginning and made it this far, you deserve a pat on the back. You're clearly committed to putting in the necessary time and effort to understand Docker, and you should be proud – this is no small feat! I hope you enjoyed going through these tutorials as much as I enjoyed writing them, and that you now clearly see how to use Docker to your advantage, and feel comfortable doing so. That being said, you might still feel like some stones are left unturned, so I will try and flip some of them in this conclusion, the same way I used the introduction to try and address some of the concerns you might have had before taking the plunge. And if that's not enough, comments are always open! \"You said it wouldn't be too slow, but really, it is\" If you feel that way, you're probably running Docker Desktop on macOS. And even though I happily run a similar setup myself, I appreciate that performance might still feel suboptimal to you, especially around script execution (e.g. Composer commands). Improvement in this regard is long overdue, as evidenced by discussions that have been going on for years now, with no clear solution on the horizon yet. You can experiment with technologies like Mutagen or docker-sync , which seem to yield some result, but I'm personally waiting for a solution that would seemlessly integrate with Docker Desktop without the need for some esoteric configuration. Another seemingly popular approach is to mix Docker services with native tools. For instance, some people advocate using Laravel Valet on macOS to run your PHP application, and to use Docker containers for all of the other services (such as MySQL and Redis). This is the philosophy behind projects like Takeout , for instance. This way, you get near-native performance for the applications you actually write code for, while still enjoying Docker's flexibility for everything else. I personally do not favour this approach, however, as it means a lot is left to the developer to set up, while what I'm trying to achieve is a unique, ready-to-use development environment that works for everyone in the team. But if you operate as a freelancer or if your company lets you deal with your working environment as you please, this could be a good solution. Then again, and as I've kept repeating throughout this series, Docker is just a tool. There are many ways in which you can use it – find one that works for you! \"How do I deploy this?\" While this tutorial series strictly focuses on building a development environment, once you get there one of the next logical questions is how you would deploy the result to a staging or production environment. I won't cover this in depth here but I can give you a few hints to get you started. What's actually deployed The first thing to mention is that I never deploy the local setup as is. While some cloud platforms support docker-compose.yml files out of the box, I normally don't use one in a production context. What usually happens is that the frontend and backend images are built and deployed separately through their own CI/CD pipelines, and the connections to whatever external services they need are configured via environment variables. For instance, in this series the application's backend uses a container based on the official MySQL image to provide a database locally, whose connection details are set in the backend's .env file. In production, it could use a managed database such as Amazon RDS instead, whose connection details would be injected as environment variables. From the backend's perspective, all that changes from one environment to the other are the values of the connection parameters; yet, the way the database is actually managed is completely different. Infrastructure as a Service Another thing to mention is that Infrastructure as a Service (IaaS) platforms will not always allow you to follow Docker's best practices, including the \"one Dockerfile = one process\" trope. For instance, Azure's App Service doesn't make it easy to manage multi-container applications (you'd basically need one App Service per image, which significantly increases the cost of your infrastructure). For a staging environment we were experimenting with, we didn't feel like paying for three different App Service instances to manage Nginx, the API and a worker separately, so we ended up bundling them up into a single image, using Supervisor to manage the corresponding processes. We achieved this by simply adding a production build stage to the API's Dockerfile. Of course the above isn't really an issue when you use something like Kubernetes , but not all teams are comfortable with it. Application files When deploying an image of your application to staging or production, that image should contain copies of your application files (including its dependencies), as mounting a local folder doesn't really apply in such an environment. Copying the files to the image is a fairly simple instruction: COPY . . The line above essentially means \"copy the content of the host's current directory (where the Dockerfile is located) to the image's working directory\". A small caveat to this is that we don't need all of the files to be copied over – some of them are not essential to the production environment (e.g. READMEs, or even the Dockerfile itself), if not downright dangerous (e.g. Git-related files ). How do we deal with this? Much like Git's .gitignore files, Docker has .dockerignore files that allow you to specify what to exclude from the build context when performing an ADD or COPY operation. I won't go into too much detail here, but this article should give you all the info you need. Application dependencies As for the application's dependencies, using our backend application as an example again, you would typically add a new stage to the Dockerfile to install the Composer packages, whose files the production stage would copy over, without keeping Composer itself (once the dependencies are installed, your production environment doesn't need it anymore). If that intermediary stage were named build and assuming there's also a base stage featuring PHP-FPM and the necessary PHP extensions only, it would look like the following: 1 2 3 4 5 6 7 8 9 10 11 FROM base as build # Copy application files COPY . . # Install application dependencies RUN composer install --no-scripts --no-autoloader --verbose --prefer-dist --no-progress --no-interaction --no-dev --no-suggest FROM base as production # Copy files from the build stage to the working directory COPY --from = build . The resulting image is the one you would push to your container registry (e.g. Docker Hub), which in turn would be pulled by whatever IaaS platform you're using. \"How about Kubernetes?\" Kubernetes is a topic on its own, but as I mentioned in the introduction , it has now established itself as the industry standard of container orchestration, which makes it more and more unavoidable when talking about Docker. It is supported by pretty much all of the major IaaS providers, which is a nice hedge against vendor lock-in and a welcome consolidation of the Docker ecosystem overall. That made me wonder about running a Kubernetes setup locally, that could be deployed as is to staging and production environments, or at least that would allow me to use the same orchestration technology across environments. I haven't come up with anything concrete yet, but tools like Minikube , Skaffold or K3s were brought to my attention, and some initiatives like the Compose Specification also seem to be aiming at bridging that gap. Potentially also related to this is Docker app , a tool that implements the Cloud Native Application Bundle (CNAB) standard , aiming at facilitating the bundling, installation and management of container-native applications (with Kubernetes support). I'm not entirely sure what to make of it just yet, but I've got the feeling it might be something interesting to follow in the near future. Their GitHub repository contains a few examples if you wish to have a gander. It is not yet clear which standard will come out on top (I would expect some convergence at some point), but once again it looks like Docker Compose is to remain at the center of it all, which I certainly identify as a good thing. It was all about the journey By now it should be clear that Docker is a technology that can be approached from a lot of different angles, which is probably why it feels so intimidating to many. Not knowing where to start is the best way not to start at all, and my aim here was merely to show you a way in. While this series is a reflection of my current knowledge of Docker, my own journey is far from finished. There is a lot more to explore as I've hinted before, and I fully expect the way I use Docker to evolve in the future. Don't let yourself be overwhelmed by the pace of Docker's evolution, though. Take your time, and don't forget to regularly tune out the noise to put into practice what you already know. There's no need to always pursue the latest fad – consolidate your current knowledge and pick up new tools only as you need them. As I said in the introduction, the reason I looked into Docker in the first place is because I started to feel hindered by Homestead. Special thanks It's a bit awkward to add this section as if I had just completed a book, but it's certainly how I felt writing this series at times. Almost six months have gone since I started working on the introduction – a lot of my free time went into this! But I haven't been doing this completely on my own – I want to send my thanks and love to my girlfriend Anna who patiently reviewed every single one of those lengthy articles (including this one), helping me sound a lot less like a non-native speaker ❤️ This post ends the Docker for web development series for now, but I'll keep writing on the subject and about web development in general in the future. You can subscribe to email alerts below or follow me on Twitter to be informed of new publications.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-conclusion-where-to-go-from-here"},{"title":"Docker for local web development, part 8: scheduled tasks","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks ⬅️ you are here Conclusion: where to go from here In this post In this series In this post Introduction Setting up the scheduler Conclusion Introduction Once we start to get comfortable around Docker and make it a full component of our development environment, inevitably there will come a time when we have to deal with some form of task scheduling. The first reflex in that case is usually to try and fit the familiar cron into the picture – and it's often surprisingly painful. In a best case scenario, we end up with a crontab that we import and install in the container via the corresponding image's Dockerfile (something like that ); more often than not though, we end up with something much clumsier. At the end of the previous part , we were left with the Laravel scheduler to run manually, which we expect to be handled as a cron entry. To address this within our setup, we could use an approach similar to the one described above – it would require a version of the image where the running process is the cron daemon instead of PHP-FPM, which could be achieved with a separate build stage. We would also need a service in docker-compose.yml , targeting the new stage in order to build and use the corresponding image. This wouldn't be too bad, and to be honest it would probably be enough in most cases. But it is not scalable, nor is it the Docker way . Imagine your application grows in complexity and you decide to add a microservice, maybe to break up a monolith, or to do something completely different using another language. Whatever this microservice does, imagine it needs to run its own tasks periodically. Following the same logic as above, you'd end up with two new services in docker-compose.yml : one to run the microservice in a regular way (e.g. as an API), and another one to manage the scheduled tasks with cron. Now imagine you need yet another microservice, also with its own cron jobs – that's another two new services to add to docker-compose.yml . Rinse and repeat – you see where this is going. One of Docker's biggest strengths in my opinion is that it enables us to think at the system level rather than the application level – instead of considering each part of the system separately, we're invited to take a step back and consider it as a whole. As scheduling tasks is a common need for many parts of a system, there must be a way to manage them at the system level, in a unified way, instead of addressing the issue for each individual part separately. The solution is to introduce an independent, system-wide scheduler. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-7 branch . If you prefer, you can also directly checkout the part-8 branch , which is the final result of this article. Setting up the scheduler There are multiple schedulers out there that we can use with Docker; they all more or less work in a similar way, but I settled on Ofelia for its simplicity. Ofelia is written in Go and allows us to quickly define tasks to be run periodically, targeting any of our setup's containers. There are various ways to define scheduled tasks with Ofelia, mostly differing in where the tasks' configuration is located. I personally favour the approach involving a config.ini file, for it doesn't require updating the other services, ensuring minimum coupling and easy replacement if necessary (another configuration approach implies changing the targeted services' definition in docker-compose.yml , which I'm less happy with). Let's see what this looks like in practice. Create a new scheduler folder under the .docker directory at the root, and add a config.ini file to it, with the following content: 1 2 3 4 [ job-exec \"Laravel Scheduler\" ] schedule = @every 1m container = demo_backend_1 command = php /var/www/backend/artisan schedule:run Here we've defined a single job – Laravel's schedule:run Artisan command – to be run every minute on the backend's container. job-exec indicates that we'll be using the already running backend container (as you may have guessed, there's also a job-run option to spin up a fresh container instead), whose name we assigned to the container key: demo_backend_1 . This is Ofelia's only small defect in my opinion: it won't guess the container's name based on the service's, and requires us to specify the running container's expected name instead. It's not really a big deal since we know the container's name is made of the project's name, the service's name and the container's number, but automatic name resolution would definitely be a plus. There are discussions about this, but it looks like it won't be implemented just yet. In any case, what's interesting to observe here is that instead of being run from inside the container, the command ends up being exposed to and run from the outside. This is how scheduling tasks in Docker should be contemplated: as some sort of internally exposed API for command line operations, centrally managed by a scheduler. We now need to define a service for that scheduler, to be added to docker-compose.yml : 1 2 3 4 5 6 7 8 # Scheduler Service scheduler : image : mcuadros/ofelia:latest volumes : - /var/run/docker.sock:/var/run/docker.sock - ./.docker/scheduler/config.ini:/etc/ofelia/config.ini depends_on : - backend Mind the fact that the config.ini file is mounted onto the container, and that the backend container is required to be started first. Believe it or not, we're pretty much done. Save the file and run demo start to download Ofelia's image and start the scheduler, before running demo logs scheduler to ensure it is functional: You should get an output similar to the above after a minute or so, and see the current time being logged in the backend's storage/logs/laravel.log file, which is the result of the execution of the job we defined in the previous part . That's it! Using this approach, the configuration of scheduled tasks is completely decoupled from the services they target. No need to rebuild your services' images to change the cron entries – just update config.ini and restart Ofelia's container. Conclusion Like I said in the introduction, the regular cron approach would probably do the trick in most cases, and running a scheduler like Ofelia instead might feel a bit like overkill. But the effort to implement the former is arguably bigger than the latter, and if you have the possibility to go for a simpler, scalable way from the get go, why would you not? Moreover, using a scheduler to run tasks across systems isn't only convenient locally – that's actually how it works in most production environments too (see for example AWS's Scheduled Tasks , or Kubernetes' CronJob ). This is the thing with Docker: it's different. If you haven't really dealt with DevOps stuff before, there is still a lot to learn, but that's all you need to do – learn. On the other hand, people who are used to dealing with VMs (locally or not) are likely to have a tougher time, because they need to unlearn part of what they know first. They will initially try to provision a container like a VM, or try to configure cron jobs like they used to – they will try to force square pegs into round holes, simply because they're used to square holes. I went through this as well. Docker is a paradigm shift, and the sooner we accept this – the sooner we let go of our old ways and embrace the Docker way – the easier it gets. Don't get me wrong though: there is substantial benefit to this trade, for enabling system-level thinking unlocks a whole new world of virtually endless possibilities. Today's article is the last of the series, but I cannot wrap this up without a proper conclusion, which will be the next and final instalment . Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-8-scheduled-tasks"},{"title":"Docker for local web development, part 7: using a multi-stage build to introduce a worker","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker ⬅️ you are here Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Introduction Installing Redis The job A proper worker Updating the initialisation script Conclusion Introduction No one likes slow websites. Pages with higher response times have higher bounce rates, which translate into lower conversion rates. When your website relies on an API, you want that API to be fast – you don't want to feel like it's having a cup of tea with your request, before insisting that the response has another butter scone prior to sending it your way. There are many ways to increase an API's responsivity, and one of them which is also the focus of today's article is the use of queues . Queues are basically to-do lists of tasks which, unlike flossing, will be completed eventually. What's important about those tasks – called jobs – is that they don't need to be performed during the lifecycle of the initial request. Typical examples of such jobs include sending a welcome email, resizing an image, or computing some statistics – whatever the task is, there's no need to make the end user wait for it to be completed. Instead, the job is placed in a queue to be dealt with later, and a response is sent immediately to the client. In other words, the job is made asynchronous , resulting in a much faster response time. Queued jobs are processed by what we call workers . Workers monitor queues and pick up jobs as they appear – they're a bit like cashiers at the supermarket, processing the content of trolleys as they come. And just like more cashiers can be called for backup when there's a sudden spike in customers, more workers can be added whenever the queues get filled up more quickly than they're emptied. Finally, queues are essentially lists of messages that need to be stored in a database, which is sometimes referred to as a message broker . Redis is an excellent choice for this, for it's super fast (in-memory storage) and it offers data structures well suited to this kind of thing. It's also very easy to set up with Docker and plays nicely with Laravel, which is why we are going to use it today. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-6 branch . If you prefer, you can also directly checkout the part-7 branch , which is the final result of this article. Installing Redis Now that all of the characters have been introduced, it's time to get into the plot. The first thing we need to do is to install the Redis extension for PHP, since it is not part of the pre-compiled ones. As this extension is a bit complicated to set up, we'll use a convenient script featured in the official PHP images' documentation , which makes it easy to install PHP extensions across Linux distributions. Replace the content of the backend's Dockerfile with this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 FROM php:8.0-fpm-alpine # Import extension installer COPY --from = mlocati/php-extension-installer /usr/bin/install-php-extensions /usr/bin/ # Install extensions RUN install-php-extensions pdo_mysql bcmath opcache redis # Install Composer COPY --from = composer:latest /usr/bin/composer /usr/local/bin/composer # Configure PHP COPY .docker/php.ini $PHP_INI_DIR /conf.d/opcache.ini # Use the default development configuration RUN mv $PHP_INI_DIR /php.ini-development $PHP_INI_DIR /php.in # Install extra packages RUN apk --no-cache add bash mysql-client mariadb-connector-c-dev Note that redis was added to the list of extensions. Build the image: $ demo build backend Our next task is to run an instance of Redis. In accordance with the principle of running a single process per container, we'll create a dedicated service in docker-compose.yml , and since the official images include an Alpine version, that's what we are going to use: 1 2 3 4 5 6 # Redis Service redis : image : redis:6-alpine command : [ \"redis-server\" , \"--appendonly\" , \"yes\" ] volumes : - redisdata:/data The image's default start-up command is redis-server with no option, but as per the documentation , it doesn't cover data persistence. In order to enable it, we need to set the appendonly option to yes , hence the command configuration setting, overriding the default one (this also shows you how to do this without using a Dockerfile). For data persistence to be fully functional, we also need a volume, to be added at the bottom of the file: # Volumes volumes: mysqldata: phpmyadmindata: redisdata: Finally, as Redis is going to be used by the backend service, we need to ensure the former is started before the latter. Update the backend service's configuration: # Backend Service backend: build: ./src/backend working_dir: /var/www/backend volumes: - ./src/backend:/var/www/backend - ./.docker/backend/init:/opt/files/init - ./.docker/nginx/certs:/usr/local/share/ca-certificates depends_on: mysql: condition: service_healthy redis: condition: service_started Save docker-compose.yml and start the project to download the new image and create the corresponding container and volume: $ demo start This will also recreate the backend container in order to use the updated image we built earlier – the one including the Redis extension. To make sure Redis is running properly, take a look at the logs: $ demo logs redis They should display something like this: There's one last thing we need to do prior to creating our job: the backend application is currently set up to run the jobs immediately, and we need to tell it to queue them using Redis instead. Open src/backend/.env , and spot the following line: QUEUE_CONNECTION = sync Replace it with these two lines: QUEUE_CONNECTION = redis REDIS_HOST = redis That's all we need here, because the other parameters' default values are already the right ones (you can find them in src/backend/config/database.php ). Monitoring Redis If you want to use an external tool to access your Redis database, you can simply update the service's configuration in docker-compose.yml and add a ports section mapping your local machine's port 6379 to the container's: ... ports: - 6379:6379 ... From there, all you need to do is configure a database connection in your software of choice, setting localhost:6379 to access the Redis database while the container is running. As pointed out by Utkarsh Vishnoi , you could also set up a new service to run Redis Commander , a bit like what we've done with phpMyAdmin. The job Laravel has built-in scaffolding tools we can use to create a job: $ demo artisan make:job Time This command will create a new Jobs folder in src/backend/app , containing a Time.php file. Open it and change the content of the handle method to this one: 1 2 3 4 5 6 7 8 9 10 11 <?php // ignore this line, it's for syntax highlighting only /** * Execute the job. * * @return void */ public function handle () { \\Log :: info ( sprintf ( 'It is %s' , date ( 'g:i a T' ))); } All the job does is log the current time. The class already has all of the necessary traits to make it queueable , so there's no need to worry about that. Laravel has a nice command scheduler we can use to define tasks that need to be executed periodically, with built-in helpers to manage queued jobs specifically. Open the src/backend/app/Console/Kernel.php file and update the schedule method: 1 2 3 4 5 6 7 8 9 10 11 12 <?php // ignore this line, it's for syntax highlighting only /** * Define the application's command schedule. * * @param \\Illuminate\\Console\\Scheduling\\Schedule $schedule * @return void */ protected function schedule ( Schedule $schedule ) { $schedule -> job ( new \\App\\Jobs\\Time ) -> everyMinute (); } We essentially ask the scheduler to dispatch the Time job every minute. It won't do that by itself though, and needs to be started via an Artisan command. But before we run it, we'll start a queue worker manually, so we can see the jobs being processed in real time. Open a new terminal window and run the following command: $ demo artisan queue:work You can now go back to the first terminal window and run the scheduler: $ demo artisan schedule:run Which should display something like this: If all went well, the other window should now show this: And if you open src/backend/storage/logs/laravel.log , you should see the new line which has been created by the job: Our queue is operational! You can now close the worker's terminal window, which will also stop it. This was just a test, however. We don't want to have to manually start the worker in a separate window every time we start our project – we need this to happen automatically. A proper worker This is where we're finally going to leverage multi-stage builds . The idea is basically to split the Dockerfile into different sections containing slightly different configurations, and which can be targeted individually to produce different images. Let's see what that means in practice. Replace the content of src/backend/Dockerfile with this one (changes highlighted in bold): FROM php:8.0-fpm-alpine as backend # Import extension installer COPY --from=mlocati/php-extension-installer /usr/bin/install-php-extensions /usr/bin/ # Install extensions RUN install-php-extensions bcmath pdo_mysql opcache redis # Install Composer COPY --from=composer:latest /usr/bin/composer /usr/local/bin/composer # Configure PHP COPY .docker/php.ini $PHP_INI_DIR/conf.d/opcache.ini # Use the default development configuration RUN mv $PHP_INI_DIR/php.ini-development $PHP_INI_DIR/php.ini # Install extra packages RUN apk --no-cache add bash mysql-client mariadb-connector-c-dev FROM backend as worker # Start worker CMD [\"php\", \"/var/www/backend/artisan\", \"queue:work\"] We now have two separate stages: backend and worker . The former is basically the original Dockerfile – we've simply named it backend using the as keyword at the very top: FROM php:8.0-fpm-alpine as backend The latter aims to describe the worker, and is based on the former: 1 2 3 4 FROM backend as worker # Start worker CMD [ \"php\" , \"/var/www/backend/artisan\" , \"queue:work\" ] All we do here is we reuse the backend stage almost as is, only overriding its default command by defining the queue:work Artisan command in its place. In other words, whenever a container is started for the worker stage, its running process will be the queue worker instead of PHP-FPM by default. How do we start such a container? We first need to define a separate service in docker-compose.yml : 1 2 3 4 5 6 7 8 9 10 # Worker Service worker : build : context : ./src/backend target : worker working_dir : /var/www/backend volumes : - ./src/backend:/var/www/backend depends_on : - backend This all looks familiar already, except for the build section which now has a couple of properties: context and target . The former points to the folder where the Dockerfile shall be found, and the latter allows us to specify which stage should be used as the base image for the service's containers. We're almost done with docker-compose.yml – we just need to update the definition of the backend service to tell it to target the backend stage: # Backend Service backend: build: context: ./src/backend target: backend working_dir: /var/www/backend volumes: - ./src/backend:/var/www/backend - ./.docker/backend/init:/opt/files/init - ./.docker/nginx/certs:/usr/local/share/ca-certificates depends_on: mysql: condition: service_healthy redis: condition: service_started Save the file and build the corresponding images: $ demo build backend $ demo build worker Start the project for the new images to be picked up: $ demo start Then run the scheduler again: $ demo artisan schedule:run If all went well, the job should be scheduled and a new line should appear in src/backend/storage/laravel.log , while the worker's container logs display a couple of new lines: $ demo logs worker Your worker is now complete! It will run silently in the background every time you start your project, ready to process any job your application throws at it. Updating the initialisation script If you've been with me from the start and are using a Bash layer to manage your setup, all that's left to do is to update the backend's initialisation script so it uses Redis for queues by default. The steps are very similar to what we did at the beginning of this article – open .docker/backend/init , and spot the following line: QUEUE_CONNECTION = sync Replace it with these two lines and save the file: QUEUE_CONNECTION = redis REDIS_HOST = redis Done! Conclusion Multi-stage builds are a powerful tool of which this is a mere introduction. Each stage can refer to a different image, basically allowing maintainers to come up with all sorts of pipeline-like builds, where the tools used at each stage are discarded to only keep the final output in the resulting image. Think about that for a minute. I also encourage you to check out these best practices , to make sure you're getting the most of your Dockerfiles. Redis can also be used in ways that go beyond a simple message broker. You could for example use it as a local cache layer right now, instead of using Laravel's array or file drivers. Finally, today's article leaves us with a couple of observations: the first one is that life is too short for flossing; the second is that so far we've been running the Laravel scheduler manually, although the documentation indicates a cron entry should be used for that. How do we fix this? In the next part of this series, we will introduce a scheduler to run tasks periodically the Docker way, without using traditional cron jobs. Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-7-using-a-multi-stage-build-to-introduce-a-worker"},{"title":"Docker for local web development, part 6: expose a local container to the Internet","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet ⬅️ you are here Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Introduction Ngrok Installing and configuring Conclusion Introduction While using Docker for local development allows us to replicate a production environment as closely as possible in a self-contained way, in some instances exposure to the outside world is unavoidable. Typical use cases include testing a third-party service's webhook (like a transaction confirmation from a payment gateway), or showing a project's advancement to a client. Sure, you could use a staging environment for that, but you might not be in a position to offer one, and in some cases it would feel akin to squashing a fly with a sledgehammer. Wouldn't it be more practical if you could make your local environment public instead? Thankfully, this isn't a new issue and several services offer to address it. This article focuses on one that's arguably become a reference over time: Ngrok. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-5 branch . If you prefer, you can also directly checkout the part-6 branch , which is the final result of today's article. Ngrok Ngrok is an online service that essentially allows developers to create secure tunnels to channel traffic from a public URL to a local address. It offers paid plans but also comes with a generous free version that doesn't even require an account for HTTP tunnels. Since our setup runs on HTTPS, however, we'll need to sign up in order to obtain an authentication token. Don't worry though, it only takes a few seconds and doesn't require credit card details. If you really don't want to open an account, there will be an information box towards the end of this tutorial explaining how to use the subscription-free plan instead. Installing and configuring There is no official Docker image for Ngrok, but the community stepped up and made a few of them available. We'll use Werner Beroux's today, for which we need to create a service in docker-compose.yml : 1 2 3 4 5 6 7 8 9 10 11 # Ngrok Service ngrok : image : wernight/ngrok:latest ports : - 4040:4040 environment : NGROK_PROTOCOL : http NGROK_PORT : nginx:443 NGROK_AUTH : ${NGROK_AUTH} depends_on : - nginx Ngrok comes with an interface served on the container's port 4040 which we mapped to localhost's, and since Nginx will once again be our entry point for the traffic, we made sure its container is started before Ngrok's using depends_on . That leaves us with a bunch of environment variables: NGROK_PROTOCOL can either be http or tcp – in our case, we need the former; NGROK_PORT is a bit misleading, as it's actually not only the port but also the host, which is nginx:443 in our case (if no host is specified, localhost is implied); NGROK_AUTH is the authentication token I mentioned earlier, whose value is another environment variable that we'll set in a minute. That's all we need for today, but you might want to have a look at the other environment variables listed in the image's documentation . We now need to set the authentication token, which you will find in your dashboard . If you remember, in the first part of this series we created a .env file at the root of the project, alongside docker-compose.yml . So far we've only used it to specify a project name (to avoid container name collisions with other projects), but that's also where we'll set the token. Open the file and change its content to this one, replacing the <YOUR TOKEN> placeholder with... your token: COMPOSE_PROJECT_NAME = demo NGROK_AUTH = <YOUR TOKEN> For good measure, we should also update .env.example , to make it clear for future users that a value is expected: COMPOSE_PROJECT_NAME = demo NGROK_AUTH = This gives you the flexibility to either set a token in .env.example for everyone to use, or to ask each developer to create their own Ngrok account and complete their local .env file accordingly. Of course, you could set the token in docker-compose.yml directly, but I wanted to show you another way to use the project's .env file. The last thing we need to do is to update the Nginx server configuration in .docker/nginx/conf.d/backend.conf : server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name backend.demo.test *.ngrok.io ; root /var/www/backend/public; ... Only the bit in bold needs to be added, the rest of the file remains as it is. Save it and restart the project in order to create the Ngrok container and to reload the Nginx configuration: $ demo restart Open localhost:4040 in your browser: Two URLs are available – spot the HTTPS one and open it in a new tab, appending /api/hello-there to the end: Our API is now available publicly! Now go back to the other tab – you should see something like this: You can now use that URL with third-party services or distribute it as much as you want, as long as the Ngrok container is running. The corresponding traffic will appear in the interface, where you can inspect it and replay requests at will. Handy! There's only one little downside: the URL will expire after 8 hours, and it will change every time the Ngrok container is restarted. That's always been fine by me as I usually need tunnels for quick tests only, but you can always look into the paid plans if that's an issue for you – they come with custom and permanent subdomains, among other things. Don't want to open an account? If you really don't want to subscribe, you can use the account-free version instead, but you will need to make the backend available on port 80. To do so, change port 443 to port 80 in the Ngrok service in docker-compose.yml and remove the NGROK_AUTH environment variable. Create a server block dedicated to *.nginx.io in the backend.conf Nginx configuration, listening on port 80 instead of port 443. Restart the project, and you should be able to use the HTTP URL provided by Ngrok. Conclusion That's it! That's all it takes to make a local container available to the Internet. This article once again demonstrates how easy it can be to leverage a piece of technology in minutes with Docker, with no prior knowledge. There's no need to worry about messing up the installation or cluttering your local setup – as long as there's a Docker image for it (and most popular technologies have one), it doesn't take much to give it a go, quickly and safely. This is also a good example of when it is OK to rely on the community, whenever the software issuer or core team hasn't provided an image yet (that is not to say that it is usually a bad idea to trust the community, simply that if there is an official image available, you're probably better off using it instead). In the next article , we'll talk about another key Docker concept: multi-stage builds. We'll use it to create a worker for our API, to listen to and consume messages from a queue. You can subscribe to email alerts below to make sure you don't miss it, or you can also follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-6-expose-a-local-container-to-the-internet"},{"title":"Docker for local web development, part 5: HTTPS all the things","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things ⬅️ you are here Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Introduction Generating the certificate Installing the certificate The Nginx server configurations Automating the process Container to container traffic Conclusion Introduction Since its inception by Netscape Communications back in 1994, Hypertext Transfer Protocol Secure (HTTPS) has been spreading over the Internet at an ever increasing rate, and now accounts for more than 80% of global traffic (as of February 2020 ). This growth in coverage has been particularly strong in the past few years, catalysed by entities like the Internet Security Research Group – the one behind the free certificate authority Let's Encrypt – and companies like Google, whose Chrome browser has flagged HTTP websites as insecure since 2018 . While it is getting ever cheaper and easier to encrypt the web, somehow this evolution doesn't extend to local environments, where bringing in HTTPS is still far from a sinecure. This article intends to ease the pain by showing you how to generate a self-signed SSL/TLS certificate and how to use it with our Docker-based setup, thus getting us one step closer to perfectly mimicking a production environment. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-4 branch . If you prefer, you can also directly checkout the part-5 branch , which is the final result of today's article. Generating the certificate We will generate the certificate and its key in a new certs folder under .docker/nginx – create that folder and add the following .gitignore file to it: 1 2 * !.gitignore These two lines mean that all of the files contained in that directory except for .gitignore will be ignored by Git (this is a nicer version of the .keep file you may sometimes encounter, which aims to version an empty folder in Git). Since one of the goals of using Docker is to avoid cluttering the local machine as much as possible, we'll use a container to install OpenSSL and generate the certificate. Nginx's is a logical choice for this – being our proxy, it will be the one receiving the encrypted traffic on port 443, before redirecting it to the right container: We need a Dockerfile for this, which we'll add under .docker/nginx : 1 2 3 4 FROM nginx:1.19-alpine # Install packages RUN apk --update --no-cache add openssl We also need to update docker-compose.yml to reference this Dockerfile and mount the certs folder onto the Nginx container, to make the certificate available to the web server. Also, since the SSL/TLS traffic uses port 443, the local machine's port 443 must be mapped to the container's (as always, changes have been highlighted in bold): # Nginx Service nginx: build: ./.docker/nginx ports: - 80:80 - 443:443 volumes: - ./src/backend:/var/www/backend - ./.docker/nginx/conf.d:/etc/nginx/conf.d - phpmyadmindata:/var/www/phpmyadmin - ./.docker/nginx/certs:/etc/nginx/certs depends_on: - backend - frontend - phpmyadmin Build the new image: $ demo build nginx All of the tools necessary to generate our certificate are now in place – we just need to add the corresponding Bash command and function. First, let's update our application menu, at the bottom of the demo file: Command line interface for the Docker-based web development environment demo. Usage: demo [options] [arguments] Available commands: artisan ................................... Run an Artisan command build [image] ............................. Build all of the images or the specified one cert ...................................... Certificate management commands generate .............................. Generate a new certificate install ............................... Install the certificate composer .................................. Run a Composer command destroy ................................... Remove the entire Docker environment down [-v] ................................. Stop and destroy all containers Options: -v .................... Destroy the volumes as well init ...................................... Initialise the Docker environment and the application logs [container] .......................... Display and tail the logs of all containers or the specified one's restart [container] ....................... Restart all containers or the specified one start ..................................... Start the containers stop ...................................... Stop the containers update .................................... Update the Docker environment yarn ...................................... Run a Yarn command To save us a trip later, I've also added the menu for the certificate installation, even if we won't implement it just yet. Add the corresponding cases to the switch: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cert ) case \" $2 \" in generate ) cert_generate ;; install ) cert_install ;; * ) cat << EOF Certificate management commands. Usage: demo cert <command> Available commands: generate .................................. Generate a new certificate install ................................... Install the certificate EOF ;; esac ;; Since there are a couple of subcommands for cert , I've also added a submenu describing them. Save the file and check out the look of the new menus: $ demo $ demo cert The second command should display something like this: Still in the demo file, add the cert_generate function: 1 2 3 4 5 # Generate a wildcard certificate cert_generate () { rm -Rf .docker/nginx/certs/demo.test.* docker compose run --rm nginx sh -c \"cd /etc/nginx/certs && touch openssl.cnf && cat /etc/ssl/openssl.cnf > openssl.cnf && echo \\\"\\\" >> openssl.cnf && echo \\\"[ SAN ]\\\" >> openssl.cnf && echo \\\"subjectAltName=DNS.1:demo.test,DNS.2:*.demo.test\\\" >> openssl.cnf && openssl req -x509 -sha256 -nodes -newkey rsa:4096 -keyout demo.test.key -out demo.test.crt -days 3650 -subj \\\"/CN=*.demo.test\\\" -config openssl.cnf -extensions SAN && rm openssl.cnf\" } The first line of the function simply gets rid of previously generated certificates and keys that may still be in the certs directory. The second line is quite long and a bit complicated, but essentially it brings up a new, single-use container based on Nginx's image ( docker compose run --rm nginx ) and runs a bunch of commands on it (that's the portion between the double quotes, after sh -c ). I won't go into the details of these, but the gist is they create a wildcard self-signed certificate for *.demo.test as well as the corresponding key. A self-signed certificate is a certificate that is not signed by a certificate authority; in practice, you wouldn't use such a certificate in production, but it is fine for a local setup. Try out the command: $ demo cert generate You should see something like this: The resulting files are generated in the container's /etc/nginx/certs folder, which, as per docker-compose.yml , corresponds to our local .docker/nginx/certs directory. If you look inside that local directory now, you will see a couple of new files – demo.test.crt and demo.test.key . Your overall file structure should now look like this: docker-tutorial/ ├── .docker/ │ ├── backend/ │ ├── mysql/ │ └── nginx/ │ ├── certs/ │ │ ├── .gitignore │ │ ├── demo.test.crt │ │ └── demo.test.key │ ├── conf.d/ │ └── Dockerfile ├── src/ ├── .env ├── .env.example ├── .gitignore ├── demo └── docker-compose.yml Installing the certificate Let's now implement the cert_install function, still in the demo file (after cert_generate ): 1 2 3 4 5 6 7 8 9 10 11 # Install the certificate cert_install () { if [[ \" $OSTYPE \" == \"darwin\" * ]] ; then sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain .docker/nginx/certs/demo.test.crt elif [[ \" $OSTYPE \" == \"linux-gnu\" ]] ; then sudo ln -s \" $( pwd ) /.docker/nginx/certs/demo.test.crt\" /usr/local/share/ca-certificates/demo.test.crt sudo update-ca-certificates else echo \"Could not install the certificate on the host machine, please do it manually\" fi } If you're on macOS or on a Debian-based Linux distribution, this function will automatically install the self-signed certificate on your machine. Unfortunately, Windows users will have to do it manually, but with the help of this tutorial the process should be fairly straightforward (you should only need to complete it roughly halfway through, up to the point where it starts talking about the Group Policy Object Editor ). Using WSL? Even if you run your project through WSL and the certificate seemingly installs properly on your Linux distribution, you're most likely still accessing the URL via a browser from Windows. For it to recognise and accept the certificate, you will need to copy the corresponding file from Linux to Windows and proceed with the manual installation as described in the aforementioned tutorial. Let's break the cert_install function down: it first verifies whether the current host system is macOS by checking the content of the pre-defined $OSTYPE environment variable, which will start with darwin if that's the case. It then adds the certificate to the trusted certificates. If the current system is Linux, the function will create a symbolic link between the certificate and the /usr/local/share/ca-certificates folder, and run update-ca-certificates so it is taken it into account. Note that this code will only work for Debian-based distributions – if you use a different one, you will need to adapt the if condition accordingly, or add extra conditions to cover more distributions. Since the sudo program is used in both cases, running the command will probably require you to enter your system account password. Let's try to install the certificate (you can also run this on Windows, but you'll be invited to install the certificate manually as mentioned earlier): $ demo cert install If all went well, it should now appear in the list of certificates, like in macOS' Keychain Access: The Nginx server configurations Now that our certificate is ready, we need to update the Nginx server configurations to enable HTTPS support. First, update the content of .docker/nginx/conf.d/backend.conf : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 server { listen 443 ssl http2 ; listen [::]:443 ssl http2 ; server_name backend.demo.test ; root /var/www/backend/public ; ssl_certificate /etc/nginx/certs/demo.test.crt ; ssl_certificate_key /etc/nginx/certs/demo.test.key ; add_header X-Frame-Options \"SAMEORIGIN\" ; add_header X-XSS-Protection \"1 ; mode=block\" ; add_header X-Content-Type-Options \"nosniff\" ; index index.html index.htm index.php ; charset utf-8 ; location / { try_files $uri $uri/ /index.php? $query_string ; } location = /favicon.ico { access_log off ; log_not_found off ; } location = /robots.txt { access_log off ; log_not_found off ; } error_page 404 /index.php ; location ~ \\.php$ { fastcgi_pass backend : 9000 ; fastcgi_index index.php ; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name ; include fastcgi_params ; } location ~ /\\.(?!well-known).* { deny all ; } } server { listen 80 ; listen [::]:80 ; server_name backend.demo.test ; return 301 https:// $server_name$request_uri ; } Then, change the content of .docker/nginx/conf.d/frontend.conf to this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 server { listen 443 ssl http2 ; listen [::]:443 ssl http2 ; server_name frontend.demo.test ; ssl_certificate /etc/nginx/certs/demo.test.crt ; ssl_certificate_key /etc/nginx/certs/demo.test.key ; location / { proxy_pass http://frontend:8080 ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection 'upgrade' ; proxy_cache_bypass $http_upgrade ; proxy_set_header Host $host ; } } server { listen 80 ; listen [::]:80 ; server_name frontend.demo.test ; return 301 https:// $server_name$request_uri ; } Finally, replace the content of .docker/nginx/conf.d/phpmyadmin.conf with the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 server { listen 443 ssl http2 ; listen [::]:443 ssl http2 ; server_name phpmyadmin.demo.test ; root /var/www/phpmyadmin ; index index.php ; ssl_certificate /etc/nginx/certs/demo.test.crt ; ssl_certificate_key /etc/nginx/certs/demo.test.key ; location ~ * \\.php $ { fastcgi_pass phpmyadmin : 9000 ; root /var/www/html ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name ; fastcgi_param SCRIPT_NAME $fastcgi_script_name ; } } server { listen 80 ; listen [::]:80 ; server_name phpmyadmin.demo.test ; return 301 https:// $server_name$request_uri ; } The principle is similar for the three of them: a second server block has been added at the end, listening to traffic on port 80 and redirecting it to port 443, which is handled by the first server block. The latter is pretty much the same as the one it replaces, except for the addition of the ssl_certificate and ssl_certificate_key configuration keys, and the appearance of http2 . We were unable to use HTTP2 so far because, while encryption is not required by the protocol, in practice most browsers only support it over an encrypted connection . By introducing HTTPS to our setup, we can now benefit from the improvements of HTTP2 . We also need to make a quick change to src/frontend/src/App.vue , where the backend endpoint should now use HTTPS instead of HTTP: ... mounted () { axios .get(' https ://backend.demo.test/api/hello-there') .then(response => (this.msg = response.data)) } ... Looks like we're ready to make a test! Restart the containers for the changes to take effect: $ demo restart Access frontend.demo.test (remember that it can take a few seconds for Vue.js' development server to start – you can run demo logs frontend to monitor what's going on): you should automatically be redirected from HTTP to HTTPS, and the website should display correctly. We're encrypted! Not working in your browser? While Chrome seems to accept self-signed certificates with no fuss across platforms, Firefox may give you a security warning. If that's the case, you may need to set the security.enterprise_roots.enabled property to true from the about:config page – once that's done, restarting the browser is usually enough to make the warning go away (read more about this configuration setting here ). Safari is proving more difficult, however; on macOS (and maybe on other systems too), even after ignoring the security warning and while the certificate is accepted for regular browser requests, AJAX requests are still failing. I haven't found a solution yet, but I haven't spent much time looking into it either because, if I'm honest, I don't really care whether or not it works on Safari (at least locally). If you find a way though, please let me know about it in the comments. Finally, if your browser still doesn't accept the certificate, I can only advise you to search for solutions on how to install a self-signed certificate for your specific setup online. Behaviour can vary based on the system, the browser and the browser's version, and it would be vain to try and list all of the potential issues here. Like I said at the beginning of this article, unfortunately, local HTTPS is not always straightforward. Automating the process Now that we've got the Bash functions to generate and install the certificate, we can integrate them into the project's initialisation process. Open the demo file again and update the init function: # Initialise the Docker environment and the application init () { env \\ && down -v \\ && build \\ && docker compose run --rm --entrypoint=\"//opt/files/init\" backend \\ && yarn install if [ ! -f .docker/nginx/certs/demo.test.crt ]; then cert_generate fi start && cert_install } The function will now check whether there's a certificate in the .docker/nginx/certs folder already, generate one if there isn't, and then proceed with starting the containers and install the certificate. In other words, all of this will now be taken care of by the initial demo init . Container to container traffic The above setup is suitable for most cases, but there's a situation where it falls short, and that is whenever a container needs to communicate with another one directly, without going through the browser. Let me walk you through this. First, bring up the project if it's currently stopped ( demo start ) and access the backend container: $ docker compose exec backend sh From there, try and ping the frontend container: $ ping frontend The ping should respond with the frontend container's private IP, which is the expected behaviour. Try pinging the frontend again, this time using the domain name: $ ping frontend.demo.test We also get a response, but from localhost , which is not quite right: we should get the same private IP address instead. Let's run a few more tests, still from the backend container, but using cURL commands: $ curl frontend Response: curl: ( 7 ) Failed to connect to frontend port 80 : Connection refused This is expected because the frontend container is set up to listen on port 8080: $ curl frontend:8080 This command correctly returns the frontend's HTML code. Let's try again, but this time using the domain name: $ curl frontend.demo.test Response: curl: ( 7 ) Failed to connect to frontend.demo.test port 80 : Connection refused Same issue as above – we should be targeting port 8080 instead: $ curl frontend.demo.test:8080 Response: curl: ( 7 ) Failed to connect to frontend.demo.test port 8080 : Connection refused Still not working... what's going on? Containers identify each other by name (e.g. frontend , backend , mysql , etc.) on the network created by Docker Compose. The domain names we defined for the frontend and the backend ( frontend.demo.test and backend.demo.test ) are recognised by our local machine because we updated its hosts file, but they have no meaning in the context of Docker Compose's network. In other words, for these domain names to be recognised on that network, we'd need to update the containers' hosts files as well, and we'd have to do it every time the containers are recreated. Thankfully, Docker Compose offers a better solution for this, in the form of network aliases . Aliases are alternative names we can give services and by which their containers will be discoverable on the network, in addition to the service's original name. These aliases can be domain names. In order to emulate a production environment as closely as possible, we should assign the frontend's domain name to the Nginx service, rather than to the frontend service directly. Things are probably getting a bit confusing, so let's bring back our diagram from earlier: This slightly updated version essentially describes what happens when we initially access frontend.demo.test : the browser asks Nginx for the frontend's content on port 443; Nginx recognises the domain name, and proxies the request to the frontend container on port 8080, which in turn returns the files for the browser to download. From then on, a copy of the frontend is running in the browser: As the end user interacts with the frontend, requests are made to the backend: These requests come to the Nginx container on port 443, where Nginx recognises the backend's domain name and proxies the requests to the backend container, on port 9000. What we're trying to achieve here, however, is direct communication between the backend and frontend containers, without involving the browser: The red route (the arrow on the right-hand side) is already functional: as the frontend and backend containers are on the same Docker Compose network, and as they can identify each other by name on it, the backend is able to reach the frontend directly on port 8080. In a production environment, however, the frontend and the backend are unlikely to be on such a network, and more likely to reach each other by domain name (I am voluntarily leaving out non-HTTP protocols here). They would basically use a similar route as the browser, through Nginx and via HTTPS – the blue route. Therefore, we want the frontend's domain name to resolve to the Nginx container and not to the frontend's directly, meaning the domain name alias should be assigned to the Nginx service. Let's add a networks section to it, in docker-compose.yml : # Nginx Service nginx: build: ./.docker/nginx ports: - 80:80 - 443:443 networks: default: aliases: - frontend.demo.test volumes: - ./src/backend:/var/www/backend - ./.docker/nginx/conf.d:/etc/nginx/conf.d - phpmyadmindata:/var/www/phpmyadmin - ./.docker/nginx/certs:/etc/nginx/certs depends_on: - backend - frontend - phpmyadmin For the change to take effect, the network has to be recreated: $ demo down && demo start We can now proceed with the same tests as earlier, starting with the ping: $ docker compose exec backend sh $ ping frontend.demo.test The command now responds with a proper private IP address. Let's try with cURL: $ curl frontend.demo.test We do get a response, but a 301 Moved Permanently one, which is expected – if you remember, we added a second server block to each Nginx config, responsible for redirecting HTTP traffic to HTTPS. Let's hit the HTTPS URL instead: $ curl https://frontend.demo.test Response: curl: ( 60 ) SSL certificate problem: self signed certificate We're now getting to the issue I mentioned at the very beginning of this section. Our browser knows and accepts the self-signed certificate, because we installed it on our local machine; on the other hand, the backend container has no idea where this certificate comes from, and has no reason to trust it. The easy way to circumvent this is by ignoring the security checks altogether: $ curl -k https://frontend.demo.test While this solution works, it is not recommended for obvious reasons, and you won't always have the luxury of setting the options as you see fit (especially if the call is made by a third-party package). What we need to do, really, is to install the certificate on the backend container as well, so it can recognise it and trust it the way our local machine does. To do that, we need to mount the directory containing the self-signed certificate onto the backend container. Exit the container (by running exit or by hitting ctrl + d ) and update docker-compose.yml : # Backend Service backend: build: ./src/backend working_dir: /var/www/backend volumes: - ./src/backend:/var/www/backend - ./.docker/backend/init:/opt/files/init - ./.docker/nginx/certs:/usr/local/share/ca-certificates depends_on: mysql: condition: service_healthy Save the file and restart the containers: $ demo restart Access the backend container once again, and install the new certificate (you can ignore the warning): $ docker compose exec backend sh $ update-ca-certificates Try the cURL command one more time: $ curl https://frontend.demo.test You should finally get the frontend's HTML code. There's one last thing we need to do before wrapping up. Update the cert_install function in the demo file: # Install the certificate cert_install () { if [[ \"$OSTYPE\" == \"darwin\"* ]]; then sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain .docker/nginx/certs/demo.test.crt elif [[ \"$OSTYPE\" == \"linux-gnu\" ]]; then sudo ln -s \"$(pwd)/.docker/nginx/certs/demo.test.crt\" /usr/local/share/ca-certificates/demo.test.crt sudo update-ca-certificates else echo \"Could not install the certificate on the host machine, please do it manually\" fi docker compose exec backend update-ca-certificates } After installing the certificate on the local machine, the function will now also do the same on the backend container. Why is this important? I must confess that the example above isn't the most relevant, as in practice I can't really think of any reason why the backend would need to interact with the frontend in such a way. Container-to-container communication is not a rare feat, however: typical use cases comprise applications querying an authentication server (think OAuth), or microservices communicating through HTTP. I simply didn't want to make this tutorial any longer by introducing another container. That being said, I would recommend installing the certificate on a container only if it's really necessary, as this is an extra step one can easily forget. If you need to recreate the container for some reason, you'd also have to remember to run demo cert install ; at the time of writing, there is no such thing as container events – like container creation – to hook on to in order to automate this. Conclusion Let's be honest: dealing with HTTPS locally is still a pain in the neck. Unfortunately, a development environment would not be complete without it, since it's pretty much become a modern Internet requirement. There is a silver lining to this, however: now that encryption is out of the way, all that's left of this tutorial series is the fun stuff. Rejoice! In the next part , we will see how to expose a local container to the Internet, which comes in handy when testing the integration of a third-party service. Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-5-https-all-the-things"},{"title":"Docker for local web development, part 4: smoothing things out with Bash","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash ⬅️ you are here Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Introduction Bash? The application menu Basic commands Initialising and updating the project Conclusion Introduction As our development environment is taking shape, the number of commands we need to remember starts to build up. Here are a few of them, as a reminder: docker compose up -d to start the containers; docker compose logs -f nginx to watch the logs of Nginx; docker compose exec backend php artisan to run Artisan commands (or with run --rm if the container isn't running); docker compose exec frontend yarn to run Yarn commands (ditto); etc. Clearly, none of the examples above is impossible to remember and, with some practice, anyone would eventually know them by heart. Yet that is a lot of text to type, repeatedly, and if you haven't used a specific command for a while, looking for the right syntax can end up taking a significant amount of time. Moreover, the scope of this tutorial series is rather limited; in practice, you're likely to deal with projects much more complex than this, requiring many more commands. There is little point in implementing an environment that ends up increasing the developer's mental load. Thankfully, there is a great tool out there that can help us mitigate this issue, one you've probably at least heard of and that is present pretty much everywhere: Bash. With little effort, Bash will allow us to add a layer on top of Docker to abstract away most of the complexity, and introduce a standardised, user-friendly interface instead. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-3 branch . If you prefer, you can also directly checkout the part-4 branch , which is the final result of today's article. Bash? Bash has been around since 1989 , meaning it's pretty much as old as the Internet as we know it. It is essentially a command processor (a shell ), executing commands either typed in a terminal or read from a file (a shell script ). Bash allows its users to automate and perform a great variety of tasks, which I am not even going to try and list. What's important to know in the context of this series, is that it can run pretty much everything a human usually types in a terminal, that it is natively present on Unix systems (Linux and macOS, and Windows via WSL 2 ). Its flexibility and portability makes it an ideal candidate for what we want to achieve today. Let's dig in! The application menu For starters, let's create a file named demo at the root of our project (alongside docker-compose.yml ) and give it execution permissions: $ touch demo $ chmod +x demo This file will contain the Bash script allowing us to interact with the application. Open it and add the following line at the very top: #!/bin/bash This is just to indicate that Bash shall be the interpreter of our script, and where to find it ( /bin/bash is the standard location on just about every Unix system, and also on Windows' Git Bash). The first thing we want to do is to create a menu for our interface, listing the available commands and how to use them. Update the content of the file with the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash case \" $1 \" in * ) cat << EOF Command line interface for the Docker-based web development environment demo. Usage: demo <command> [options] [arguments] Available commands: artisan ................................... Run an Artisan command build [image] ............................. Build all of the images or the specified one composer .................................. Run a Composer command destroy ................................... Remove the entire Docker environment down [-v] ................................. Stop and destroy all containers Options: -v .................... Destroy the volumes as well init ...................................... Initialise the Docker environment and the application logs [container] .......................... Display and tail the logs of all containers or the specified one's restart ................................... Restart the containers start ..................................... Start the containers stop ...................................... Stop the containers update .................................... Update the Docker environment yarn ...................................... Run a Yarn command EOF exit 1 ;; esac case is a basic control structure allowing us to do different things based on the value of $1 (referred to as a switch in some programming languages), $1 being the first parameter passed on to the demo script. For example, with the following command, $1 would contain the string unicorn : $ demo unicorn For now, we only address the default case, which is represented by * . In other words, if we call our script without any parameter, or one whose value is not a specific case of the switch, the menu will be displayed. We now need to make this script available from anywhere in a terminal. To do so, add the following function to your local .bashrc file (or .zshrc , or anything else according to your configuration): 1 2 3 4 function demo { cd /PATH/TO/YOUR/PROJECT && bash demo $* cd - } Wait. What? Each time you open a new terminal window, Bash will try and read the content of some files, if it can find them. These files contain commands and instructions you basically want Bash to run at start-up, such as updating the $PATH variable, running a script somewhere or, in our case, make a function available globally. Different files can be used, but to keep it simple we'll stick to updating or creating the .bashrc file in your home folder, and add the demo function above to it: $ vi ~/.bashrc From then on, everytime you open a terminal window, this file will be read and the demo function made available globally. This will work whatever your operating system is (including Windows, as long as you do this from Git Bash, or from your terminal of choice). Make sure to replace /PATH/TO/YOUR/PROJECT with the absolute path of the project's root (if you are unsure what that is, run pwd from the folder where the docker-compose.yml file sits and copy and paste the result). The function essentially changes the current directory for the project's root ( cd /PATH/TO/YOUR/PROJECT ) and executes the demo script using Bash ( bash demo ), passing on all of the command parameters to it ( $* ), which are basically all of the characters found after demo . For example, if you'd type: $ demo I am a teapot This is what the function would do behind the scenes: $ cd /PATH/TO/YOUR/PROJECT && bash demo I am a teapot The last instruction of the function ( cd - ) simply changes the current directory back to the previous one. In other words, you can run demo from anywhere – you will always be taken back to the directory the command was initially run from. Save the changes and open a new console window or source the file for them to take effect: $ source ~/.bashrc source will essentially load the content of the sourced file into the current shell, without having to restart it entirely. Let's display our menu: $ demo If all went well, you should see something similar to this: Looks fancy, doesn't it? Yet, so far none of these commands is doing anything. Let's fix this! Windows users: watch out for CRLF line endings! Unix files and Windows files use different invisible characters for line endings – the former adds the LF character only, while the latter adds both CR and LF. Your Bash script won't work with the latter, so make sure you change the file's line endings for LF if necessary. If you are not sure how to proceed, simply search for your IDE followed by \"line endings\" online – most modern text editors offer an easy way to make the switch. More generally, it is a good practice to create a .gitattributes file at the root of your project containing the line * text=auto , which will automatically convert line endings at checkout and commit (see here ). Basic commands We will start with a simple command, to give you a taste. Update the switch in the demo file so it looks like this: 1 2 3 4 5 6 7 case \" $1 \" in start ) start ;; * ) cat << EOF ... We've added the start case, in which we call the start function without any parameters. That function doesn't exist yet – at the top of the file, under #!/bin/bash , add the following code: 1 2 3 4 # Create and start the containers and volumes start () { docker compose up -d } This short function simply runs the now familiar docker compose up -d , which starts the containers in the background. Notice that we don't need to change the current directory, as when we invoke the demo function, we are automatically taken to the folder where the demo file is, which is also where docker-compose.yml resides. Save the file and try out the new command (it doesn't matter whether the containers are already running): $ demo start That's it! You can now start your project from anywhere in a console using the command above, which is much simpler to type and remember than docker compose up -d . Let's give this another go, this time to display the logs. Add another case to the structure: 1 2 3 4 5 6 7 8 9 10 case \" $1 \" in logs ) logs ;; start ) start ;; * ) cat << EOF ... And the corresponding function: 1 2 3 4 # Display and tail the logs logs () { docker compose logs -f } Try it out: $ demo logs You now have a shortcut command to access the containers' logs easily. That's nice, but how about displaying the logs of a specific container? Let's modify the case slightly: 1 2 3 4 5 6 7 8 9 10 case \" $1 \" in logs ) logs \" ${ @: 2 } \" ;; start ) start ;; * ) cat << EOF ... Instead of directly calling the logs function, we are now also passing on the script's parameters to it, starting from the second one, if any (that's the \"${@:2}\" bit). The reason is that when we type demo logs nginx , the first parameter of the script is logs , and we only want to pass on nginx to the start function. Update the logs function accordingly: 1 2 3 4 # Display and tail the logs logs () { docker compose logs -f \" ${ @: 1 } \" } Using the same syntax, we append the function's parameters to the command if any, starting from the first one ( \"${@:1}\" ) . Save the file again and give it a try: $ demo logs nginx Now that you get the principle, and as most of the other functions work in a similar fashion, here is the rest of the file, with some block comments to make it more readable: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 #!/bin/bash ####################################### # FUNCTIONS ####################################### # Run an Artisan command artisan () { docker compose run --rm backend php artisan \" ${ @: 1 } \" } # Build all of the images or the specified one build () { docker compose build \" ${ @: 1 } \" } # Run a Composer command composer () { docker compose run --rm backend composer \" ${ @: 1 } \" } # Remove the entire Docker environment destroy () { read -p \"This will delete containers, volumes and images. Are you sure? [y/N]: \" -r if [[ ! $REPLY = ~ &#94; [ Yy ] $ ]] ; then exit ; fi docker compose down -v --rmi all --remove-orphans } # Stop and destroy all containers down () { docker compose down \" ${ @: 1 } \" } # Display and tail the logs of all containers or the specified one's logs () { docker compose logs -f \" ${ @: 1 } \" } # Restart the containers restart () { stop && start } # Start the containers start () { docker compose up -d } # Stop the containers stop () { docker compose stop } # Run a Yarn command yarn () { docker compose run --rm frontend yarn \" ${ @: 1 } \" } ####################################### # MENU ####################################### case \" $1 \" in artisan ) artisan \" ${ @: 2 } \" ;; build ) build \" ${ @: 2 } \" ;; composer ) composer \" ${ @: 2 } \" ;; destroy ) destroy ;; down ) down \" ${ @: 2 } \" ;; logs ) logs \" ${ @: 2 } \" ;; restart ) restart ;; start ) start ;; stop ) stop ;; yarn ) yarn \" ${ @: 2 } \" ;; * ) cat << EOF Command line interface for the Docker-based web development environment demo. Usage: demo <command> [options] [arguments] Available commands: artisan ................................... Run an Artisan command build [image] ............................. Build all of the images or the specified one composer .................................. Run a Composer command destroy ................................... Remove the entire Docker environment down [-v] ................................. Stop and destroy all containers Options: -v .................... Destroy the volumes as well init ...................................... Initialise the Docker environment and the application logs [container] .......................... Display and tail the logs of all containers or the specified one's restart ................................... Restart the containers start ..................................... Start the containers stop ...................................... Stop the containers update .................................... Update the Docker environment yarn ...................................... Run a Yarn command EOF exit ;; esac Mind the fact that run --rm is used to execute Artisan, Composer and Yarn commands on the backend and frontend containers respectively, basically allowing us to do so whether the containers are running or not. Also, you'll notice that the restart command is essentially a shortcut for demo stop followed by demo start , even though docker compose restart is also an option. The reason for that is that the latter is only truly useful to restart the containers' processes (e.g. Nginx, so it picks up a server configuration's changes, for instance). But say you've updated and rebuilt an image: running docker compose restart won't recreate the corresponding container based on the image's new version, but reuse the old container instead. Essentially, in most cases stopping and starting ( up ) the containers is more likely to achieve the desired effect than a simple docker compose restart , even if it takes a few more seconds. Finally, as the destroy function's job is to delete all of the containers, volumes and images, it would be quite a pain to run it by mistake, so I made it failsafe by adding a confirmation prompt. Most of the commands are now covered, but you might have noticed that a couple of them are still missing: init and update . These are a bit special, so the next section is dedicated to them. Initialising and updating the project Let's take a step back for a minute. Imagine you've been given access to the project's repository in order to install it on your machine. The first thing you'd do is to clone it locally, and to add the demo function to .bashrc so you can interact with the application. From there, you would still need to perform the following actions: Copy .env.example to .env and complete the latter at the root of the project; Do the same in src/backend ; Download and build the images; Install the frontend's dependencies; Install the backend's dependencies; Run the backend's database migrations; Generate the backend's application key; Start the containers. While the Bash layer facilitates going through that list, that's still quite some work to do in order to obtain a functional setup, and it would be easy to miss a step. What if you need to reset the environment? Or to install it on another developer's machine? Or guide a client through the process? Thankfully, now that we've introduced Bash to the mix, automating the tasks above is fairly simple. First, add the two missing cases to demo : 1 2 3 4 5 6 init ) init ;; update ) update ;; And the corresponding functions (if your file is getting messy, you can also take a look at the final result here ): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Create .env from .env.example env () { if [ ! -f .env ] ; then cp .env.example .env fi } # Initialise the Docker environment and the application init () { env \\ && down -v \\ && build \\ && docker compose run --rm --entrypoint = \"//opt/files/init\" backend \\ && yarn install \\ && start } # Update the Docker environment update () { git pull \\ && build \\ && composer install \\ && artisan migrate \\ && yarn install \\ && start } Let's have a look at init first, whose job is to initialise the whole project. The first thing the function does is to call another function, env , which we defined right above it, and which is responsible for creating the .env file as a copy of .env.example if it doesn't exist. Next thing init does is ensuring the containers and volumes are destroyed, as the aim is to be able to both initialise the project from scratch and reset it. It then builds the images (which will also be downloaded if necessary), and goes on with running some sort of script on the backend container. Finally, it installs the frontend's dependencies and starts the containers. You will probably recognise most of the items from the list at the beginning of this section, but what are this init file and that entrypoint we are referring to? Since the backend requires a bit more work, we can isolate the corresponding steps into a single script that we will mount on the container in order to run it there. This means we need to make a small addition to the backend service in docker-compose.yml : # Backend Service backend: build: ./src/backend working_dir: /var/www/backend volumes: - ./src/backend:/var/www/backend - ./.docker/backend/init:/opt/files/init depends_on: mysql: condition: service_healthy Since the script will be run on the container, we need Bash to be installed on it, which is not the case by default on Alpine. We need to update the backend's Dockerfile accordingly (in src/backend ): FROM php:8.0-fpm-alpine # Install extensions RUN docker-php-ext-install pdo_mysql bcmath opcache # Install Composer COPY --from=composer:latest /usr/bin/composer /usr/local/bin/composer # Configure PHP COPY .docker/php.ini $PHP_INI_DIR/conf.d/opcache.ini # Use the default development configuration RUN mv $PHP_INI_DIR/php.ini-development $PHP_INI_DIR/php.ini # Install Bash RUN apk --no-cache add bash Build the image: $ demo build backend Finally, let's create the init file, in the .docker/backend folder: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash # Install Composer dependencies composer install -d \"/var/www/backend\" # Deal with the .env file if necessary if [ ! -f \"/var/www/backend/.env\" ] ; then # Create .env file cat > \"/var/www/backend/.env\" << EOF APP_NAME=demo APP_ENV=local APP_KEY= APP_DEBUG=true APP_URL=http://backend.demo.test LOG_CHANNEL=single DB_CONNECTION=mysql DB_HOST=mysql DB_PORT=3306 DB_DATABASE=demo DB_USERNAME=root DB_PASSWORD=root BROADCAST_DRIVER=log CACHE_DRIVER=file QUEUE_CONNECTION=sync SESSION_DRIVER=file EOF # Generate application key php \"/var/www/backend/artisan\" key:generate --ansi fi # Database php \"/var/www/backend/artisan\" migrate Let's break this down a bit. First, we install the Composer dependencies, specifying the folder to run composer install from with the -d option. We then check whether there's already a .env file, and if there isn't, we create one with some pre-configured settings matching our Docker setup. Notice that we leave the APP_KEY environment variable empty; that is why we run the command to generate the Laravel application key right after creating the .env file. We then move on to setting up the database. Just like the demo file at the beginning of this article, we need to make the init file executable: $ chmod +x .docker/backend/init By setting it as the entrypoint for the container we invoke, it will be the first and only script to be run before the container is destroyed. You can try the command out straight away, regardless of the current state of your project: $ demo init At this point though, you've probably taken care of most of the steps covered by the script already (e.g. generating the .env files or installing the dependencies). If you wish to test the complete process, you can run demo destroy , and either delete the entire project and/or start afresh by cloning your own repository or this one (in the latter case, checkout the part-4 branch), without forgetting to update the function in .bashrc if the path has changed. Then run demo init again. I personally find the experience of seeing the whole project setting itself up incredibly satisfactory, but maybe that's just me. Why the double slash? You might have noticed that, in the init function, the path to the init file is preceded with a double slash: ... docker compose run --rm --entrypoint=\" //opt/files/init \" backend ... This is not a typo. For some reason, when there's only one slash Windows will prepend the current local path to the script's, consequently complaining that the file cannot be found (duh). Adding another slash prevents that behaviour, while being ignored on other platforms. That leaves us with the update function, whose job is to make sure our environment is up to date: 1 2 3 4 5 6 7 8 9 # Update the Docker environment update () { git pull \\ && build \\ && composer install \\ && artisan migrate \\ && yarn install \\ && start } This is a convenience method that will pull the repository, build the images in case the Dockerfiles have changed, make sure any change of dependency is applied and new migrations are run, and restart the containers that need it (i.e. whose image has changed). Managing separate repositories As I mentioned before, in a regular setup you are more likely to have the Docker environment, the backend application and the frontend application in separate repositories. Bash can also help in this situation: assuming the src folder is git-ignored and the code is hosted on GitHub, a function pulling the applications' repositories could look like this: # Clone or update the repositories repositories () { repos=(frontend backend) cd src for repo in \"${repos[@]}\"; do git clone \"git@github.com:username/${repo}.git\" \"$repo\" || (cd \"$repo\" ; git pull ; cd ..) || true done cd .. } Conclusion The aim of this series is to build a flexible environment that makes our lives easier. That means the user experience must be as slick as possible, and having to remember dozens of complicated commands doesn't quite fit the bill. Bash is a simple yet powerful tool that, when combined with Docker, makes for a great developer experience. After today's article, it will be much simpler to interact with our environment, and if you happen to forget a command, a refresher is now just one demo away. I kept things as simple as possible to avoid cluttering the post, but there is obviously much more you can get out of this duo. Many more commands can be simplified – just tailor the layer to your needs. In the next part of this series, we will see how to generate a self-signed SSL/TLS certificate in order to bring HTTPS to our environment. Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-4-smoothing-things-out-with-bash"},{"title":"Docker for local web development, part 3: a three-tier architecture with frameworks","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks ⬅️ you are here Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Foreword A three-tier architecture? The backend application OPcache The frontend application Conclusion Foreword In all honesty, what we've covered so far is pretty standard. Articles about LEMP stacks on Docker are legion, and while I hope to add some value through a beginner-friendly approach and a certain level of detail, there was hardly anything new (after all, I was already writing about this back in 2015 ). I believe this is about to change with today's article. There are many ways to manage a multitiered project with Docker, and while the approach I am about to describe certainly isn't the only one, I also think this is a subject that doesn't get much coverage at all. In that sense, today's article is probably where the rubber meets the road for some of you. That is not to say the previous ones are negligible – they constitute a necessary introduction contributing to making this series comprehensive – but this is where the theory meets the practical complexity of modern web applications. The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the repository's part-2 branch . If you prefer, you can also directly checkout the part-3 branch , which is the final result of today's article. Again, this is by no means the one and only approach, just one that has been successful for me and the companies I set it up for. A three-tier architecture? After setting up a LEMP stack on Docker and shrinking down the size of the images , we are about to complement our MySQL database with a frontend application based on Vue.js and a backend application based on Laravel , in order to form what we call a three-tier architecture . Behind this somewhat intimidating term is a popular way to structure an application, consisting in separating the presentation layer (a.k.a. the frontend), the application layer (a.k.a. the backend) and the persistence layer (a.k.a. the database), ensuring each part is independently maintainable, deployable, scalable, and easily replaceable if need be. Each of these layers represents one tier of the three-tier architecture. And that's it! In such a setup, it is common for the backend and frontend applications to each have their own repository. For simplicity's sake, however, we'll stick to a single repository in this tutorial. The backend application Before anything else, let's get rid of the previous containers and volumes (not the images, as we still need them) by running the following command from the project's root directory: $ docker compose down -v Remember that down destroys the containers, and -v deletes the associated volumes. Let's also get rid of the previous PHP-related files, to make room for the new backend application. Delete the .docker/php folder, the .docker/nginx/conf.d/php.conf file and the src/index.php file. Your file and directory structure should now look similar to this: docker-tutorial/ ├── .docker/ │ ├── mysql/ │ │ └── my.cnf │ └── nginx/ │ └── conf.d/ │ └── phpmyadmin.conf ├── src/ ├── .env ├── .env.example ├── .gitignore └── docker-compose.yml Replace the content of docker-compose.yml with this one (changes have been highlighted in bold): version: '3.8' # Services services: # Nginx Service nginx: image: nginx:1.19-alpine ports: - 80:80 volumes: - ./src/backend:/var/www/backend - ./.docker/nginx/conf.d:/etc/nginx/conf.d - phpmyadmindata:/var/www/phpmyadmin depends_on: - backend - phpmyadmin # Backend Service backend: build: ./src/backend working_dir: /var/www/backend volumes: - ./src/backend:/var/www/backend depends_on: mysql: condition: service_healthy # MySQL Service mysql: image: mysql:8 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: demo volumes: - ./.docker/mysql/my.cnf:/etc/mysql/conf.d/my.cnf - mysqldata:/var/lib/mysql healthcheck: test: mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD interval: 5s retries: 10 # PhpMyAdmin Service phpmyadmin: image: phpmyadmin/phpmyadmin:5-fpm-alpine environment: PMA_HOST: mysql volumes: - phpmyadmindata:/var/www/html depends_on: mysql: condition: service_healthy # Volumes volumes: mysqldata: phpmyadmindata: The main update is the removal of the PHP service in favour of the backend service, although they are very similar. The new service looks for a Dockerfile located in the backend application's directory ( src/backend ), which is mounted as a volume on the container. As the backend application will be built with Laravel, let's create an Nginx server configuration based on the one provided in the official documentation . Create a new backend.conf file in .docker/nginx/conf.d : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 server { listen 80 ; listen [::]:80 ; server_name backend.demo.test ; root /var/www/backend/public ; add_header X-Frame-Options \"SAMEORIGIN\" ; add_header X-XSS-Protection \"1 ; mode=block\" ; add_header X-Content-Type-Options \"nosniff\" ; index index.html index.htm index.php ; charset utf-8 ; location / { try_files $uri $uri/ /index.php? $query_string ; } location = /favicon.ico { access_log off ; log_not_found off ; } location = /robots.txt { access_log off ; log_not_found off ; } error_page 404 /index.php ; location ~ \\.php$ { fastcgi_pass backend : 9000 ; fastcgi_index index.php ; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name ; include fastcgi_params ; } location ~ /\\.(?!well-known).* { deny all ; } } Mind the values for server_name and fastcgi_pass (now pointing to port 9000 of the backend container). Why not use Laravel Sail? If you're already a Laravel developer, you might have come across Sail , Laravel's official Docker-based development environment. While their approach is similar to this series' in many ways, the main difference is that Sail is meant to cater for Laravel monoliths only. In other words, Sail is meant for applications that are fully encompassed within Laravel, as opposed to applications where the Laravel bit is only one component among others, as is the case in this series. If you're curious about Laravel Sail, I've published a comprehensive guide that will teach you everything you need to know. You also need to update your local hosts file with the new domain names (have a quick look here if you've forgotten how to do that): 127 .0.0.1 backend.demo.test frontend.demo.test phpmyadmin.demo.test Note that we also added the frontend's domain to save us a trip later, and that we changed phpMyAdmin's from phpmyadmin.test to phpmyadmin.demo.test for consistency. You should open the phpmyadmin.conf file in the .docker/nginx/conf.d folder now and update the following line accordingly: server_name phpmyadmin.demo.test Now create the src/backend directory and add a Dockerfile with this content: FROM php:8.0-fpm-alpine Your file structure should now look like this: docker-tutorial/ ├── .docker/ │ ├── mysql/ │ │ └── my.cnf │ └── nginx/ │ └── conf.d/ │ └── backend.conf │ └── phpmyadmin.conf ├── src/ │ └── backend/ │ └── Dockerfile ├── .env ├── .env.example ├── .gitignore └── docker-compose.yml Laravel requires a few PHP extensions to function properly, so we need to ensure those are installed. The Alpine version of the PHP image comes with a number of pre-installed extensions which we can list by running the following command (from the project's root, as usual): $ docker compose run --rm backend php -m Now if you remember, in the first part of this series we used exec to run Bash on a container, whereas this time we are using run to execute the command we need. What's the difference? exec simply allows us to execute a command on an already running container, whereas run does so on a new container which is immediately stopped after the command is over. It does not delete the container by default, however; we need to specify --rm after run for it to happen. The command essentially runs php -m on the backend container, and gives the following result: [ PHP Modules ] Core ctype curl date dom fileinfo filter ftp hash iconv json libxml mbstring mysqlnd openssl pcre PDO pdo_sqlite Phar posix readline Reflection session SimpleXML sodium SPL sqlite3 standard tokenizer xml xmlreader xmlwriter zlib [ Zend Modules ] That is quite a lot of extensions, which might come as a surprise after reading the previous part praising Alpine images for featuring the bare minimum by default. The reason is also hinted at in the previous article: since it is not always simple to install things on an Alpine distribution, the PHP image's maintainers chose to make their users' lives easier by preinstalling a bunch of extensions. The final result is around 80 MB, which is still very small. Now that we know which extensions are missing, we can complete the Dockerfile to install them, along with Composer which is needed for Laravel: 1 2 3 4 5 6 7 FROM php:8.0-fpm-alpine # Install extensions RUN docker-php-ext-install pdo_mysql bcmath # Install Composer COPY --from = composer:latest /usr/bin/composer /usr/local/bin/composer Here we're leveraging multi-stage builds (more specifically, using an external image as a stage ) to get Composer's latest version from the Composer Docker image directly (we'll talk more about multi-stage builds in part 7 ). Updating the Dockerfile means we need to rebuild the image: $ docker compose build backend Once this is done, run the following command: $ docker compose run --rm backend composer create-project --prefer-dist laravel/laravel tmp \"8.*\" This will use the version of Composer installed on the backend container (no need to install Composer locally!) to create a new Laravel 8 project in the container's /var/www/backend/tmp folder. As per docker-compose.yml , the container's working directory is /var/www/backend , onto which the local folder src/backend was mounted – if you look into that directory now on your local machine, you will find a new tmp folder containing the files of a fresh Laravel application. But why did we not create the project in backend directly? Behind the scenes, composer create-project performs a git clone , and that won't work if the target directory is not empty. In our case, backend already contains the Dockerfile, which is necessary to run the command in the first place. We essentially created the tmp folder as a temporary home for our project, and we now need to move back the files to their final location: $ docker compose run --rm backend sh -c \"mv -n tmp/.* ./ && mv tmp/* ./ && rm -Rf tmp\" This will run the content between the double quotes on the container, sh -c basically being a trick allowing us to run more than a single command at once (if we ran docker compose run --rm backend mv -n tmp/.* ./ && mv tmp/* ./ && rm -Rf tmp instead, only the first mv instruction would be executed on the container, and the rest would be run on the local machine). Shouldn't Composer be in its own container? A popular way to deal with package managers is to isolate them into their own containers, the main reason being that they are external tools mostly used during development and that they've got no business shipping with the application itself (which is all correct). I actually used this approach for a while, but it comes with downsides that are often overlooked: as Composer allows developers to specify a package's requirements (necessary PHP extensions, PHP's minimum version, etc.), by default it will check the system on which it installs the application's dependencies to make sure it meets those criteria. In practice, this means the configuration of the container hosting Composer must be as close as possible to the application's, which often means doubling the work for the maintainer. As a result, some people choose to run Composer with the --ignore-platform-reqs flag instead, ensuring dependencies will always install regardless of the system's configuration. This is a dangerous thing to do: while most of the time dependency-related errors will be spotted during development, in some instances the problem could go unnoticed until someone stumbles upon it, either on staging or even in production (this is especially true if your application does't have full test coverage). Moreover, staged builds are an effective way to separate the package manager from the application in a single Dockerfile, but that's a topic I will broach later in this series. Bear with! By default, Laravel has created a .env file for you, but let's replace its content with this one (you will find this file under src/backend ): APP_NAME = demo APP_ENV = local APP_KEY = base64:BcvoJ6dNU/I32Hg8M8IUc4M5UhGiqPKoZQFR804cEq8 = APP_DEBUG = true APP_URL = http://backend.demo.test LOG_CHANNEL = single DB_CONNECTION = mysql DB_HOST = mysql DB_PORT = 3306 DB_DATABASE = demo DB_USERNAME = root DB_PASSWORD = root BROADCAST_DRIVER = log CACHE_DRIVER = file QUEUE_CONNECTION = sync SESSION_DRIVER = file Not much to see here, apart from the database configuration (mind the value of DB_HOST ) and some standard application settings. Let's try out our new setup: $ docker compose up -d Once it is up, visit backend.demo.test ; if all went well, you should see Laravel's home page: Pump up that RAM! If the backend's response time feels slow, a quick and easy trick for Docker Desktop is to increase the amount of RAM it is allowed to use. Open the preferences and adjust the Memory slider under Resources : The default value is 2 GB; I doubled that and it made a notable difference (if you've got 8 GB or more, it will most likely not impact your machine's performance in any visible way). There are other things you could do to try and speed things up on macOS (e.g. Mutagen ) but they feel a bit hacky and I'm personally not a big fan of them. If PHP is your language of choice though, make sure to check out the OPcache section below. And if you're a Windows user and haven't given WSL 2 a try yet, you should probably look into it. Let's also verify our database settings by running Laravel's default migrations: $ docker compose exec backend php artisan migrate You can log into phpmyadmin.demo.test (with the root / root credentials) to confirm the presence of the demo database and its newly created tables ( failed_jobs , migrations , password_resets and users ). There is one last thing we need to do before moving on to the frontend application: since our aim is to have it interact with the backend, let's add the following endpoint to routes/api.php : 1 2 3 4 5 <?php // ignore this line, it's for syntax highlighting only Route :: get ( '/hello-there' , function () { return 'General Kenobi' ; }); Try it out by accessing backend.demo.test/api/hello-there , which should display \"General Kenobi\". Our API is ready! We are done for this section but, if you wish to experiment further, while the backend's container is up you can run Artisan and Composer commands like this: $ docker compose exec backend php artisan $ docker compose exec backend composer And if it's not running: $ docker compose run --rm backend php artisan $ docker compose run --rm backend composer Using Xdebug? I don't use it myself, but at this point you might want to add Xdebug to your setup. I won't cover it in detail because this is too PHP-specific, but this tutorial will show you how to make it work with Docker and Docker Compose. OPcache You can skip this section if PHP is not the language you intend to use in the backend. If it is though, I strongly recommend you follow these steps because OPcache is a game changer when it comes to local performance, especially on macOS (but it will also improve your experience on other operating systems). I won't explain it in detail here and will simply quote the official PHP documentation : OPcache improves PHP performance by storing precompiled script bytecode in shared memory, thereby removing the need for PHP to load and parse scripts on each request. It's not enabled by default but we can easily do so by following a few steps borrowed from this article (which I invite you to read for more details on the various parameters). First, we need to introduce a custom configuration for PHP. Create a new .docker folder in src/backend , and add a php.ini file to it with the following content: 1 2 3 4 5 6 7 8 9 [opcache] opcache.enable = 1 opcache.revalidate_freq = 0 opcache.validate_timestamps = 1 opcache.max_accelerated_files = 10000 opcache.memory_consumption = 192 opcache.max_wasted_percentage = 10 opcache.interned_strings_buffer = 16 opcache.fast_shutdown = 1 We place this file here and not at the very root of the project because this configuration is specific to the backend application, and we need to reference it from its Dockerfile. The Dockerfile is indeed where we can import this configuration from, and also where we will install the opcache extension. Replace its content with the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 FROM php:8.0-fpm-alpine # Install extensions RUN docker-php-ext-install pdo_mysql bcmath opcache # Install Composer COPY --from = composer:latest /usr/bin/composer /usr/local/bin/composer # Configure PHP COPY .docker/php.ini $PHP_INI_DIR /conf.d/opcache.ini # Use the default development configuration RUN mv $PHP_INI_DIR /php.ini-development $PHP_INI_DIR /php.ini We're simply copying the php.ini file over to the directory where custom configurations are expected to go in the container, and whose location is given by the $PHP_INI_DIR environment variable. And since we're here, we can also use the default development settings provided by the image's maintainers, which sets up error reporting parameters, among other things (that's what the last instruction is for). And that's it! Build the image again and restart the containers – you should notice some improvement around the backend's responsiveness: $ docker compose build backend $ docker compose up -d The frontend application The third tier of our architecture is the frontend application, for which we will use the ever more popular Vue.js . The steps to set it up are actually quite similar to the backend's; first, let's add the corresponding service to docker-compose.yml , right after the backend one: 1 2 3 4 5 6 7 8 # Frontend Service frontend : build : ./src/frontend working_dir : /var/www/frontend volumes : - ./src/frontend:/var/www/frontend depends_on : - backend Nothing that we haven't seen before here. We also need to specify in the Nginx service that the frontend service should be started first: # Nginx Service nginx: image: nginx:1.19-alpine ports: - 80:80 volumes: - ./src/backend:/var/www/backend - ./.docker/nginx/conf.d:/etc/nginx/conf.d - phpmyadmindata:/var/www/phpmyadmin depends_on: - backend - phpmyadmin - frontend Then, create a new frontend folder under src and add the following Dockerfile to it: FROM node:15-alpine We simply pull the Alpine version of Node.js' official image for now, which ships with both Yarn and npm (which are package managers like Composer, but for JavaScript). I will be using Yarn, as I am told this is what the cool kids use nowadays. Let's build the image: $ docker compose build frontend Once the image is ready, create a fresh Vue.js project with the following command: $ docker compose run --rm frontend sh -c \"yarn global add @vue/cli && vue create tmp --default --force\" By using the same sh -c trick as earlier in order to run multiple commands at once on the container, we install Vue CLI ( yarn global add @vue/cli ) and use it straight away to create a new Vue.js project with some default presets, in the tmp directory ( vue create tmp --default --force ). This directory is located under /var/www/frontend , which is the container's working directory as per docker-compose.yml . We don't install Vue CLI via the Dockerfile here, because the only use we have for it is to create the project. Once that's done, there's no need to keep it around. Just like the backend, let's move the files out of tmp and back to the parent directory: $ docker compose run --rm frontend sh -c \"mv -n tmp/.* ./ && mv tmp/* ./ && rm -Rf tmp\" If all went well, you will find the application's files under src/frontend on your local machine. Issues around node_modules ? A few readers have reported symlink issues related to the node_modules folder on Windows. To be honest I am not entirely sure what is going on here, but it seems that creating a .dockerignore file at the root of the project containing the line node_modules fixes it (you might need to delete that folder first). Again, not sure why is that, but you might find answers here . .dockerignore files are very similar to .gitignore files in that they allow us to specify files and folders that should be ignored when copying or adding content to a container. They are not really necessary in the context of this series and I only touch upon them in the conclusion , but you can read more about them here . We've already added frontend.demo.test to the hosts file earlier, so let's move on to creating the Nginx server configuration. Add a new frontend.conf file to .docker/nginx/conf.d , with the following content (most of the location block comes from this article ): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 80 ; listen [::]:80 ; server_name frontend.demo.test ; location / { proxy_pass http://frontend:8080 ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection 'upgrade' ; proxy_cache_bypass $http_upgrade ; proxy_set_header Host $host ; } } This simple configuration redirects the traffic from the domain name's port 80 to the container's port 8080, which is the port Vue.js uses for its development server . Let's also add a new vue.config.js file in src/frontend : 1 2 3 4 5 6 7 8 9 10 11 module . exports = { devServer : { disableHostCheck : true , sockHost : 'frontend.demo.test' , watchOptions : { ignored : /node_modules/ , aggregateTimeout : 300 , poll : 1000 , } }, }; This will ensure the hot-reload feature is functional. I won't go into details about each option here, as the point is not so much about configuring Vue.js as seeing how to articulate a frontend and a backend application with Docker, regardless of the chosen frameworks. You are welcome to look them up though! Let's complete our Dockerfile by adding the command that will start the development server: 1 2 3 4 FROM node:15-alpine # Start application CMD [ \"yarn\" , \"serve\" ] Rebuild the image: $ docker compose build frontend And start the project so Docker picks up the image changes (for some reason, the restart command won't do that): $ docker compose up -d Once everything is up and running, access frontend.demo.test and you should see Vue.js' welcome page: Give it a minute if it doesn't show up immediately, as the server takes some time to start. If need be, you can monitor it with the following command: $ docker compose logs -f frontend Open src/frontend/src/components/HelloWorld.vue and update some content (one of the <h3> tags, for example). Go back to your browser and you should see the change happen in real time: this is hot-reload doing its magic! To make sure our setup is complete, all we've got left to do is to query the API endpoint we defined earlier in the backend, with the help of Axios . Let's install the package with the following command: $ docker compose exec frontend yarn add axios Once Yarn is done, replace the content of src/frontend/src/App.vue with this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 < template > < div id = \"app\" > < HelloThere :msg = \"msg\" /> </ div > </ template > < script > import axios from 'axios' import HelloThere from './components/HelloThere.vue' export default { name : 'App' , components : { HelloThere }, data () { return { msg : null } }, mounted () { axios . get ( 'http://backend.demo.test/api/hello-there' ) . then ( response => ( this . msg = response . data )) } } </ script > All we are doing here is hitting the hello-there endpoint we created earlier and assigning its response to the msg property, which is passed on to the HelloThere component. Once again I won't linger too much on this, as this is not a Vue.js tutorial – I merely use it as an example. Delete src/frontend/src/components/HelloWorld.vue , and create a new HelloThere.vue file in its place: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 < template > < div > < img src = \"https://tech.osteel.me/images/2020/03/04/hello.gif\" alt = \"Hello there\" class = \"center\" > < p > {{ msg }} </ p > </ div > </ template > < script > export default { name : 'HelloThere' , props : { msg : String } } </ script > < style > p { font-family : \"Arial\" , sans-serif ; font-size : 90 px ; text-align : center ; font-weight : bold ; } . center { display : block ; margin-left : auto ; margin-right : auto ; width : 50 % ; } </ style > The component contains a little bit of HTML and CSS code, and displays the value of msg in a <p> tag. Save the file and go back to your browser: the content of our API endpoint's response should now display at the bottom of the page. If you want to experiment further, while the frontend's container is up you can run Yarn commands like this: $ docker compose exec frontend yarn And if it's not running: $ docker compose run --rm frontend yarn Using separate repositories As I mentioned at the beginning of this article, in a real-life situation the frontend and backend applications are likely to be in their own repositories, and the Docker environment in a third one. How to articulate the three of them? The way I do it is by adding the src folder to the .gitignore file at the root, and I checkout both the frontend and backend applications in it, in separate directories. And that's about it! Since src is git-ignored , you can safely checkout other codebases in it, without conflicts. Theoretically, you could also use Git submodules to achieve this, but in practice it adds little to no value, especially when applying what we'll cover in the next part. Conclusion That was another long one, well done if you made it this far! This article once again underscores the fact that, when it comes to building such an environment, a lot is left to the maintainer's discretion. There is seldom any clear way of doing things with Docker, which is both a strength and a weakness – a somewhat overwhelming flexibility. These little detours contribute to making these articles dense, but I think it is important for you to know that you are allowed to question the way things are done. On the same note, you might also start to wonder about the practicality of such an environment, with the numerous commands and syntaxes one needs to remember to navigate it properly. And you would be right. That is why the next article will be about using Bash to abstract away some of that complexity, to introduce a nicer, more user-friendly interface in its place. You can subscribe to email alerts below to make sure you don't miss it, or you can also follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-3-a-three-tier-architecture-with-frameworks"},{"title":"Docker for local web development, part 2: put your images on a diet","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet ⬅️ you are here Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here In this post In this series In this post Getting started \"I'm not fat, I'm big boned\" Alpine Linux When not to use Alpine Conclusion Getting started The assumed starting point of this tutorial is where we left things at the end of the previous part , corresponding to the part-1 branch of the repository. If you prefer, you can also directly checkout part-2 , which is the final result of today's article. \"I'm not fat, I'm big boned\" In part 1 of this series, we went through the steps of creating a simple but functional LEMP stack running on Docker and orchestrated by Docker Compose, resulting in four containers running simultaneously. These containers are based on images that were downloaded from Docker Hub , each of these images having a different weight. But how much space are we talking about? Let's find out with this simple command, to be run from the root of our project: $ docker compose images It will display a table containing the images used by the application and some information about them, including their weight: The total amounts to roughly 1.5 GB, which is not light. Why is that? Most Linux distributions come with many services that are expected to cover common use cases; they offer a large amount of programs, intended to address a broad audience whose needs may evolve over time. On the other hand, Docker containers are supposed to run a single process, meaning what they need to perform their jobs usually doesn't amount to much, and is unlikely to change over time. By using standard Linux distributions, we embark a lot of tools and services we don't always need, unnecessarily increasing the size of the images in the process. In turn, this has an impact on performance, security and, sometimes, the cost of deployment. Is there anything we can do about it? Alpine Linux Alpine is a Linux distribution that takes the opposite approach: focused on security and with a small footprint, it features the bare minimum by default and lets you install what you actually need for your application. The dockerised version of Alpine is as small as 4 MB, and most official Docker images provide a version based on this distribution. Before we modify our setup, let's get rid of the current one: $ docker compose down -v --rmi all --remove-orphans This command will stop and/or destroy the containers, as well as remove the volumes and images, allowing us to start afresh. Replace the content of docker-compose.yml with this one (changes have been highlighted in bold): version: '3.8' # Services services: # Nginx Service nginx: image: nginx:1.19-alpine ports: - 80:80 volumes: - ./src:/var/www/php - ./.docker/nginx/conf.d:/etc/nginx/conf.d - phpmyadmindata:/var/www/phpmyadmin depends_on: - php - phpmyadmin # PHP Service php: build: ./.docker/php working_dir: /var/www/php volumes: - ./src:/var/www/php depends_on: mysql: condition: service_healthy # MySQL Service mysql: image: mysql:8 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: demo volumes: - ./.docker/mysql/my.cnf:/etc/mysql/conf.d/my.cnf - mysqldata:/var/lib/mysql healthcheck: test: mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD interval: 5s retries: 10 # PhpMyAdmin Service phpmyadmin: image: phpmyadmin/phpmyadmin:5-fpm-alpine environment: PMA_HOST: mysql volumes: - phpmyadmindata:/var/www/html depends_on: mysql: condition: service_healthy # Volumes volumes: mysqldata: phpmyadmindata: Let's break this down. On the Nginx side, we simply appended -alpine to the image tag, to pull the Alpine-based version (remember that the available versions of an image are listed on Docker Hub ). We also mounted a new named volume phpmyadmindata (declared at the bottom of the file) and used depends_on to indicate that the phpMyAdmin container should be started first. The reason is that Nginx will now be serving phpMyAdmin as well as our PHP application, where previously the phpMyAdmin image featured its own HTTP server (Apache). As its name suggests, the 5-fpm-alpine tag is the Alpine-based version of the image, whose container runs PHP-FPM as a process and expects PHP files to be handled by an external HTTP server. Where to find help? Using an external HTTP server for phpMyAdmin is actually not documented, and I had to dig up some GitHub issue to put me on the right track. This is a good example of where to find help whenever official documentations fall short: browsing GitHub issues is usually a good place to start, as someone is likely to have stumbled upon the same problem before. I also sometimes find it helpful to have a look at the image's Dockerfile, as it is easier to use an image once we understand how it is built. As a result, we need an Nginx configuration for phpMyAdmin. Let's create a new phpmyadmin.conf file in .docker/nginx/conf.d , alongside php.conf : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 server { listen 80 ; listen [::]:80 ; server_name phpmyadmin.test ; root /var/www/phpmyadmin ; index index.php ; location ~ * \\.php $ { fastcgi_pass phpmyadmin : 9000 ; root /var/www/html ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name ; fastcgi_param SCRIPT_NAME $fastcgi_script_name ; } } Again, a pretty standard server configuration that looks a lot like php.conf , except that it points to port 9000 of the phpMyAdmin service. You also need to update your local hosts file to add the new domain name (have a quick look here if you've forgotten how to do that): 127 .0.0.1 php.test phpmyadmin.test Back to docker-compose.yml : similar to our PHP application, the phpmyadmindata volume ensures the phpMyAdmin files are available to Nginx, the only difference being that instead of mounting a local folder of our choice (e.g. src ), we let Docker Compose pick a local folder to mount both onto the Nginx and phpMyAdmin containers, effectively making the latter's content available to the former. Finally, we also got rid of the mapping of port 8080, since we will now be using Nginx's port 80 directly. Next in line is PHP. If you followed the previous part , you already know that we use a Dockerfile located in .docker/php to describe and build our image. Replace its content with this one: 1 2 3 FROM php:8.0-fpm-alpine RUN docker-php-ext-install pdo_mysql Just like Nginx, the only difference is we appended -alpine at the end of the image tag to get the Alpine-based version instead. That leaves us with the MySQL service, which hasn't changed at all. The reason is that at the time of writing, there is simply no available Alpine version for the MySQL image, for reasons laid out in this GitHub issue . We are now ready to test out our new setup. Run the now familiar docker compose up -d again, followed by docker compose images : The total size of our images now amounts to around 700 MB, which is less than half the initial weight. And, as a bonus, you can now access phpMyAdmin at phpmyadmin.test , instead of localhost:8080 . When not to use Alpine As often, however, there is no silver bullet. Alpine is great as long as what you need is available from the official package repository (which is well-stocked, to be fair, unlike bathroom hygiene aisles at the moment), but if something is missing you might be in for some fun in order to add it manually. Unless you are well versed in system administration, you probably don't want to go there. So how to pick the right version of an image? A good approach would be to start with the most minimal available version and move up the footprint ladder in case of lack of dependency support only. Although nothing is ever set in stone and you can always change the base image later, the more complex a Dockerfile, the more painful it can get to port it to a different Linux distribution. This is why I'm introducing Alpine so early on: instead of going for the first available image without a second thought, consider your options and identify what seems to be the best compromise – it will most likely save you some headaches down the line. Also remember that, beyond sheer size considerations, the smaller the image, the smaller the potential attack surface . Conclusion We now have a better idea of how to pick a base image for our containers, and we optimised our LEMP stack as a result. This is a good place to upgrade our setup to a more complex three-tier architecture and to introduce application frameworks, which we will cover in the next part . You can subscribe to email alerts below to make sure you don't miss it, or you can also follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-2-put-your-images-on-a-diet"},{"title":"Docker for local web development, part 1: a basic LEMP stack","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack ⬅️ you are here Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post The first steps Identifying the necessary containers Docker Compose Nginx PHP MySQL phpMyAdmin Domain name Environment variables Commands summary and cleaning up your environment Conclusion The first steps I trust you've already read the introduction to this series and are now ready for some action. The first thing to do is to head over to the Docker website and download and install Docker Desktop for Mac or PC, or head over here for installation instructions on various Linux distributions. If you're on Windows, make sure to install Windows Subsystem for Linux (WSL 2) , and to configure Docker Desktop to use it . The second thing you will need is a terminal. Once both requirements are covered, you can either get the final result from the repository and follow this tutorial, or start from scratch and compare your code to the repository's whenever you get stuck. The latter is my recommended approach for Docker beginners, as the various concepts are more likely to stick if you write the code yourself. Note that this post is quite dense because of the large number of notions being introduced. I assume no prior knowledge of Docker and I try not to leave any detail unexplained. If you are a complete beginner, make sure you have some time ahead of you and grab yourself a hot drink: we're taking the scenic route. Identifying the necessary containers Docker recommends running only one process per container, which roughly means that each container should be running a single piece of software. Let's remind ourselves what the programs underlying the LEMP stack are: L is for Linux; E is for Nginx; M is for MySQL; P is for PHP. Linux is the operating system Docker runs on, so that leaves us with Nginx, MySQL and PHP. For convenience, we will also add phpMyAdmin into the mix. As a result, we now need the following containers: one container for Nginx; one container for PHP (PHP-FPM); one container for MySQL; one container for phpMyAdmin. This is fairly straightforward, but how do we get from here to setting up these containers, and how will they interact with each other? Docker Compose Docker Desktop comes with a tool called Docker Compose that allows you to define and run multi-container Docker applications (if your system runs on Linux, you will need to install it separately ). Docker Compose isn't absolutely necessary to manage multiple containers, as doing so can be achieved with Docker alone, but in practice it is very inconvenient to do so (it would be similar to doing long division while there is a calculator on the desk: while it is certainly not a bad skill to have, it is also a tremendous waste of time). The containers are described in a YAML configuration file and Docker Compose will take care of building the images and starting the containers, as well as some other useful things like automatically connecting the containers to an internal network. Don't worry if you feel a little confused; by the end of this post it will all make sense. Nginx The YAML configuration file will actually be our starting point: open your favourite text editor and add a new docker-compose.yml file to a directory of your choice on your local machine (your computer), with the following content: 1 2 3 4 5 6 7 8 9 10 version : '3.8' # Services services : # Nginx Service nginx : image : nginx:1.19 ports : - 80:80 The version key at the top of the file indicates the version of Docker Compose we intend to use (3.8 is the latest version at the time of writing). It is followed by the services key, which is a list of the application's components. For the moment we only have the nginx service, with a couple of keys: image and ports . The former indicates which image to use to build our service's container; in our case, version 1.19 of the Nginx image . Open the link in a new tab: it will take you to Docker Hub, which is the largest registry for container images (think of it as the Packagist or PyPI of Docker). Why not use the latest tag? You will probably notice that all images have a latest tag corresponding to the most up-to-date version of the image. While it might be tempting to use it, you don't know how the image will evolve in the future – it is very likely that breaking changes will be introduced sooner or later. The same way you do a version freeze for an application's dependencies (via composer.lock for PHP or requirements.txt in Python, for example), using a specific version tag ensures your Docker setup won't break due to unforeseen changes. Much like a Github repository, image descriptions on Docker Hub usually do a good job at explaining how to use it and what the available versions are. Here, we are looking at Nginx's official image: Docker keeps a curated list of \"official\" images (sometimes maintained by upstream developers, but not always), which I always use whenever possible. They are easily recognisable: their page mentions Docker Official Images at the top, and Docker Hub separates them clearly from the community images when doing a search: Note the \"Verified Content\" at the top Back to docker-compose.yml : under ports , 80:80 indicates that we want to map our local machine's port 80 (used by HTTP) to the container's. In other words, when we will access port 80 on our local machine (i.e. your computer), we will be forwarded to the port 80 of the Nginx container. Let's test this out. Save the docker-compose.yml file, open a terminal and change the current directory to your project's before running the following command: $ docker compose up -d It might take a little while as the Nginx image will first be downloaded from Docker Hub. When it is done, open localhost in your browser, which should display Nginx's welcome page: Congratulations! You have just created your first Docker container. Let's break down that command: by running docker compose up -d , we essentially asked Docker Compose to build and start the containers described in docker-compose.yml ; the -d option indicates that we want to run the containers in the background and get our terminal back. You can see which containers are currently running by executing the following command: $ docker compose ps Which should display something similar to this: To stop the containers, simply run: $ docker compose stop At this point, you might be wondering what the difference is between a service, an image and a container. A service is just one of your application's components, as listed in docker-compose.yml . Each service refers to an image, which is used to start and stop containers based on this image. To help you grasp the nuance, think of an image as a class, and of a container as an instance of that class. Speaking of OOP, how about we set up PHP? PHP By the end of this section, we will have Nginx serving a simple index.php file via PHP-FPM , which is the most widely used process manager for PHP. Not a PHP fan? As mentioned in the introduction , while PHP is used on the server side throughout this series, swapping it for another language should be fairly straightforward. Replace the content of docker-compose.yml with this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 version : '3.8' # Services services : # Nginx Service nginx : image : nginx:1.19 ports : - 80:80 volumes : - ./src:/var/www/php - ./.docker/nginx/conf.d:/etc/nginx/conf.d depends_on : - php # PHP Service php : image : php:8.0-fpm working_dir : /var/www/php volumes : - ./src:/var/www/php A few things going on here: let's forget about the Nginx service for a moment, and focus on the new PHP service instead. We start from the php:8.0-fpm image, corresponding to the tag 8.0-fpm of PHP's official image , featuring version 8.0 and PHP-FPM. Let's skip working_dir for now, and have a look at volumes . This section allows us to define volumes (basically, directories or single files) that we want to mount onto the container. This essentially means we can map local directories and files to directories and files on the container; in our case, we want Docker Compose to mount the src folder as the container's /var/www/php folder. What's in the src/ folder? Nothing yet, but that's where we are going to place our application code. Once it is mounted onto the container, any change we make to our code will be immediately available, without the need to restart the container. Create the src directory (at the same level as docker-compose.yml ) and add the following index.php file to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <!DOCTYPE html> < html > < head > < meta charset = \"UTF-8\" > < title > Hello there </ title > < style > . center { display : block ; margin-left : auto ; margin-right : auto ; width : 50 % ; } </ style > </ head > < body > < img src = \"https://tech.osteel.me/images/2020/03/04/hello.gif\" alt = \"Hello there\" class = \"center\" > </ body > </ html > It only contains a little bit of HTML and CSS, but all we need for now is to make sure PHP files are correctly served. Back to the Nginx service: we added a volumes section to it as well, where we mount the directory containing our code just like we did for the PHP service (this is so Nginx gets a copy of index.php , without which it would return a 404 Not Found when trying to access the file), and this time we also want to import the Nginx server configuration that will point to our application code: - ./.docker/nginx/conf.d:/etc/nginx/conf.d As Nginx automatically reads files ending with .conf located in the /etc/nginx/conf.d directory, by mounting our own local conf.d directory in its place we make sure the configuration files it contains will be processed by Nginx on the container. Create the .docker/nginx/conf.d folder and add the following php.conf file to it: 1 2 3 4 5 6 7 8 9 10 11 12 13 server { listen 80 default_server ; listen [::]:80 default_server ; root /var/www/php ; index index.php ; location ~ * \\.php $ { fastcgi_pass php : 9000 ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name ; fastcgi_param SCRIPT_NAME $fastcgi_script_name ; } } Note Placing Docker-related files under a .docker folder is a common practice. This is a minimalist PHP-FPM server configuration borrowed from Linode's website that doesn't have much to it; simply notice we point the root to /var/www/php , which is the directory onto which we mount our application code in both our Nginx and PHP containers, and that we set the index to index.php . The following line is also interesting: fastcgi_pass php : 9000 ; It tells Nginx to forward requests for PHP files to the PHP container's port 9000, which is the default port PHP-FPM listens on. Internally, Docker Compose will automatically resolve the php keyword to whatever private IP address it assigned to the PHP container. This is another great feature of Docker Compose: at start-up, it will automatically set up an internal network on which each container is discoverable via its service's name. A word on networks Docker Compose sets up a network with the bridge driver by default, but you can also specify the networks. I've personally never used any other network than the default one, but you can read about other options here . Finally, let's have a look at the last configuration section of the Nginx service: 1 2 depends_on : - php Sometimes, the order in which Docker Compose starts the containers matters. As we want Nginx to forward PHP requests to the PHP container's port 9000, the following error might occur if Nginx happens to be ready before PHP: [ emerg ] 1 #1: host not found in upstream \"php\" in /etc/nginx/conf.d/php.conf:7 nginx_1 | nginx: [ emerg ] host not found in upstream \"php\" in /etc/nginx/conf.d/php.conf:7 nginx_1 exited with code 1 This causes the Nginx process to stop, and as the Nginx container will only run for as long as the Nginx process is up, the container stops as well. The depends_on configuration ensures the PHP container will start before the Nginx one, saving us an embarrassing situation. Your directory and file structure should now look similar to this: docker-tutorial/ ├── .docker/ │ └── nginx/ │ └── conf.d/ │ └── php.conf ├── src/ │ └── index.php └── docker-compose.yml We are ready for another test. Go back to your terminal and run the same command again (this time, the PHP image will be downloaded): $ docker compose up -d Refresh localhost : if everything went well you will be greeted by the man who can disappear in a bathrobe. Update index.php (modify the content of the <title> tag, for instance) and reload the page: the change should appear immediately. If you run docker compose ps you will observe that you now have two containers running: nginx_1 and php_1 . Let's inspect the PHP container: $ docker compose exec php bash By running this command, we ask Docker Compose to execute Bash on the PHP container. You should get a new prompt indicating that you are currently under /var/www/php : this is what the working_directory configuration we ran into earlier is for. Run a simple ls to list the content of the directory: you should see index.php , which is expected as we mounted our local src folder onto the container's /var/www/php folder. Run exit to leave the container. Before we move on to the next section, let me show you one last trick. Go back to your terminal and run the following command: $ docker compose logs -f Wait for a few logs to display, and hit the return key a few times to add some empty lines. Refresh localhost again and take another look at your terminal, which should have printed some new lines: This command aggregates the logs of every container, which is extremely useful for debugging: if anything goes wrong, your first reflex should always be to look at the logs. It is also possible to display the information of a specific container simply by appending the name of the service (e.g. docker compose logs -f nginx ). Hit ctrl+c to get your terminal back. MySQL The last key component of our LEMP stack is MySQL. Let's update docker-compose.yml again: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 version : '3.8' # Services services : # Nginx Service nginx : image : nginx:1.19 ports : - 80:80 volumes : - ./src:/var/www/php - ./.docker/nginx/conf.d:/etc/nginx/conf.d depends_on : - php # PHP Service php : build : ./.docker/php working_dir : /var/www/php volumes : - ./src:/var/www/php depends_on : mysql : condition : service_healthy # MySQL Service mysql : image : mysql:8 environment : MYSQL_ROOT_PASSWORD : root MYSQL_DATABASE : demo volumes : - ./.docker/mysql/my.cnf:/etc/mysql/conf.d/my.cnf - mysqldata:/var/lib/mysql healthcheck : test : mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD interval : 5s retries : 10 # Volumes volumes : mysqldata : The Nginx service is still the same, but the PHP one was slightly updated. We are already familiar with depends_on : this time, we indicate that the new MySQL service should be started before PHP. The other difference is the presence of the condition option; but before I explain it all, let's take a look at the new build section of the PHP service, which seemingly replaced the image one. Instead of using the official PHP image as is, we tell Docker Compose to use the Dockerfile from .docker/php to build a new image. A Dockerfile is like the recipe to build an image: every image has one, even official ones (see for instance Nginx's ). Create the .docker/php folder and add a file named Dockerfile to it, with the following content: 1 2 3 FROM php:8.0-fpm RUN docker-php-ext-install pdo_mysql PHP needs the pdo_mysql extension in order to read from a MySQL database. Although it doesn't come with the official image, the Docker Hub description provides some instructions to install PHP extensions easily. At the top of our Dockerfile, we indicate that we start from the official image, and we proceed with installing pdo_mysql with a RUN command. And that's it! Next time we start our containers, Docker Compose will pick up the changes and build a new image based on the recipe we gave it. A lot more can be done with a Dockerfile, and while this is a very basic example some more advanced use cases will be covered in subsequent articles. For the time being, let's update index.php to leverage the new extension: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 <!DOCTYPE html> < html > < head > < meta charset = \"UTF-8\" > < title > Hello there </ title > < style > body { font-family : \"Arial\" , sans-serif ; font-size : larger ; } . center { display : block ; margin-left : auto ; margin-right : auto ; width : 50 % ; } </ style > </ head > < body > < img src = \"https://tech.osteel.me/images/2020/03/04/hello.gif\" alt = \"Hello there\" class = \"center\" > <?php $connection = new PDO('mysql:host=mysql;dbname=demo;charset=utf8', 'root', 'root'); $query = $connection->query(\"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA = 'demo'\"); $tables = $query->fetchAll(PDO::FETCH_COLUMN); if (empty($tables)) { echo '<p class=\"center\">There are no tables in database <code>demo</code>.</p>'; } else { echo '<p class=\"center\">Database <code>demo</code> contains the following tables:</p>'; echo '<ul class=\"center\">'; foreach ($tables as $table) { echo \"<li>{$table}</li>\"; } echo '</ul>'; } ?> </ body > </ html > The main change is the addition of a few lines of PHP code to connect to a database that does not exist yet. Let's now have a closer look at the MySQL service in docker-compose.yml : 1 2 3 4 5 6 7 8 9 10 11 12 13 # MySQL Service mysql : image : mysql:8 environment : MYSQL_ROOT_PASSWORD : root MYSQL_DATABASE : demo volumes : - ./.docker/mysql/my.cnf:/etc/mysql/conf.d/my.cnf - mysqldata:/var/lib/mysql healthcheck : test : mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD interval : 5s retries : 10 The image section points to MySQL's official image for version 8, and it is followed by a section we haven't come across yet: environment . It contains a couple of keys, MYSQL_ROOT_PASSWORD and MYSQL_DATABASE , which are environment variables that will be set on the container upon creation. They are documented in the image's description and essentially allow us to set the root password and create a default database respectively. In other words, a demo database will automatically be created for us when the container starts. After the environment key is the now familiar volumes . The first volume is a configuration file we will be using to set the character set to utf8mb4_unicode_ci by default, which is pretty standard nowadays. Create the .docker/mysql folder and add the following my.cnf file to it: 1 2 3 [ mysqld ] collation-server = utf8mb4_unicode_ci character-set-server = utf8mb4 Password plugin error? Some older versions of PHP are incompatible with MySQL's new default password plugin introduced with version 8. If you require such a version, you might also need to add the following line to the configuration file: default-authentication-plugin = mysql_native_password If the containers are already running, destroy them as well as the volumes with docker compose down -v and run docker compose up -d again. The second volume looks a bit different than what we have seen so far: instead of pointing to a local folder, it refers to a named volume defined in a whole new volumes section which sits at the same level as services : 1 2 3 4 # Volumes volumes : mysqldata : We need such a volume because without it, every time the mysql service container is destroyed the database is destroyed with it. To make it persistent, we basically tell the MySQL container to use the mysqldata volume to store the data locally, local being the default driver (just like networks, volumes come with various drivers and options which you can learn about here ). As a result, a local directory is mounted onto the container, the difference being that instead of specifying which one, we let Docker Compose pick a location. The last section is a new one: healthcheck . It allows us to specify on which condition a container is ready , as opposed to just started . In this case, it is not enough to start the MySQL container – we also want to create the database before the PHP container tries to access it. In other words, without this heath check the PHP container might try to access the database even though it doesn't exist yet, causing connection errors. This is what these lines in the PHP service description were about: 1 2 3 depends_on : mysql : condition : service_healthy By default, depends_on will just wait for the referenced containers to be started, unless we specify otherwise. This health check might not work on the first attempt, however; that's why we set it up to retry every 5 seconds up to 10 times, using the interval and retries keys respectively. The health check itself uses mysqladmin , a MySQL server administration program, to ping the server until it gets a response. It does so using the root user and the value set in the MYSQL_ROOT_PASSWORD environment variable as the password (which also happens to be root in our case). Go back to your terminal and run docker compose up -d again. Once it is done downloading the MySQL image and all of the containers are up and running, refresh localhost . You should see this: We now have Nginx serving PHP files that can connect to a MySQL database, meaning our LEMP stack is pretty much complete. The next steps are about improving our setup, starting with seeing how we can interact with the database in a user-friendly way. phpMyAdmin When it comes to dealing with a MySQL database, phpMyAdmin remains a popular choice; conveniently, they provide a Docker image which is pretty straightforward to set up. Not using phpMyAdmin? If you are used to some other tool like Sequel Ace or MySQL Workbench , you can simply update the MySQL configuration in docker-compose.yml and add a ports section mapping your local machine's port 3306 to the container's: ... ports: - 3306:3306 ... From there, all you need to do is configure a database connection in your software of choice, setting localhost:3306 as the host and root , root as login and password to access the MySQL database while the container is running. If you choose to do the above, you can skip this section altogether and move on to the next one. Open docker-compose.yml one last time and add the following service configuration after MySQL's: 1 2 3 4 5 6 7 8 9 10 # PhpMyAdmin Service phpmyadmin : image : phpmyadmin/phpmyadmin:5 ports : - 8080:80 environment : PMA_HOST : mysql depends_on : mysql : condition : service_healthy We start from version 5 of the image and we map the local machine's port 8080 to the container's port 80. We indicate that the MySQL container should be started and ready first with depends_on , and set the host that phpMyAdmin should connect to using the PMA_HOST environment variable (remember that Docker Compose will automatically resolve mysql to the private IP address it assigned to the container). Save the changes and run docker compose up -d again. The image will be downloaded, then, once everything is up, visit localhost:8080 : Enter root / root as username and password, create a couple of tables under the demo database and refresh localhost to confirm they are correctly listed. And that's it! That one was easy, right? Let's move on to setting up a proper domain name for our application. Domain name We have come a long way already and all that's left for today mostly boils down to polishing up our setup. While accessing localhost is functional, it is not particularly user friendly. Replace the content of .docker/nginx/conf.d/php.conf with this one: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 80 ; listen [::]:80 ; server_name php.test ; root /var/www/php ; index index.php ; location ~ * \\.php $ { fastcgi_pass php : 9000 ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name ; fastcgi_param SCRIPT_NAME $fastcgi_script_name ; } } We essentially removed default_server (since the server will now be identified by a domain name) and added the server_name configuration, giving it the value php.test , which will be our application's address. There is one extra step we need to take for this to work: as php.test is not a real domain name (it is not registered anywhere), you need to edit your local machine's hosts file so it recognises it. Where to find the hosts file? On UNIX-based systems (essentially Linux distributions and macOS), it is located at /etc/hosts . On Windows, it should be located at c:\\windows\\system32\\drivers\\etc\\hosts . You will need to edit it as administrator ( this tutorial should help if you are unsure how to do that). Add the following line to your hosts file and save it: 127 .0.0.1 php.test Since we haven't updated docker-compose.yml nor any Dockerfile, this time a simple docker compose up -d won't be enough for Docker Compose to pick up the changes. We need to explicitly tell it to restart the containers so the Nginx process is restarted and the new configuration is taken into account: $ docker compose restart Your application is now available at php.test , as well as localhost . Environment variables We are almost there, folks! The last thing I want to show you today is how to set environment variables for the whole Docker Compose project, rather than for a specific service like we have been doing so far (using the environment section in docker-compose.yml ). Before we do that, I would like you to list the current containers: $ docker compose ps Notice how each container is prefixed by the name of your project directory (which would be docker-tutorial if you cloned the repository ): Now, before we proceed further, let's destroy our containers and volumes so we can start afresh: $ docker compose down -v Create a .env file alongside docker-compose.yml , with the following content: COMPOSE_PROJECT_NAME = demo Save the file and run docker compose up -d again, followed by docker compose ps : each container is now prefixed with demo_ . Why is this important? By assigning a unique name to your project, you ensure that no name collision will happen with other ones. If there are multiple Docker-based projects on your system that share the same name or directory name, and more than one use a service called nginx , Docker may complain that another container named xxx_nginx already exists when you bring up a Docker environment. While this might not seem essential, it is an easy way to avoid potential hassle in the future, and provides some consistency across the team. Speaking of which: if you've dealt with .env files before, you probably know that they are not supposed to be versioned and pushed to a code repository. Assuming you are using Git, you should add .env to a .gitignore file, and create a .env.example file that will be shared with your coworkers. Here is what the final directory and file structure should look like: docker-tutorial/ ├── .docker/ │ ├── mysql/ │ │ └── my.cnf │ ├── nginx/ │ │ └── conf.d/ │ │ └── php.conf │ └── php/ │ └── Dockerfile ├── src/ │ └── index.php ├── .env ├── .env.example ├── .gitignore └── docker-compose.yml That is the extent to which we need environment variables for this article, but you can read more about them over here . Commands summary and cleaning up your environment Before we wrap up, I'd like to summarise all of the commands we have been using so far, and throw a few more in so you can clean up your environment if you wish to. This can be used as a reference you can easily come back to if need be, especially in the beginning. Remember that they need to be run from your project's directory. Start and run the containers in the background $ docker compose up -d If you update docker-compose.yml , an image or a Dockerfile, running this command again will pick up the changes automatically. Restart the containers $ docker compose restart Useful when some changes require a process to restart, e.g. restart Nginx to pick up some server configuration changes. List the containers $ docker compose ps Tail the containers' logs $ docker compose logs [ service ] Replace [service] with a service name (e.g. nginx ) to display this service's logs only. Stop the containers $ docker compose stop Stop and/or destroy the containers $ docker compose down Stop and/or destroy the containers and their volumes (including named volumes) $ docker compose down -v Delete everything, including images and orphan containers $ docker compose down -v --rmi all --remove-orphans Orphan containers are left behind containers that used to match a Docker Compose service but are now not connected to anything, which sometimes happens while you're building your Docker setup. Conclusion Here is a summary of what we have covered today: what Docker Compose is; what the difference between a service, an image and a container is; how to search for images on Docker Hub; what running a single process per container means; how to split our application into different containers accordingly; how to describe services in a docker-compose.yml file; what a Dockerfile is; how to declare and use volumes; how Docker Compose makes containers discoverable on an internal network; how to assign a domain name to our application; how to set environment variables; a bunch of useful commands. That is an awful lot to digest. Congratulations if you made it this far, that must have been a real effort. The good news is that the next posts will be lighter, and the result of this one can already be used as a decent starting point for any web project. Don't worry if you feel a little bit confused or overwhelmed, that is perfectly normal. Docker is a strong case for practice makes perfect : it is only by using it regularly that its concepts eventually click. In the next part of this series, we will see how to choose and shrink the size of our images. Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack"},{"title":"Docker for local web development, introduction: why should you care?","text":"In this series Introduction: why should you care? ⬅️ you are here Part 1: a basic LEMP stack Part 2: put your images on a diet Part 3: a three-tier architecture with frameworks Part 4: smoothing things out with Bash Part 5: HTTPS all the things Part 6: expose a local container to the Internet Part 7: using a multi-stage build to introduce a worker Part 8: scheduled tasks Conclusion: where to go from here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Who this series is for \"Why should I listen to you?\" \"I already use Vagrant/Homestead\" \"How about [insert favourite Docker-based solution]?\" \"Serverless is the way to go\" \"Performance is terrible\" \"It is not my job to deal with Docker\" It's all about the journey Who this series is for Among developers, exposure to Docker ranges from having vaguely heard of the technology to using it on a daily basis, the latter category singing its praises while the former is sometimes still struggling with the sheer concept of containers. Wherever you are on your journey, as a developer there are many reasons why you might want to delve into this technology, including, but not limited to: \"I read a few things about Docker and I am now looking for something more hands-on\" \"I use a Vagrant-based solution like Homestead and it starts to show its limits\" \"I already use a turnkey solution like Laradock and I want to understand how things work under the hood\" \"I want greater control over my development environment\" \"I want to better understand application architecture\" \"My current project uses microservices\" Whatever your motive is, you might notice the emergence of a theme, revolving around understanding and being in control . In my opinion, Docker is about empowering the developer, from providing a safe environment to try out technologies easily – without messing up your local environment – to opening the gates to the world of Ops. The aim of this tutorial series is to demystify a technology that can feel daunting at times, all the while providing most of the tools a developer might need to build a web project locally with Docker. It strictly focuses on the development environment and does not cover deploying Docker applications whatsoever , even though the end result can provide the foundation for a deployable setup. No prior knowledge of Docker is required, but being comfortable around a terminal will help, as well as being familiar with Git over SSH. We will start our journey with a basic LEMP stack and gradually increase the environment's complexity as we go, covering more and more use cases along the way. While the focus is mostly put on PHP on the server side, the overall principles can be applied to any language. You might find yourself stopping at a certain point because all your needs have been covered already, only to come back later when you need more. Wherever that is, each part of the series comes with its own Git branch , which you can fork or download and use as a starting point for your own projects. That's it for the elevator pitch. Convinced already? Great, let's move on to the first part . If not, let's try to address some of the doubts you may still have, starting with a little background check. \"Why should I listen to you?\" I am a backend developer with more than a decade of experience, having worked with various types of companies in three different countries. I witnessed the evolution of the development environment landscape over time, from the early days of using WAMP and Notepad++ to migrating to full-fledged IDEs and custom virtual machines, before moving on to Vagrant and eventually, Homestead . Back in 2015, I explored the possibility of using Docker locally as a development environment and wrote about it in a couple of articles that were picked up by the Docker newsletter at the time, but eventually moved away from it mostly because of performance issues. I was forced to revisit the idea at the beginning of 2018, however, as Vagrant started to show its limits on a project I was working on back then (more on that in the next section). As I looked into Docker again, I realised progress had been made both in terms of performance and adoption. After tinkering about for a few days, I came up with a decent development environment capable of handling the application's complexity and, as I soon realised, greatly simplifying the onboarding of new developers. Since then, I successfully implemented and improved upon a similar setup in several companies. \"I already use Vagrant/Homestead\" Homestead is great. I used Laravel 's pre-packaged virtual machine (VM) for years, both for personal and client projects. It features pretty much everything you need for a PHP application, and allows you to spin up new websites very quickly. So why did I make the switch? The short answer is microservices . Back at the beginning of 2018, I was tasked with exposing an API from a legacy monolith to serve a new Single Page Application, and to progressively phase out the monolith by extracting all its business logic into microservices. Happy days. All of a sudden, I had to manage some legacy code running on PHP 5.x on the one hand, and some microservices running on PHP 7.x on the other. I initially got both versions of the language running on the same VM but it involved some dirty workarounds that made the overall user experience terrible. Besides, I would eventually need to add other microservices with different stacks, and managing them all under the same VM wasn't a realistic endeavour. I briefly tried to run separate Vagrant boxes for each microservice, but running the whole setup was far too heavy for my machine and managing things like VM-to-VM communication felt very cumbersome. I needed something else, and that something else was Docker. But how does it help the situation? One of the promises of Docker is to provide isolated environments ( containers ) running on a single virtual machine, starting in a few seconds. In my case, that meant replacing all of my heavy Vagrant virtual machines with a single, super-fast virtual machine featuring Docker, and run all my microservices on top of it, each in its own isolated container. Using Docker's very own logo in an attempt to illustrate this, it would be like going from each crate requiring its own dedicated whale to carry it, to a single whale carrying all the crates like shown on the picture. Imagine if every single crate in the logo needed its own whale: beyond the ridiculous amount of plankton that would require, would it look efficient? Now revisit the previous sentence and replace \"whale\" with \"virtual machine\" and \"crate\" with \"microservice\" - you should get the idea. While being an overly simplistic explanation, this right there is what made it click for me. As a developer, this use case made perfect sense and impacted the way I worked on a day-to-day basis, way more than trying to understand how virtualisation or shared operating systems work. Does that mean you should use Docker locally only if your application involves microservices? The answer is yes and no. The more complex the application – the more moving parts it is composed of – the more likely you will need a solution like Docker. But even if your application is rather simple, starting and stopping some containers is way faster than booting and halting a virtual machine, and in the eventuality of your application evolving in some unexpected ways (like they always do), adopting Docker from the get go gives you the confidence that your setup is future-proof. Besides, instead of using a pre-packaged virtual machine like Homestead, featuring way more tools than any given application has any use for, using Docker the way I suggest ensures that you only ever instal what you actually need, in a proactive way. You regain control of your environment. \"How about [insert favourite Docker-based solution]?\" There is already a number of Docker-based development environments out there, and it seems that a new one pops up every other week. Laradock was arguably the first to get some traction, and is to Docker what Homestead is to Vagrant: a pre-packaged environment for PHP applications. While I don't personally use it, I have heard a lot of good things and it might just be enough for your needs. Their motto is Use Docker First - Then Learn About It Later , which is an excellent approach in my opinion. Laravel unveiled Sail (their official Docker-based environment) at the end of 2020; there's also Takeout , DDEV , and I'm sure plenty of others that are very good at what they do. The purpose of this series, however, is to give you a deeper understanding of Docker so you can build your own setup, one that matches perfectly any given project's requirements. If all you want to do for the moment is using a Docker-based environment without the hassle of setting it up yourself, by all means give one of the aforementioned solutions a try, and feel free to come back whenever you want to create your own, tailored setup. All in all I see this explosion of Docker-based environments as a good sign – a confirmation that this is the right tool for the job, one that will be embraced more widely in the future. \"Serverless is the way to go\" Serverless computing is on the rise and the word on the street is that it's coming for Docker . Meanwhile, others argue that the comparison is moot because the two technologies serve completely different purposes. As far as local development is concerned, the comparison is mostly irrelevant – serverless or not, you will need a local environment to build your projects, and whatever their specifications are Docker will fit the bill. \"Performance is terrible\" It used to be terrible. Now it's largely OK. Performance on Linux was always fine, and since the release of WSL 2 it's fine on Windows, too. That leaves us with macOS, where speed remains a pain point depending on the context, due to the poor performance of the underlying file-sharing implementation (gRPC-FUSE). There are ways to make it better, however, which are mentioned throughout this series. While you're unlikely to get the same kind of speed as Homestead or something like Valet , it will be acceptable. \"It is not my job to deal with Docker\" Finally, some developers simply dismiss Docker as not being their problem. This is a fair point, as Docker sits somewhere beyond the realm of pure application development. If anything, a sysadmin or DevOps engineer should set it up for you, right? While you are entitled to feel that way, DevOps not only refers to individuals with an interest for both system administration and coding, effectively acting as bridges between the two, but can also be viewed as people from both sides meeting halfway. In all fairness, you don't need to know Docker to be a good developer. My point is simply that by ignoring it, you are missing out on an opportunity to get better. It won't improve your syntax nor teach you new design patterns, but it will help you to understand how the code you write fits into the bigger picture. It is a bit like playing an instrument without learning to read sheet music: you might sound good, but you are unlikely to write a symphony. Taking a step back to reflect on the application's architecture to understand how the different pieces fit together will give you invaluable perspective that will influence the technical choices you make for your application. Whatever your specialty – backend, frontend or fullstack – and to whatever extent you feel concerned with the DevOps movement, I promise that learning about the cogs that make your applications tick is worth your time, and that you will be a better developer for it. It's all about the journey Ultimately, you may not fully embrace the final result of this series. Be it for performance reasons as mentioned earlier, or because you feel some aspects of the setup are too complicated or don't work for you, this development environment may not entirely satisfy you in its suggested form. But that is not the true purpose of this series. While you can take my approach at face value, my main objective is to give you enough confidence to integrate Docker to your development workflow in any way you see fit – to provide enough practical knowledge to adapt it to your own use cases. And I can guarantee that by the end of this series, you will have plenty of ideas as to how to use Docker for your own benefit. Now. Shall we get started?","tags":"DevOps","url":"https://tech.osteel.me/posts/docker-for-local-web-development-introduction-why-should-you-care"},{"title":"Talking about Collections at PHP Quebec","text":"Who knew Collections were so popular? After Laravel Montreal last month , I will be speaking about Collections again at PHP Quebec on July 4 . This talk will be less Laravel-centric, instead highlighting Tighten Co.'s standalone package , which extracted the Collection class from the web artisans' framework to make it available to any PHP project. So let's grab a beer and have a chat at Gorilla's office next Thursday! (If you can't make it but wish to know more about Collections nevertheless, I have put together a GitHub repository with a demo application containing all of the examples I will be showing, as well as the presentation's slides .)","tags":"PHP","url":"https://tech.osteel.me/posts/talking-about-collections-at-php-quebec"},{"title":"Talking about Collections at Laravel Montreal","text":"I have been a bit busy since I moved to Montreal back in February last year and one of the things I've been doing is regularly attending Laravel Montreal meetups. It is a small but active community that gathers every month or so around various subjects gravitating around the framework, and after sitting in the audience for a few months I in turn decided to stand before it and share a bit of my experience. I'll be talking about Laravel Collections tomorrow night (Weds May 29) at ctrlweb 's office from 6:30pm. If you live in Montreal don't hesitate to join, either tomorrow or at a future meetup - presentations are in French but you can ask questions in English. I have also put together a GitHub repository with a demo application containing all of the examples I'll be showing, as well as the presentation's slides (also in French, but there are a lot of code samples and with the demo app it should be fairly easy to follow). Happy coding!","tags":"Laravel","url":"https://tech.osteel.me/posts/talking-about-collections-at-laravel-montreal"},{"title":"Having issues with your cordless Logitech Unifying device on MacOS? Don't throw it away just yet","text":"TL;DR The cordless Logitech Unifying devices can be a bit funny when it comes to MacOS Sierra, be it the scrolling acting weird or the device not being detected altogether. In my case it was the latter, and it took me a ridiculous amount of time to find a solution, mostly because of borked software. This post is mainly for my own future reference, but if you are running into the same kind of troubles, hopefully following the quick steps below will help. Full story Having worked with a Magic Trackpad (version 1) for a while, then with a mouse again for one of my contracts, I realised I actually felt more comfortable using the latter. I decided I would get one for my personal use as well, and first peeked at Apple's horrendously (and unsurprisingly) expensive Magic Mouse , before I remembered that once upon a time I had a wireless mouse, when I was still a Windows user. I found it among old RJ45 cables and dusty keyboards: a Logitech Anywhere Mouse MX , using the Unifying technology - basically a tiny dongle to plug to one of the machine's USB ports, and supporting devices of the same product range. I inserted some batteries, plugged the dongle and switched the mouse on: nothing. I made a quick search online and everything seemed to point to the same piece of software, the Logitech Control Center . I downloaded it, installed it and rebooted my machine: the Logitech Control Center icon did appear in System Preferences , but my mouse did not show up and no matter what I did, clicking the Open Unifying Software button made the application crash (which was a bummer since my mouse obviously needed to be paired with the dongle again). I couldn't find a solution at the time and had better things to do with my life, so I gave up and moved on, and only decided to give it another shot a few weeks later, before I sold it on Gumtree and gave Apple more of my money to acquire their fancy Magic Mouse. I found an indirectly related GitHub thread where a dude mentioned another piece of software, Logitech Options . I first uninstalled the Logitech Control Center (search for \"LCC Uninstaller\", follow instructions, reboot to complete the removal of the driver), and went on installing Logitech Options. I'm not entirely sure what its purpose is but for some reason the version of the Unifying Software it comes with does work and allowed me to pair my mouse again. Hurray. Now there are multiple reports of weird scrolling behaviours concerning Logitech mouses on MacOS Sierra - if that's your case, it seems that installing the Logitech Control Center might help after all. I installed it again and I can configure my mouse properly - the application seems to happily live alongside the Logitech Options one in the System Preferences. I had enough emotions for one day though, and didn't dare to open the Unifying Software from the Control Center (will you?). Quick steps Uninstall the Logitech Control Center if already present on your system (search for \"LCC Uninstaller\", follow instructions, reboot to complete the removal of the driver) Install Logitech Options Open it and pair your device(s) (Re-)Install the Logitech Control Center if you wish to customise further your device(s)","tags":"Misc","url":"https://tech.osteel.me/posts/having-issues-with-your-cordless-logitech-unifying-device-on-macos-dont-throw-it-away-just-yet"},{"title":"UK Contractors: should you switch off the Flat Rate Scheme?","text":"TL;DR Probably. With the new fiscal year just a few days ahead, as a contractor you've probably heard about the Budget changes. And if you are like me up until a few days ago, you might not be sure whether you should take action or not. Let's try and bring some clarification to these changes. Note: if you are new to the contracting world and/or some of the terms below are unknown to you, you might want to read my UK tax breakdown as well. Table of contents A brief summary of the new Budget The Flat Rate Scheme's change The current state of things The new Limited Cost Trader category The Standard VAT Scheme Should you make the switch? Conclusion Sources A brief summary of the new Budget There are a few things that are effectively going to change between April 1st and April 6th 2017, some of which affect contractors, and which can be summarised in three bullet points: The income tax allowance is raised from £11,000 to £11,500 (April 6th) The Corporation Tax is reduced from 20% to 19% (April 1st) A new 16.5% VAT flat rate category is introduced ( Limited Cost Trader - April 1st) You might also have heard about the dividend tax allowance being reduced from £5,000 to £2,000, around which there seems to be some confusion. No need to worry about it just yet - it will actually come into effect in the 2018/19 tax year . There was also quite some fuss around an announced National Insurance Contributions increase, but the Government eventually dropped it . The Flat Rate Scheme's change The one big change that should get your attention here is the one related to VAT's Flat Rate Scheme (FRS). I will assume you already know what the scheme is, and if you don't, I invite you to read the section dedicated to VAT from my previous article about taxes. The current state of things The way the FRS currently works is, on an invoice of £2,000, you charge 20% of VAT, corresponding to an amount of £400, so a total of £2,400. Then, you must apply the flat rate corresponding to your trade sector to the total, the result being the VAT you pay back to HMRC. As an IT Consultant for example, the rate applying to my business is 14.5%, so I'd need to pay £2,400 * 0.145 = £348 back to HMRC. This is quite interesting, since the difference of VAT, called the VAT Credit , becomes part of my company's profit. Or to put it differently, I charge £400 VAT and pay back £348, keeping the remaining £52. The new Limited Cost Trader category In order to \"tackle aggressive abuse of the VAT Flat Rate Scheme\" (I won't go into details here, follow the link for more info), the Government is introducing a new category called Limited Cost Trader , with a fixed 16.5% rate for whichever business falls under it, regardless of the trade sector. To be affected by this new rate, your business expenditure must represent less than 2% of its VAT inclusive turnover, or less than £1,000 per annum if it is greater than 2%. Now if you are like me, looking at your business expenses you might think you are way above the threshold and thus not concerned by this at all. That was before I realised only certain kinds of expenses qualify. Basically, everything including food, transport, phone, computer ( capital expenditures ), etc doesn't qualify and, this list accounting for 95% of my expenses, it turns out my business automatically falls under the Limited Cost Trader category. Bummer. But how bad is it? If we look at the same example again, I now need to pay £2,400 * 0.165 = £396 back to HMRC. That's a VAT Credit of £4. The Standard VAT Scheme If your business falls under the new category, the VAT Credit it benefits from becomes very low. So should you switch to the Standard Scheme instead? Let's review how it works first. You still charge 20% VAT to your client, which you must entirely pay back to HMRC. However, you can claim the VAT on any VAT inclusive expense related to your business, when you could only do so for expenses greater than £2,000 with the FRS. Food and transport usually don't qualify, but capital expenditures do (computer, phone), and so do accountancy fees, for example. Let's take an example and consider your accountant costs your business £90 per month. They charge the VAT as well, which accounts for £90 - (£90 * 100 / 120) = £15. That means that you could claim £15 of VAT per month. Note that with the Standard Scheme, you need to keep track of the VAT you charge and the VAT you claim, which is a bit more work. Should you make the switch? To make that decision, you need to assess whether you'd be better off being able to claim VAT on VAT inclusive business expenditures (Standard Scheme) or applying the 16.5% rate and keeping the VAT Credit (Flat Rate Scheme's Limited Cost Trader category). You should be able to do so looking at how much you invoice in an average month as well as the nature of your usual expenses, and use the calculations above to determine what works best. If you are planning on buying hardware this year, it is very likely that the Standard Scheme would be better for you. Personally, with accountancy fees alone, I'm better off de-registering from the FRS. And since these fees are directly managed by my accountant, I don't even need to keep track of the claimed VAT, as they automatically do it for me. No extra work. Conclusion I hope this will help you assess whether you are affected by these changes at all, and if you need to do something about it. Bear in mind that I am not an accountant and that you should talk to yours before taking any action. If you do so and decide to switch off the FRS, you should also remember that you won't be able to register for it again for another twelve months . Sources Tax and tax credit rates and thresholds for 2017-18 Corporation Tax: main rate Income Tax: dividend allowance reduction 'This is grossly unfair': self-employed readers react to NICs increase Budget U-Turn As Chancellor Scraps National Insurance Hike On Self-Employed Tackling aggressive abuse of the VAT Flat Rate Scheme - technical note What are capital expenditures? VAT Flat Rate Scheme - Join or leave the scheme","tags":"Business","url":"https://tech.osteel.me/posts/uk-contractors-should-you-switch-off-the-flat-rate-scheme"},{"title":"Contracting in the UK: a tax breakdown","text":"Edit 26/03/17 The 2017/18 tax year is almost here! Read about the changes here . When I started contracting about a year and a half ago, I absolutely didn't want to have to deal with anything accounting-related. Which is fine really, as getting an accountant is highly recommended anyway - for various good reasons - and there is a plethora of companies on the market that do just that. I picked one that had been recommended to me by a fellow contractor and they did everything for me, from opening my limited company and business bank account to dealing with VAT registration, among other boring things. All I had to do was to sign papers and pretend to understand what they were telling me over the phone. Easy. Earlier this year though, as I had been in the contracting business for a little more than a year, I started to feel like I should try and understand a bit more what was what. Needless to say, it didn't turn out to be an easy task. I started googling around and realised the first few result pages were hogged by accounting companies that were all more or less providing the same content - enough to get a global grasp, but nothing breaking down things in detail. I kept digging nevertheless and, with my findings and some answers provided by my accounting team, I eventually got to a point which I think is a basic but decent understanding of what's going on, now all compiled below in a post I wrote while enjoying a well-deserved end-of-year holiday. What could be better than an article about taxes to put you in the Christmas mood anyway? Summary Disclaimer Tax Typology 1. Corporation Tax 2. PAYE and National Insurance contributions 3. Tax on dividends 4. VAT Example Yearly projection Conclusion Sources Disclaimer What follows is closely related to my own contracting experience as an IT consultant, to the way I operate on a daily basis as the director of my limited company. While this post certainly isn't exhaustive, I think it covers most of the questions that will arise for the average contractor at some point, and should allow you to make a projection for your own company based on your personal situation. The aim here is not for you to deal with your own company accounts, but more to understand the stuff your accountant is sending you, beginning with your personal illustration when you're just getting started. I should also point out that what is described below only applies if you can prove that you operate outside of IR35, which is a whole other subject that I won't get into here. You can read more about it over there if you wish to. Unless stated otherwise, all the info provided is valid for the 2016-17 financial year. Tax typology Taxes come in different flavours and the four main ones are: Corporation Tax, PAYE and National Insurance contributions, tax on dividends, and VAT. In the United Kingdom, the department responsible for collecting taxes is HMRC (Her Majesty's Revenue & Customs). 1. Corporation Tax Corporation Tax applies to the company's profits and is paid annually by the company, the deadline usually being nine months and a day after the end of your accounting period (which normally corresponds to the fiscal year - from the 6th of April to the 5th of April the following year). The tax is 20%. 2. PAYE and National Insurance contributions PAYE Income tax or PAYE (Pay As You Earn) is based on the employee's income (meaning the director) (meaning you) and is retained and paid for by the employer monthly or quarterly. There is a yearly Personal Allowance of £11,000 on standard income, which essentially means that the first £11,000 of income are tax free. Here are the tax bands for the 2016-17 fiscal year: Allowance (0%) Up to £11,000 Basic rate (20%) From £11,001 to £43,000 Higher rate (40%) From £43,001 to £150,000 Additional rate (45%) Over £150,000 Expenses I should probably drop a quick note about expenses at this point. These are the purchases you as the employee make that are related to your professional activity (transport, meals, hardware, etc). You declare these purchases as business expenses so your company can pay you back as part of your income, but that amount is tax free. To put it differently, say you file £65 of expenses in a certain week, you will get this amount back as part of your income but it will not be deducted from your Personal Allowance. National Insurance contributions You pay National Insurance contributions so you can benefit from advantages like the State Pension or the Jobseeker's Allowance (the full list of benefits is available here ). There are different classes and category letters corresponding to different rates, but as a company director yours will most likely be class 1 and category letter A (unless specific circumstances as listed in the two previous links). National Insurance contributions are paid by both the employee and the employer, based on rates and only above a certain yearly threshold, and the employer pays the total amount to HMRC along with the income tax. Employee: 0% From £112 (minimum wage) to £155 per week 12% From £155.01 to £827 per week 2% Over £827 per week The employee's threshold is £8,060, meaning the employee will start to see NI contributions deducted from their income once it meets £8,060 for the current fiscal year. Employer: 0% From £112 (minimum wage) to £156 per week 13.8% From £156.01 to £827 per week 13.8% Over £827 per week The employer's threshold is £8,112, meaning the employer will start to pay NI contributions once the employee's income meets £8,112 for the current fiscal year. 3. Tax on dividends Being a company director allows you to pay yourself with dividends (since you own 100% of the company's shares) as well as the standard income. Just like the latter, there is a Personal Allowance under which dividends are not taxed. Its amount is £5,000, which means that the first £5,000 of dividends are tax free. Allowance (0%) Up to £5,000 Basic rate (7.5%) From £5,001 to £32,000 Higher rate (32.5%) From £32,001 to £150,000 Additional rate (38.1%) Over £150,000 Tax on dividends is paid annually while you complete your tax return. We can already clearly see that standard income tax rates are significantly higher than the dividend ones - that's why contractors usually pay themselves the standard income's Personal Allowance only (so it remains tax free), and the rest with dividends. 4. VAT The VAT (Value-Added Tax) is basically a tax that is paid every time goods or services are sold. As a business, you must register for VAT only if your annual VAT-taxable turnover is over a certain threshold (£83,000 for 2016-17, usually updated every year), but you can also voluntarily do so even if your turnover is less than that amount ( here are some reasons why you would do such a thing ). When you pay for it depends on the chosen VAT scheme, which are quite numerous and vary according to your activity. The standard way of dealing with VAT is to keep track of the VAT you invoice (and to pay the corresponding amount to HMRC whether your client paid the invoice or not) as well as the VAT of your business expenditure (which you can likewise claim even if you haven't paid the invoice yet), and to report these amounts to HMRC quarterly. Depending on the side of your business and its turnover, however, different options are available to you. Going into details for each of them is much like sticking your head down the rabbit hole, so I will only briefly describe some of them and linger for a bit longer on the Flat Rate Scheme, since that's the one I personally registered for (and seems to often be picked by contractors - or is advised by accountants anyway, since it also makes their life easier as we will see). Annual Accounting Scheme This scheme allows you to submit your VAT return once a year instead of quarterly, for a fixed amount based on the previous return (or an estimate for the first year you register for that scheme). You then pay for or get back the difference at the end of the year. Cash Accounting Scheme The difference between this scheme and the standard one is that you pay and claim VAT for invoices that were actually paid rather than from the moment you send or receive them. Margin schemes The VAT corresponds to 16.67% of the difference between the price you paid for an object (antiques, works of art...) and the price you sold it for. Flat Rate Scheme At first glance the Flat Rate Scheme is quite appealing, since it greatly simplifies things and in most cases will actually save you some dough (via the VAT credit you get by charging more VAT than your company actually pays for - see the example below ). The way it works is that you charge 20% VAT to your clients as usual, but you only pay a fixed (lower) percentage of VAT to HMRC in return. That percentage depends on your activity (see the table on this page for comparison) and is reduced by 1% the first year - e.g. as an IT consultant my rate is 14.5% (and was 13.5% the first year). That means that I charge a 20% VAT to my clients and pay 14.5% back to HMRC - the difference is the VAT credit, added to my company profit. This is quite simple indeed since there is no need to keep track of the VAT for each business expenditure. The Flat Rate Scheme doesn't allow you to claim VAT on such expenses, apart for purchases above £2,000 . While it might seem like a no-brainer, there is slightly more to this: the scheme certainly simplifies the process around VAT, but this simplification also benefits to your accountant and HMRC. You will most certainly get VAT credit, which is a good thing, but whether it is more beneficial to you than say the Cash Accounting Scheme should be estimated first, based on your expected business expenses. At the end of the day which option is best for you really depends on your personal situation, but in any case you should ask your accountant for comparative simulations between the different schemes to make sure you pick what works best for your business, not only what works best for them. There is obviously much more to VAT than the above (specific rules for builders , charities , retailers ... or even depending on whether you trade internationally), and there's quite some literature about it on HMRC's website if you need more information. Example Alright, enough with the theory, it all truly starts to make sense once applied concretely. Let's have a look at an example, in which we'll calculate the amount of take-home based on a weekly invoice of £1,500 plus VAT (+20%), so five days at £300 plus VAT per day. In order to be tax-efficient, say we only want to pay ourselves the Personal Allowance of £11,000 mentioned earlier, which over a year (52 weeks) is £11,000 / 52 = £210 per week (approximately). This will allow us not to pay any tax on income, meaning we will pay ourselves with dividends instead, which are taxed at a lower rate as we already saw. Note I am using a weekly average for the sake of simplicity, but then again this is just an example, YMMV. VAT credit Since our daily rate is £300 plus VAT and we charge 20% VAT to the client, we send a weekly invoice of: £1,500 + VAT = £1,500 + (£1,500 * 0.2) = £1,500 + £300 = £1,800 Say we registered for the Flat Rate Scheme which, as an IT consultant, means we have to pay a 14.5% rate. 14.5% of £1,800 is £261, which is the amount we owe to the taxman. Since the amount of VAT charged to the client is £300, in effect that's a VAT credit of £300 - £261 = £39. Your company essentially keeps the difference between the VAT it charges and the VAT it pays (the VAT credit). In our example, that means our company profit is now £1,500 + £39 = £1,539. PAYE, NI contributions and expenses We already saw that since we'll pay ourselves the Personal Allowance only, we won't pay any PAYE tax on our income. But as this allowance (£11,000) is more than the NI contributions thresholds (£8,060 and £8,112 for the employee and the employer respectively), we will have to pay some NI contributions eventually. Assuming you would pay yourself the same weekly income throughout the year (52 weeks), the NI contributions could be calculated like so, as a weekly average: The NI contributions rate is 0% up to £155, and 12% of the remaining £55, so £55 * 0.12 = £6.6 paid by the employee per week. NI contributions are 0% up to £156, and 13.8% of the remaining £54, so £54 * 0.138 = £7.45 paid by the employer per week. In practice however, NI deductions would only appear on your payslips once the employee's threshold is met, and the company's liability in this regard would only start to be accounted for once the employer's threshold is met. For the sake of simplicity however, and to show you how they are accounted for, let's consider that we take these NI contributions into account on a weekly basis. Our company pays us (as the employee) £210, plus £7.45 of NI contributions to HMRC, so £217.45 in total. The company profit is now £1,539 - £217.45 = £1,321.55. As we saw earlier, however, as a contractor chances are you will have expenses to file, such as transport costs, meals, books, etc. These expenses are paid to you by your company as tax-free income, so if you spend say £50 on that week, the company will pay you back £50. Hence the company profit becomes £1,321.55 - £50 = 1,271.55. You as the employee are paid £210 - £6.6 of NI contributions = £203.4. We don't count the £50 of expenses here, since you paid for it in the first place - your company is just paying you back. It's cancelled out. (Don't worry if you start to feel a bit schizophrenic at this point, that's perfectly normal.) Corporation Tax It is on this remaining company profit that the Corporation Tax is applied (20%): £1,271.55 * 0.2 = £254.31 £254.28 is thus what you should keep in your company's bank account for the taxman. That also means that the remaining £1,017.24 (£1,271.55 - £254.31) can be paid to you as dividends. The total take-home is thus £1,017.24 + £203.4 = £1,220.64. Wrapping it up Let's go over our initial assumptions again and sum it all up: our daily rate is £300 + VAT we work five days a week we pay ourselves £210 per week throughout the year (that's 52 weeks) we registered for the Flat Rate Scheme With that in mind, the weekly take-home would be £1,220.64 out of the £1,500 we invoice or, to put it differently, a whopping 81% of the initial amount. But then again that's only an average amount, as in practice you wouldn't pay for the NI contributions until the threshold is reached and it is very unlikely you would work five days a week for 52 weeks (it's actually impossible, if only because of bank holidays). You could probably still pay yourself £210 a week (even when you don't work), but not as much in dividends. Let's consider it could be the case anyway, just for the sake of the example: does that mean you could earn 52 * £1,220.64 = £63,473.28 a year? Well, not quite. We are not paying any tax on income, that's true, but let's not forget that dividends are subject to their own tax too. That's why it now makes sense to think on a yearly scale, now that you understand how things work on a weekly one. Yearly projection We know that the Personal Allowance for the current fiscal year (2016-17) is £11,000, which is tax free. As for dividends, the allowance is £5,000, then the basic rate is 7.5% up to £32,000, and then 32.5% above that (and 38.1% for the higher tax band). If we decide to stay beneath the first band for tax-efficiency purposes, that means that £32,000 - £5,000 = £27,000 is taxed at a 7.5% rate, so we would pay £27,000 * 0.075 = £2,025 of tax on dividends. Meaning that over the year, we could get £32,000 - £2,025 = £29,975 as dividends after tax. £11,000 is above the NI contributions thresholds of both the employee and the employer, but we're mainly interested in the take-home here so we can ignore the latter. As we saw earlier the employee's threshold for NI contributions is £8,060 for the current financial year, and anything above that is taxed at a 12% rate up to £43,004 (£827 by 52 weeks). This means that £11,000 - £8,060 = £2,940 is taxed at a 12% rate, so we would pay £2,940 * 0.12 = £352.8 of NI contributions over the year. Combining all of the above, we could get an income of £11,000 - £352.8 = £10,647.2, plus £29,975 of dividends, so a total of £40,622.2 after tax over the year. With the same assumptions we used in the previous example, we would need to work £40,622.2 / £1,220.64 = 33.28, basically a bit more than 33 weeks over the year to reach that amount. Conclusion I hope this will shed some light on an obscure income statement or help your decision if you are considering taking the plunge and becoming a contractor. I am not by any measure an accountant so there might be inaccuracies here and there, or I might have left out elements I'm not aware of (if that's the case please let me know about it in the comments). Then again, the aim here is not for you to do your own accounting, but rather to give you the keys so you can make a projection for your own company based on your personal situation. Finally, if you are contemplating becoming a contractor in the near future and are not sure which accountant to go for, I can hook you up with mine - just drop me a line . Sources Corporation Tax Corporation Tax Corporation Tax rates and reliefs PAYE Income Tax Income Tax rates and Personal Allowances NI contributions National Insurance National Insurance for company directors National Insurance rates and categories Tax on dividends Tax on dividends VAT VAT VAT registration The Advantages Of Voluntary VAT Registration For Small Business VAT Schemes for Small Businesses VAT Flat Rate Scheme IT contractor guide to Flat Rate VAT Other related sources Preparing for the inevitable: What taxes will I have to pay? Rates and thresholds for employers 2016 to 2017","tags":"Business","url":"https://tech.osteel.me/posts/contracting-in-the-uk-a-tax-breakdown"},{"title":"How to enable NFS on Laravel Homestead","text":"I currently work on a Laravel project composed of multiple microservices that I run locally using Homestead (box v0.4.0 at the time of writing). As I started tinkering around I noticed that requesting the different APIs was super slow - up to 20s per request, which was really unexpected (and annoying to say the least). Googling around it appeared that most fingers were pointing to the same suspect: VirtualBox's shared folders system. Most people were also advising to use NFS instead, and as a matter of fact there is a whole section of Vagrant's documentation on the subject . I tried different things found on various forums over the Internet (such as this topic on Laracast ) and, once I got it set up, the time per request dropped to around 1s. This, you will agree, was quite an improvement. If it originally required a few tweaks in Homestead's configuration, it turns out that with the recent versions of the box, almost all the work is already done. The following is of course assuming that you are already using Homestead locally. Note for Windows users: You may have noticed that the Vagrant documentation states that NFS is not available on Windows. If you are a PC user, you might want to have a look at Vagrant WinNFSd whose promise is to add support for NFS on Windows (I didn't try it myself). Note for Ubuntu users: You might need to install the NFS server: apt - get install nfs - kernel - server In ~/.homestead/Homestead.yaml , under the folders section, add a type option under the folders you wish to map using NFS, as such: folders : - map : ~/ Work / www / homestead to : / home / vagrant / projects type : \"nfs\" Now, stop Homestead if it was running ( vagrant halt from ~/Homestead or however you usually do it) and start it again, forcing the provisioning step, e.g.: vagrant up — - provision You will be asked to enter your Mac OS user password before Vagrant mounts the folder(s). That's it! You may wonder why isn't NFS activated by default, then: the reason is because it won't work out of the box on all operating systems, as seen in the notes above.","tags":"Laravel","url":"https://tech.osteel.me/posts/how-to-enable-nfs-on-laravel-homestead"},{"title":"From Vagrant to Docker: How to use Docker for local web development","text":"Heads-up! This is old content. I published an up to date, more complete tutorial series about using Docker for local web development, which I invite you to read instead. If you are somewhat following what's happening in the tech world, you must have heard of Docker. If you haven't, first, get out of your cave, and then, here is a short description of the thing, borrowed from Wikipedia : Docker is an open-source project that automates the deployment of applications inside software containers, by providing an additional layer of abstraction and automation of operating-system-level virtualization on Linux. \"Wow, cool. What the hell does that mean tho?\" I hear you say. Hang on! It goes on, saying that: [it allows] independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines. Hmm ok. It starts to sound familiar and vaguely useful. I don't know about you, but working on a Mac (and previously on a Windows machine) (shush), I set up a Vagrant box for almost every single project I work on (if you have no clue what Vagrant is, please take a look at this post first, especially this short bit ). Whenever I need to work on any of these projects, I run a quick vagrant up and get it running in its own isolated virtual machine (VM) in a matter of minutes. Pretty handy. But that's still a few minutes to get up and running, and having a VM for each project quickly ends up taking a shitload of resources and space on the disk. You could take Laravel's initial approach with Homestead and run several projects on the same VM, but it kinda defeats the purpose of having isolated environments. So what does Docker have to do with this? Well, the promise behind it is to provide isolated environments running on a same virtual machine (with Boot2Docker ) that starts in about five seconds. \"So... I could replace all my Vagrant boxes with a single super-fast VM running Docker?\" Exactamundo. And that's what we are going to do today, step by step. Table of contents Installation Setup Nginx PHP Data Volumes and data containers MySQL phpMyAdmin Handling multiple projects Troubleshooting Conclusion Sources Installation As mentioned earlier, I'm working on a Mac, so this tutorial will be written from a Mac user point of view. That being said, in practice, once the installation is done, the way Docker is used shouldn't differ much (if at all). So, whether you're on a Mac or a PC, just head over here , download and install Docker Toolbox and follow the instructions for your platform ( Mac OS , Windows ) (or, if you're a Linux user, go straight to the specific installation guide - no need to get Docker Toolbox). As stated on the website, Docker Toolbox will install all the stuff you need to get started. Follow the steps of the Get Started with Docker guide all the way down to the Tag, push, and pull your image section: it's all very clear and well written and will even help you get your bearings with the terminal if you're not familiar with it. I'll take it from there. In the meantime, here is a dancing Tyrion: Note: You might get this kind of error when trying to complete the first step: Network timed out while trying to connect to https : // index . docker . io / v1 / repositories / library / hello - world / images . You may want to check your internet connection or if you are behind a proxy . In that case, simply run docker-machine restart default and try again. All good? Well done. By now, you should have a virtual machine running with VirtualBox (the virtualisation software used by default under the hood) and know how to find, run, build, push and pull images, and have a better idea of what Docker is and how it works. That's pretty cool already, but not exactly concrete. How do you get to the point where you open your browser and display the website you are currently building and interact with a database and everything? Stick with me. Setup Everything we'll do under this section is available as a GitHub repository you can refer to if you get stuck at any time (you can also directly use it as is if you want). Also, as you may have noticed I already started to use the terms virtual machine and VM to refer to the same thing. I will sometimes mention a Docker machine as well. Don't get confused: they are all the same. Now that this is clear, let's decide on the technologies. I usually work with the LEMP stack , and I would like to get my hands on PHP7, so let's go for a Linux/PHP7/Nginx/MySQL stack (we'll see how to throw a framework into the mix in another post). As we want to have the different parts of the stack to run in separate containers, we need a way to orchestrate them: that's when Docker Compose comes in. A lot of tutorials will teach you how to set up a single container first, or a couple of and how to link them together using Docker commands, but in real life it is very unlikely that only one or two containers are going to be needed, and using simple Docker commands to link more containers to each other can quickly become a pain in the bottom. Docker Compose allows you to describe your stack specifying the different containers that will compose it through a YAML config file. As the recommended practice is to have one process per container, we will separate things as follows: a container for Nginx a container for PHP-FPM a container for MySQL a container for phpMyAdmin a container to make MySQL data persistent a container for the application code Another common thing among the tutorials and articles I came through is that their authors often use their own images, which I find somewhat confusing for the newcomers, especially as they rarely explain why they do so. Here, we'll use the official images and extra Dockerfiles to extend them. Nginx But first, let's start with an extremely basic configuration to make sure everything is working properly and that we're all on the same page (this will also allow you to familiarise with a few commands in the process). Create a folder for your project (I named mine docker-tutorial ) and add a docker-compose.yml file into it, with this content: nginx : image : nginx : latest ports : - 80 : 80 Save it and, from that same folder in your terminal, run: $ docker-compose up -d It might take a little while as the Nginx image needs to be pulled first. When it is done, run: $ docker-machine ip default Copy the IP address that displays and paste it in the address bar of your favourite browser. You should see Nginx's welcome page: Nice! So what did we do here? First we told Docker Compose that we wanted a container named nginx to use the latest official Nginx image and publish its port 80 (the standard port used by HTTP) on the port 80 of our host machine (that's my Mac in my case). Then we asked Docker Compose to build and start the containers described in docker-compose.yml (just one so far) with docker-compose up . Option -d allows to have the containers running in the background and gives the terminal back. Finally, we displayed the private IP address of the virtual machine created by Docker and named default (you can check this running docker-machine ls , which will give you the list of running machines). As we published the port 80 of this virtual machine, we can access it from our host machine. One last thing to observe before we move on: in your terminal, type docker ps . You should see something like that: That's the list of all the running containers and the images they use. For now we only have one container, using the official Nginx image. Here its name is dockertutorial_nginx_1 : Docker Compose came up with it using the name of the current directory and the image's and appended a digit to prevent name collisions. PHP Still following? Good. Now let's try and add PHP and a custom index.php file to be displayed when accessing the machine's private IP. Replace the content of docker-compose.yml with this one: nginx : build : ./ nginx / ports : - 80 : 80 links : - php volumes : - ./ www /html:/var/www/ html php : image : php : 7.0 - fpm expose : - 9000 volumes : - ./ www /html:/var/www/ html A few things here: we added a new container named php , which will use the official PHP image, and more specifically the 7.0-fpm tag. As this image doesn't expose the port 9000 by default, we specify it ourselves. At this point you might be wondering what is the difference between expose and ports : the former allows to expose some ports to the other containers only , and the latter makes them accessible to the host machine . We also added a volumes key. What we're saying here is that the directory ./www/html must be mounted inside the container as its /var/www/html directory. To simplify, it means that the content of ./www/html on our host machine will be in sync with the container's /var/www/html directory. It also means that this content will be persistent even if we destroy the container. More on that later. Note: If you have trouble mounting a local folder inside a container, please have a look at the corresponding documentation . The nginx container's config has been slightly modified as well: it got the same volumes key as the php one (as the Nginx container needs an access to the content to be able to serve it), and a new links key appeared. We are telling Docker Compose that the nginx container needs a link to the php one (don't worry if you are confused, this will make sense soon). Finally, we replaced the image key for a build one, pointing to a nginx/ directory inside the current folder. Here, we tell Docker Compose not to use an existing image but to use the Dockerfile from nginx/ to build a new image. If you followed the get started guide, you should already have an idea of what a Dockerfile is. Basically, it is a file allowing to describe what must be installed on the image, what commands should be run on it, etc. Here is what ours looks like (create a Dockerfile file with this content under a new nginx/ directory): FROM nginx : latest COPY . / default . conf / etc / nginx / conf . d / default . conf Not much to see, eh! We start from the official Nginx image we have already used earlier and we replace the default configuration it contains with our own (might be worth noting that by default the official Nginx image will only take files named following the pattern *.conf and under conf.d/ into account - a mere detail but it drove me crazy for almost three hours at the time). Let's add this default.conf file into nginx/ : server { listen 80 default_server ; root / var / www / html ; index index . html index . php ; charset utf - 8 ; location / { try_files $ uri $ uri / / index . php ?$ query_string ; } location = / favicon . ico { access_log off ; log_not_found off; } location = / robots . txt { access_log off ; log_not_found off; } access_log off ; error_log / var / log / nginx / error . log error ; sendfile off ; client_max_body_size 100 m ; location ~ \\. php $ { fastcgi_split_path_info &#94; ( . + \\. php )( / . + ) $ ; fastcgi_pass php : 9000 ; fastcgi_index index . php ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $d ocument_root $fa stcgi_script_name ; fastcgi_intercept_errors off ; fastcgi_buffer_size 16 k ; fastcgi_buffers 4 16 k ; } location ~ / \\. ht { deny all ; } } It is a very basic Nginx server config. What is interesting to note here however is this line: fastcgi_pass php : 9000 ; We are asking Nginx to proxy the requests to the port 9000 of our php container: that's what the links key from the config for the nginx container in the docker-compose.yml file was for! We just need one more file - index.php , inside www/html under the current directory: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello World! </ title > </ head > < body > < img src = \"https://tech.osteel.me/images/2015/12/18/docker-tutorial2.gif\" alt = \"Hello World!\" /> </ body > </ html > Yes, it only contains HTML but we just want to make sure that PHP files are correctly served. Here is what your tree should look like by now: Go back to your terminal and run docker-compose up -d again. Docker Compose will detect the configuration changes and build and start the containers again (it will also pull the PHP image): Browse the virtual machine's private IP again ( docker-machine ip default if you closed the tab): you should be greeted by a famous Doctor. Now type docker ps in your terminal to display the list of running containers again: We can see a new one appeared, using the official PHP image, and the Nginx one looks a bit different from the previous one: Docker Compose used the Dockerfile to automatically build a new image from the official Nginx one, and used it for the container. Now if you remember, I said earlier that the current directory is in sync with the containers' (because of the volumes key in docker-compose.yml ). Let's check if I'm not a liar: open index.php and change the page title for \"Hello Universe!\" , for example. Save and reload the page. See the change? Sweet. Now we have got two containers for Nginx and PHP, talking to each other and serving files we can update from our host machine and see the result instantly. Time to add some database madness! Data Volumes and data containers Before we actually dive into the configuration of MySQL, let's have a closer look at this volumes thing. Both the nginx and php containers have the same directory mounted inside, and it's common practice to use what is called a data container to hold this kind of data. In other words, it's a way to factorise the access to this data by other containers. Change the content of docker-compose.yml for the following: nginx : build : ./ nginx / ports : - 80 : 80 links : - php volumes_from : - app php : image : php : 7.0 - fpm expose : - 9000 volumes_from : - app app : image : php : 7.0 - fpm volumes : - ./ www /html:/var/www/ html command : \"true\" Several things happened: first, we added a new container named app , using the same volumes parameter as the nginx and php ones. The purpose of this container is solely to hold the application code: when Docker Compose will create it, it is going to be stopped at once as it doesn't do anything apart from executing the command \"true\" . This is not a problem as for the volume to be accessible, the container needs to exist but doesn't need to be running, also preventing the pointless use of extra resources. Besides, you'll notice that we're using the same PHP image as the php container's: this is a good practice as this image already exists and reusing it doesn't take any extra space (as opposed to using a data-only image such as busybox , as you may see in other tutorials out there). The other change we made is volumes was replaced with volumes_from in nginx and php 's configurations and both are pointing to this new app container. This is quite self-explanatory, but basically we are telling Docker Compose to mount the volumes from app in both these containers. Run docker-compose up -d again and make sure you can still access the virtual machine's private IP properly. Running docker ps now should display this: \"Wait a minute. Where's the app container?\" I'm glad you asked. If you recall I've just said that the container was stopped right after its creation, and docker ps only displays the running containers. Now run docker ps -a : There it is! If you're interested in reading more about data containers and volumes (and I encourage you to do so), I'd suggest this article by Adrian Mouat which gives a good overview (you will also find all the sources I used at the end of this article). MySQL Alright! Enough digression, back to MySQL. Open docker-compose.yml again and add this at the end: mysql : image : mysql : latest volumes_from : - data environment : MYSQL_ROOT_PASSWORD : secret MYSQL_DATABASE : project MYSQL_USER : project MYSQL_PASSWORD : project data : image : mysql : latest volumes : - /var/lib/ mysql command : \"true\" And update the config for the php container to add a link to the mysql one and use a Dockerfile to build the image: php : build : ./ php / expose : - 9000 links : - mysql volumes_from : - app You already know what the purpose of the links parameter is, so let's have a look at the new Dockerfile: FROM php : 7 . 0 - fpm RUN docker - php - ext - install pdo_mysql Again, not much in there: we simply install the pdo_mysql extension so we can connect to the database (see How to install more PHP extensions from the image's doc ). Put this file in a new php/ directory. Moving on to the MySQL configuration: we start from the official MySQL image , and as you can see there is a new environment key we haven't met so far: it allows to declare some environment variables that will be accessible in the container. More specifically here, we set the root password for MySQL, and a name ( project ), a user and a password for a database to be created (all the available variables are listed in the image's documentation ). Following the same principle as exposed earlier, we also declare a data container whose aim is only to hold the MySQL data present in /var/lib/mysql on the container (and reusing the same MySQL image to save disk space). You might have noticed that, unlike what we were doing so far, we do not declare a specific directory on the host machine to be mounted into /var/lib/mysql (normally specified before the colon): we don't need to know where this directory is, we just want its content to persist, so we let Docker Compose handle this part. Although that does not mean we have no idea where this folder sits - but we'll have a look at this later. One thing worth noting right now tho, is that if this volume already contains MySQL data, the MYSQL_ROOT_PASSWORD variable will be ignored and if the MYSQL_DATABASE already exists, it will remain untouched. In order to be able to test the connection to the database straight away, let's update the index.php file a bit: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"utf-8\" > < title > Hello World! </ title > </ head > < body > < img src = \"https://tech.osteel.me/images/2015/12/18/docker-tutorial2.gif\" alt = \"Hello World!\" /> <?php $database = $user = $password = \"project\" ; $host = \"mysql\" ; $connection = new PDO ( \"mysql:host= { $host } ;dbname= { $database } ;charset=utf8\" , $user , $password ); $query = $connection -> query ( \"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_TYPE='BASE TABLE'\" ); $tables = $query -> fetchAll ( PDO :: FETCH_COLUMN ); if ( empty ( $tables )) { echo \"<p>There are no tables in database \\\" { $database } \\\" .</p>\" ; } else { echo \"<p>Database \\\" { $database } \\\" has the following tables:</p>\" ; echo \"<ul>\" ; foreach ( $tables as $table ) { echo \"<li> { $table } </li>\" ; } echo \"</ul>\" ; } ?> </ body > </ html > This script will try to connect to the database and list the tables it contains. We're all set! Run docker-compose up -d in your terminal again, followed by docker ps -a (again, it might take a little while as the MySQL image needs to be pulled). You should see five containers, two of which are exited: Now refresh the browser tab: \"There are no tables in database 'project'\" should appear. Event though the next point is about phpMyAdmin and you will be able to use it to edit your databases, I am now going to show you how to access the running MySQL container and use the MySQL command line interface. From the result of the previous command, copy the running MySQL container ID ( 5207587d116b in our case) and run: $ docker exec - it 5207587 d116b / bin / bash You are now running an interactive shell in this container (you can also use its name instead of its ID). docker exec allows to execute a command in a running container, -t attaches a terminal and -i makes it interactive. Finally, /bin/bash is the command that is run and creates a bash instance inside the container. Of course, you can use the same command for other containers too. From there, all you need to do is run mysql -uroot -psecret to enter the MySQL CLI. List the databases running show databases; : Change the current database for project and create a new table: $ mysql> use project $ mysql> CREATE TABLE users ( id int ) ; Refresh the project page: the table users should now be listed. You can exit the MySQL CLI entering \\q and the container with ctrl + d . There is one question about the MySQL data we haven't answered yet: where does it sit on the host machine? Earlier we set up the data container in docker-compose.yml as follows: data : image : mysql : latest volumes : - /var/lib/ mysql command : \"true\" That means we let Docker Compose mount a directory of its choice from the host machine into /var/lib/mysql . So where is it? Run docker ps -a again, copy the ID of the exited MySQL container this time ( 7970b851b07a in our case) , and run : $ docker inspect 7970b851b07a Some JSON data should appear on your screen. Look for the Mounts section: ..., \"Mounts\" : [ { \"Name\" : \"0cd0f26f7a41e40437019d9e5514b237e492dc72a6459da88d36621a9af2599f\" , \"Source\" : \"/mnt/sda1/var/lib/docker/volumes/0cd0f26f7a41e40437019d9e5514b237e492dc72a6459da88d36621a9af2599f/_data\" , \"Destination\" : \"/var/lib/mysql\" , \"Driver\" : \"local\" , \"Mode\" : \"\" , \"RW\" : true } ], ... The data contained in the volume sits in the \"Source\" directory. \"Hmm I see. But… what happens to the volumes if we remove the containers that hold them?\" Excellent question! Well, they actually stay around, taking disk space for nothing. Two solutions for this. First, we can make sure to remove the volumes along with the container using the -v option: $ docker rm -v containerid Or, if some containers were removed without the -v option, resulting in dangling volumes: $ docker volume rm $( docker volume ls -qf dangling = true ) This command uses the docker volume ls command with the q and f options to respectively list the volumes' names only and keep the dangling ones ( dangling=true ). Check the Docker command line documentation for more info. phpMyAdmin Being able to access a container and deal with MySQL using the command line interface is a good thing, but sometimes it is convenient to have a more friendly user interface. PHPMyAdmin is arguably the de facto choice when it comes to MySQL, so let's set it up using Docker! Open docker-compose.yml again and add the following: phpmyadmin : image : phpmyadmin / phpmyadmin ports : - 8080 : 80 links : - mysql environment : PMA_HOST : mysql Once again, we start from the official phpMyAdmin image . We publish its port 80 to the virtual machine's port 8080, we link it to the mysql container (obviously) and we set it as the host using the PMA_HOST environment variable. Save the changes and run docker-compose up -d again. The image will be downloaded and, once everything is up, visit the project page again, appending :8080 to the private IP (that's how we access the VM's port 8080): Enter root / secret as credentials and you're in ( project / project will work too, giving access to the project database only, as defined in the mysql container configuration). That one was easy, right? That's actually it for the setup! This is a lot to digest already, so taking a break might be a good idea. Don't forget to read on eventually though, as the next couple of sections will most likely clarify a few points. Again, the setup is also available as a GitHub repository . Feel free to clone it and play around. Handling multiple projects One big advantage of using Docker rather than say, multiple Vagrant boxes, is if several containers use the same base images, the disk space usage will only increase by the read-write layer of each container, which is usually only a few megabytes. To put it differently, if you use the same stack for most of your projects, each new project will basically only take a few extra megabytes to run (not taking into account the size of the codebase here, obviously). Say you want to use the same stack except you need PostgreSQL instead of MySQL. All you need to do is change the database container image for your new project, and all the other containers will reuse the images you already have locally. Pretty neat. But how do you concretely deal with several projects using the same virtual machine? First thing to consider is a Docker machine has only one private IP address, so unless you decide to use a different port per project (which should be absolutely fine), you won't be able to have multiple web projects running on the port 80 at the same time. That's not really an issue as you can easily stop containers. You might have noticed that we used docker and docker-compose commands in turn, which might be a tad confusing. To simplify, let's say docker-compose allows to run the same commands as docker , but on all the containers defined in the docker-compose.yml file at once, or for these containers only. Let's take a couple of examples: docker ps - a You already know this command: it displays all the containers of the Docker machine, be they running or not. All of them. However: docker - compose ps will do the same, but for the containers defined in the docker-compose.yml file of the current directory only (you will notice that -a is not necessary and that the order of the displayed info is slightly different). It comes in handy when you begin to have a lot of containers managed by the same VM. But there is more: docker - compose stop will stop all the containers described in the current docker-compose.yml file. Basically when you are done with a project, run this command to stop all its related containers. To connect the dots with what I said above, that will also free the port 80 for another project if need be. Start them again using the now familiar docker-compose up -d . You can also delete all the stopped containers of the current project running: docker - compose rm Just like its equivalent docker rm , you can add the option -v if you want to remove the corresponding volumes as well (if you don't, you might end up with dangling volumes as already mentioned earlier). Check the docker and docker-compose references for more details. Troubleshooting Obviously, not everything is always going to work at once. Docker Compose might refuse to build an image or start a container and what is displayed in the console is not always very helpful. When something goes wrong, run: docker - compose logs This will display the logs for the containers of the current docker-compose.yml file. You can also run docker-compose ps and check the State column: if there is an exit code different than 0 , there was a problem with the container. Display the logs specific to a container with: docker logs containerid The container's name will work too. Conclusion Docker is an amazing technology and of course there is much more to it (I'm not even started with deployment and how to use it into production and believe me, it is very promising). Here we used Docker Toolbox and Boot2Docker for simplicity, because it sets up a lot of things for us automatically, but we could have used a Vagrant box just the same and install Docker on it. There is no obligation whatsoever. Putting all this together was no trivial exercise, especially as I knew very little about Docker before I started to write this article. It actually took me a couple of days of tinkering around before I started to make any sense of it. Docker is evolving super quickly and there are many resources out there from which it's not always easy to separate the wheat from the chaff. I believe I only kept the wheat, but then again I'm still a humble beginner so if you spot some chaff, you are very welcome to let me know about it in the comments. Sources Discovering Docker (e-book) How I develop in PHP with CoreOS and Docker Docker for PHP Developers Understanding Volumes in Docker Manage data in containers Tips for Deploying NGINX (Official Image) with Docker Use the Docker command line Compose CLI reference Dockerfile reference","tags":"DevOps","url":"https://tech.osteel.me/posts/from-vagrant-to-docker-how-to-use-docker-for-local-web-development"},{"title":"How to use the fork of a repository with Composer","text":"When using packages maintained by other developers, you may eventually find yourself waiting for a fix, an update, or the merge of a PR that will be available with the next release. If you can't wait, a workaround is to fork the corresponding repository, make the changes you need and then use your fork instead of the original package. Say I want to use my own version of Guzzle – I would edit composer.json like so ( osteel is my GitHub username): { ... \"repositories\" : [ { \"type\": \"vcs\", \"url\": \"git@github.com:osteel/guzzle\" } ] , \"require\" : { \"guzzlehttp/guzzle\" : \"dev-master\" } ... } This would use the master branch of my fork. If my fix were in a branch named bugfix , I would require dev-bugfix instead. I would then run the following command: $ composer update guzzlehttp/guzzle It would download and use my fork instead of the original package. Done! Just make sure to change it back to the latter once your fix is merged 😉","tags":"PHP","url":"https://tech.osteel.me/posts/how-to-use-the-fork-of-a-repository-with-composer"},{"title":"Handling CORS with Nginx","text":"[UPDATE 2015/08/02] As @OtaK_ pointed out , in most cases CORS should be handled directly by the app as it should return the allowed verbs by endpoint, instead of all of them being allowed by Nginx. This config should only be used for quick development, of a prototype or PoC for example, or if you are certain that the same verbs are allowed for all the endpoints (that would be the case for the assets returned by a CDN, for instance). [/UPDATE] With the always wider adoption of API-driven architecture, chances are you already had to deal with cross-origin resource sharing at some point. Whilst it is possible to deal with it from the code and you will find many packages or snippets to do so, we can remove the CORS handling from our app and let the HTTP server take care of it. The Enable CORS website contains useful resources to this end, but when I tried to use their Nginx config for my own projects it didn't quite work as expected. The following examples are based on the Nginx server configurations generated by Homestead , but the steps won't change much even if you are not using Laravel's dev environment. nginx-extras First of all, Nginx's traditional add_header directive doesn't work with 4xx responses. As we still want to add custom headers to them, we need to install the ngx_headers_more module to be able to use the more_set_headers directive, which also works with 4xx responses. While the documentation suggests to build the Nginx source with the module, if you are on a Debian distro you can actually easily install it with the nginx-extras package : sudo apt - get install nginx - extras The server configuration Here is what a typical server config of a Laravel project looks like, without the CORS bit (I am voluntarily omitting the SSL part to keep the post short, but it works exactly the same): server { listen 80 ; server_name example - site . com ; root \" /home/vagrant/projects/example-site/public \" ; index index . html index . htm index . php ; charset utf - 8 ; location / { try_files $ uri $ uri / / index . php ?$ query_string ; } location = / favicon . ico { access_log off ; log_not_found off; } location = / robots . txt { access_log off ; log_not_found off; } access_log off ; error_log / var / log / nginx / example - site . com - error . log error ; sendfile off ; client_max_body_size 100 m ; location ~ \\. php $ { fastcgi_split_path_info &#94; ( . + \\. php )( / . + ) $ ; fastcgi_pass unix : / var / run / php5 - fpm . sock ; fastcgi_index index . php ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $d ocument_root $fa stcgi_script_name ; fastcgi_intercept_errors off ; fastcgi_buffer_size 16 k ; fastcgi_buffers 4 16 k ; } location ~ / \\. ht { deny all ; } } Now, with the CORS handling: server { listen 80 ; server_name example - site . com ; root \" /home/vagrant/projects/example-site/public \" ; index index . html index . htm index . php ; charset utf - 8 ; more_set_headers ' Access-Control-Allow-Origin: $http_origin ' ; more_set_headers ' Access-Control-Allow-Methods: GET, POST, OPTIONS, PUT, DELETE, HEAD ' ; more_set_headers ' Access-Control-Allow-Credentials: true ' ; more_set_headers ' Access-Control-Allow-Headers: Origin,Content-Type,Accept,Authorization ' ; location / { if ( $ request_method = ' OPTIONS ' ) { more_set_headers ' Access-Control-Allow-Origin: $http_origin ' ; more_set_headers ' Access-Control-Allow-Methods: GET, POST, OPTIONS, PUT, DELETE, HEAD ' ; more_set_headers ' Access-Control-Max-Age: 1728000 ' ; more_set_headers ' Access-Control-Allow-Credentials: true ' ; more_set_headers ' Access-Control-Allow-Headers: Origin,Content-Type,Accept,Authorization ' ; more_set_headers ' Content-Type: text/plain; charset=UTF-8 ' ; more_set_headers ' Content-Length: 0 ' ; return 204 ; } try_files $ uri $ uri / / index . php ?$ query_string ; } location = / favicon . ico { access_log off ; log_not_found off; } location = / robots . txt { access_log off ; log_not_found off; } access_log off ; error_log / var / log / nginx / example - site . com - error . log error ; sendfile off ; client_max_body_size 100 m ; location ~ \\. php $ { fastcgi_split_path_info &#94; ( . + \\. php )( / . + ) $ ; fastcgi_pass unix : / var / run / php5 - fpm . sock ; fastcgi_index index . php ; include fastcgi_params ; fastcgi_param SCRIPT_FILENAME $d ocument_root $fa stcgi_script_name ; fastcgi_intercept_errors off ; fastcgi_buffer_size 16 k ; fastcgi_buffers 4 16 k ; } location ~ / \\. ht { deny all ; } } And that is pretty much it. All you need to do now is to reload your Nginx confs: sudo service nginx reload Extra considerations Note that this allows any domain to access your app, and while this is most likely enough for local development, on a production server you might want to fine-tune this configuration to allow specific domains only ( Access_Control_Allow_Origin ). More generally, all the headers' values are examples and you can modify them as you see fit. You could also put the global and options-related snippets into separate files (in /etc/nginx/shared/ , for example) and import them with the Nginx's include directive.","tags":"DevOps","url":"https://tech.osteel.me/posts/handling-cors-with-nginx"},{"title":"Database management with Adminer (and how to install on Homestead)","text":"For quite some time now I have been prefering accessing databases from the CLI, but sometimes it can feel overkill when wanting to quickly check or update something, say. In this kind of case I usually look to PHPMyAdmin, but as I was looking for an equivalent for PostgreSQL I stumbled upon Adminer . It is basically a single file PHP script allowing to connect to and manage different DBMSs (namely MySQL, PostgreSQL, SQLite, MS SQL, Oracle, Firebird, SimpleDB, Elasticsearch and MongoDB) super easily. Here are a few simple steps to use it on Homestead, but the process wouldn't differ much whatever your development environment is. First, download adminer.php , rename it index.php and move it to an adminer folder under your usual Homestead projects directory. Add a new site to your Homestead.yaml file (adapt the path to your own config if necessary): sites : - map : adminer . local to : / home / vagrant / projects / adminer Re-provision your box: $ cd ~/Homestead $ vagrant provision Edit your local hosts file to match the Homestead box IP address to the domain (on a Mac that's /etc/hosts ): 192 . 168 . 10 . 10 adminer . local You should now be able to access http://adminer.local : To access the MySQL databases, simply select \"MySQL\" in the \"System\" dropdown, type \"localhost\" in \"Server\" (there is a placeholder that looks like the value is entered by default but you actually need to type it in), and the default \"homestead\" and \"secret\" as \"Username\" and \"Password\" respectively (you can also tick the \"Permanent login\" box so the details are added as a link on the left column for quick access). The default PostgreSQL user doesn't have a password so you need to set one first. ssh your Homestead box and run: $ sudo -u postgres psql You should be connected to PostgreSQL (notice postgres=# at the beginning of the prompt). Now type: $ postgres = # alter user postgres password 'secret'; You've just set up a password for the default user ( \"secret\" , just like for MySQL). Quit this interface typing \\q and try to access the PostgreSQL databases from Adminer (select \"PostgreSQL\" in the \"System\" dropdown, type \"localhost\" in \"Server\" , and \"postgres\" and \"secret\" as \"Username\" and \"Password\" respectively). I personnally don't like the default theme, and there are a few other designs available from the website . Download whichever you fancy (I went for \"pappu687\" ) and place the adminer.css file at the same level as the index.php file you moved earlier. Refresh http://adminer.local . You're done!","tags":"Laravel","url":"https://tech.osteel.me/posts/database-management-with-adminer-and-how-to-install-on-homestead"},{"title":"Laravel Homestead: debug an API with Xdebug and cURL in Sublime Text","text":"Foreword There are a few tutorials out there about how to set up Sublime Text and Xdebug so they play nice together. The good news is that in our case, Homestead has covered the configuration of Xdebug for us: the tool is already available and reporting for duty. You will find its settings in /etc/php5/fpm/conf.d/20-xdebug.ini , which should look like this: zend_extension = xdebug . so xdebug . remote_enable = 1 xdebug . remote_connect_back = 1 xdebug . remote_port = 9000 xdebug . max_nesting_level = 250 I won't go into more detail about this, but you can have a look at this post by sitepoint for a more complete explanation. Might be worth mentioning that my host machine runs Mac OS, and the steps below might slightly differ if you are on a different OS. Prerequisites This is assuming Homestead is installed on your machine and that you've got a basic knowledge of it. If you are confused by the documentation, this free Laracast might help. I also wrote a short guide on how to set up a basic PHP/MySQL project on Homestead . We will use a small Laravel project (v5.0.31) for testing purpose, which you can clone from this repository , inside a directory of your choice (you probably already have one containing your Laravel projects): git clone git @github . com : osteel / xdebug - api - blog - tutorial . git xdebug - api Add the site in your ~/.homestead/Homestead.yaml file: sites : - map : xdebug - api . local to : / home / vagrant / path / to / xdebug - api / public Where /path/to/ should be replaced with the path to the project on your Homestead box. Match the Homestead IP address with \"xdebug-api.local\" in your local hosts file ( /etc/hosts on a Mac) (of course, adapt the IP address if you changed the default one): 192 . 168 . 10 . 10 xdebug - api . local You should now re-provision your Homestead box. Go to ~/Homestead/ and run vagrant provision if the box is already running, or vagrant up --provision if it needs to be started. You should now be able to access http://xdebug-api.local , which should display the Laravel welcome page. Sublime Text We first need to get the Xdebug Client package for Sublime. If you've never installed any package before, the easiest way is to install package control first. Follow the steps, then hit shift + cmd + p in Sublime (or shift + ctrl + p on Windows) and type \"install\" until the \"Package Control: Package Install\" entry appears: Hit enter and wait for the packages input to display. Type \"xdebug\" and hit enter after having selected the Xdebug Client one to install it. We now need to set up a Sublime Text project. Open the project folder in Sublime, then go to the \"Project\" menu, \"Save Project As...\". Name it \"xdebug-api.sublime-project\" and save it at the root. Open it and replace its content with: { \"folders\" : [ { \"follow_symlinks\" : true , \"path\" : \".\" } ], \"settings\" : { \"xdebug\" : { \"url\" : \"http://xdebug-api.local/\" , \"path_mapping\" : { \"/home/vagrant/path/to/xdebug-api/\" : \"/path/to/xdebug-api/\" } } } } where the two occurrences of /path/to/ should be replaced with the path to the project on the Homestead box and the path to the project on your host machine respectively. Using Xdebug Let's make sure everything is correctly set up. The project is extremely simple and offers a couple of endpoints, both in the WelcomeController.php file and declared in routes.php . One will receive GET requests and the other POST ones (corresponding to the get and post methods in WelcomeController.php ). Route : : get ( '/' , 'WelcomeController@index' ); Route : : get ( '/get' , 'WelcomeController@get' ); Route : : post ( '/post' , 'WelcomeController@post' ); Since we installed the package, there is a new \"Xdebug\" menu available under \"Tools\": Add a breakpoint in the index method of WelcomeController.php (notice the circle on the left): Now start the debugging mode (\"Tools\", \"Xdebug\", \"Start Debugging (Launch Browser)\"): your default browser will open http://xdebug-api.local/?XDEBUG_SESSION_START=sublime.xdebug . The page should just hang in there, and switching back to Sublime Text you should see something like that: The script execution stopped at your breakpoint, and Xdebug displays information about the different objects available at this point and their values in the panels at the bottom. Now you might think that you could basically test all the GET endpoints of your API this way, and you would not be entirely wrong. But how about other methods (POST, PUT, etc.)? And what if you need a specific header, such as a bearer token? That's where cURL comes in handy. cURL First let's add another breakpoint, in the get method this time: Open a terminal on your host machine and run (cURL is available on Mac OS by default, but you might need to install it if you use another platform): curl http : // xdebug - api . local / get You should get an array of pizzas in JSON format. The breakpoint was ignored, the reason being Xdebug needs a cookie to be read so its session can be started (that's what happens when you append \"?XDEBUG_SESSION_START=sublime.xdebug\" to the URL: a cookie is created). What we are going to do is to \"add\" this cookie to the cURL call. Now run: curl - b \"XDEBUG_SESSION=sublime.xdebug\" http : // xdebug - api . local / get The script execution should stop at the breakpoint and Sublime Text should look like: What happened here is we told cURL to pass a cookie to the request, created from a string ( \"XDEBUG_SESSION=sublime.xdebug\" ). Now let's try a POST request. Create a new breakpoint in the post method: And run: curl - b \"XDEBUG_SESSION=sublime.xdebug\" - X POST - H \"Content-Type: application/json\" - d '{\"name\":\"Pepperoni\"}' http : // xdebug - api . local / post Displaying back Sublime Text; you should get something like: To explain the command a little bit: -X allows to specify the method ( GET by default) -H allows to specify headers -d allows to pass some data (here, some JSON) There are many things you can do with cURL, that should cover pretty much all the cases presented by your API. I encourage you to have a look at its documentation . Extra considerations Alias You will probably agree that having to type -b \"XDEBUG_SESSION=sublime.xdebug\" every time is somewhat annoying. To speed up the process a little bit, I use an alias command I named curlx . To add it on a Mac for example, edit ~/.bash_profile : vim ~/ . bash_profile Add this line: alias curlx = 'curl -b \"XDEBUG_SESSION=sublime.xdebug\"' Then source the file so the new alias is taken into account: source ~/ . bash_profile And run: curlx http : // xdebug - api . local / get If the breakpoint previously set in the get method is still active, your script should stop there. cURL command handy You may also have noticed that I put a cURL command in the description of each method in WelcomeController.php : I do so so I can test them quickly, with a terminal open on the side. How to use Xdebug Finally, this tutorial doesn't actually explain how you might use Xdebug. For concrete examples, you can watch a couple of Laracast videos (free upon opening an account): Xdebug (from 5'20'') Xdebug and Laravel These videos were shot using PHPStorm, but the described processes apply all the same.","tags":"Laravel","url":"https://tech.osteel.me/posts/laravel-homestead-debug-an-api-with-xdebug-and-curl-in-sublime-text"},{"title":"Extending Homestead: how to customize Laravel's Virtual Machine (the example of Apache)","text":"Homestead offers a nice pre-packaged environment. But as a project grows in complexity, there will be a time where extra packages will be necessary. How to install them properly, and not to lose everything any time we need to recreate the box? How does one extend Homestead? Foreword Impatient kids can skip to the next section . The more I use Laravel, the more I enjoy it. I am also quite a fan of Vagrant . That's actually one of the things that got me interested in the framework in the first place: that they were offering a pre-packaged Vagrant box (namely Homestead). The fact they are using such a cool tool can only be a good sign, right? But then I realized they were implicitely advocating using the same box for all sorts of PHP projects. I understand that most of them wouldn't even use all of the included features , but it nonetheless feels like it goes against the principle of Vagrant, that is isolating project environments and more specifically mimicking the production server as closely as possible. It's like leaving the window open for the \"it works on my machine\" syndrom when the door has been carefully locked. It also poses another problem: if the included software is pretty good as it is, chances are developers will need extra packages at some point. There will be those who don't really know how to install them, as everything was set up for them from the start, and those who are comfortable enough to do it from the CLI, but then what if they need to recreate the Vagrant box for some reason? All the extra stuff is gone. I stumbled upon Yitzchak Schaffer's post on the subject, where I got introduced to the \"Laragarden\" term, which defines the feeling quite well. He explains his concerns in a more detailed way than I do, and if you are interested in the subject I invite you to read his post. Now those familiar with Vagrant will probably have thought of provisioning by now, and that was my thinking too. I started to look for a clean way to customize Homestead, which led me to this post , in which the author advocates creating a bash script to run any time the Homestead box is (re)created. This looked clean enough to me, except that I couldn't be bothered executing the script manually every time, hence my decision to edit Homestead's VagrantFile instead. That's when I realised the guys behind Laravel had that covered already. More on that in a bit. Prerequisites This is assuming you already have homestead running on your machine . If you are confused by the documentation, this free Laracast might help. Might be worth mentioning that my host machine runs Mac OS, and the steps below might slightly differ if you are on a different OS. Custom provisioning Looking at the VagrantFile , I noticed a few interesting lines: afterScriptPath = File . expand_path ( \"~/.homestead/after.sh\" ) and, later on: if File . exists ? afterScriptPath then config . vm . provision \" shell \" , path : afterScriptPath end And here is the content of ~/.homestead/after.sh : # If you would like to do some extra provisioning you may # add any commands you wish to this file and they will # be run after the Homestead machine is provisioned . Gotta love Laravel... What it means is that after all the base Homestead set up is done, the script will look for a file named after.sh in the .homestead/ folder under your user's directory, and run its content if it exists. This file is created along with the Homestead.yaml file when you first run bash init.sh after cloning the Homestead repository. As ~/.homestead/ is an independent folder, and if you work with other developers on the same project(s), it might be a good idea to version it for consistency across the team. From there, all that is left to do is actually to put the necessary install scripts in after.sh . Handy! The example of Apache Let's take a simple example here. Homestead comes with Nginx preinstalled (nothing bad about that, quite the opposite actually), but you might have some projects that need to be run on Apache. If both servers cannot run at the same time, there is no harm in having them both installed and starting one or the other according to the current need. From your host machine, open a terminal window and edit the after.sh mentioned above: vim ~/ . homestead / after . sh Add these lines at the end of it: sudo apt - get update sudo apt - get install - y apache2 What this will do is it will update the package lists to make sure we'll grab the latest version of the Apache one, and then install it ( -y is there to answer \"yes\" by default where the user is normally prompted). Save the file and go to ~/Homestead/ , then run: vagrant provision This will force Homestead to re-run the provision scripts, and install the Apache server using the script you've just added. ssh your Homestead box and run: sudo service apache status If everything went alright, you should get something like that: * apache2 is not running This is expected, as it means that 1. Apache has been correctly installed and 2. Nginx is already running and we can't have both running at the same time. What we need to do now is to stop Nginx first and then start Apache: sudo service nginx stop sudo service apache2 start Accessing your Homestead machine from the browser should now display the default Apache screen (chances are the address is http://192.168.10.10 ): That being said and done, you should be aware that Homestead will not create the Apache Virtual Hosts for you (as it does for the Nginx server confs). You will have to add them yourself in /etc/apache2/ . This also means that if you recreate the Homestead box, these Virtual Hosts will be gone (it is not that difficult to use provisioning to automate this, but this is beyond the scope of this article - more on that in the conclusion). Conclusion Here is how to extend your Homestead box in a clean and simple way (all it takes is to update an existing file, after all). Of course it implies using bash scripts and that might be a bit frustrating for Chef or Puppet users (even though I don't think there would be any harm in updating the Vagrantfile directly), but I'd say it is a good start already, and it kinda shows that Laravel's intent is not necessarily to lock you up in their Laragarden (even though you won't find any mention of after.sh in the official documentation, at the time of writing). To my opinion, Homestead is definitely a good kick-start and a nice first exposure to Vagrant to some users, but as a project grows in complexity, using a dedicated Vagrant box might be a good idea. By the way, if you want to learn more about Vagrant, I wrote an article explaining how to set up a Vagrant box for local development step by step . It will also show you how to import a Nginx server config, which can be adapted to Apache Virtual Hosts. Just sayin' ;)","tags":"Laravel","url":"https://tech.osteel.me/posts/extending-homestead-how-to-customize-laravels-virtual-machine-the-example-of-apache"},{"title":"How to start a new Laravel 5 project with Homestead - quick reference","text":"I wrote this short get-started guide mainly for my own use, to have a reference handy to quickly set up a new Laravel project with a MySQL database. But as I felt the need to write it, one might find some interest in it as well. [UPDATE 11/05/2015]: As the use of Composer to install Homestead has been deprecated (at least it disappeared from the doc), this post is not using the homestead commands anymore. Prerequisites This is assuming homestead is installed . If it is not the case, please follow the instructions first: once completed, you will have a fully provisioned virtual machine run by Vagrant and containing everything necessary to develop with Laravel in the best conditions (if you are having trouble following the different steps, here is a free Laracast video that might explain it better). If you have no idea what Vagrant is and why you need to install it along with something like VirtualBox, feel free to read the article I wrote on the subject . A note about ssh Laravel's documentation suggests to add an alias to ssh the Homestead box more quickly. I personnally prefer to use an ssh config to that purpose, as it is intended to. Update or create a config file in the .ssh folder of your home directory: vim ~/ . ssh / config Add the following content: # Homestead Host homestead HostName 127 . 0 . 0 . 1 Port 2222 User vagrant You can now access the Homestead machine from anywhere running: ssh homestead Create the project First add a new site to the ~/.homestead/Homestead.yaml file: vim ~/ . homestead / Homestead . yaml Something like: sites : - map : site1 . local to : / home / vagrant / projects / site1 - map : new - site . local to : / home / vagrant / projects / new - site / public Here we want to start a new Laravel project (arbitrarily called \"new-site\") so we point the root to the public/ directory. There is also another method, using the serve script, allowing to add a new project without having to edit Homestead.yaml nor to provision the box again. My preference goes to the first method because I like keep track of my existing projects in Homestead.yaml . I don't specify the database in this file, because I don't want the databases to be reset every time I provision the Vagrant box. Speaking of which, we now need to run this command, from ~/Homestead/ : vagrant provision or: vagrant up — - provision if your box isn't running yet. This will basically create the right Nginx config on the Vagrant box and restart Nginx in there (and, like I said, reset the databases that are declared in the Homestead.yaml file, so be careful. Simply remove the databases for which you don't want that to happen under databases from that file). Now, edit the hosts file of your host machine ( sudo vim /etc/hosts on MacOS) to match the new domain to the box's IP: 192 . 168 . 10 . 10 new - site . local Then, ssh the box and create your new project using Composer (in my config, all my projects are under the ~/projects/ directory of the Vagrant box) (this will probably take a little while): ssh homestead cd projects composer create - project laravel / laravel new - site This will copy Laravel and all its dependencies in the new-site directory. If everything went fine, you should now be able to access http://new-site.local and see the default Laravel screen: Set up the database From the Homestead box, connect to MySQL and create the database with the right user (the password for homestead is \"secret\"): mysql - uhomestead - p create database newsite ; grant usage on * . * to newsite @localhost identified by 'password' ; grant all privileges on newsite . * to newsite @localhost ; flush privileges ; \\ q Be sure to note the chosen password and edit the .env file of your project, changing the values for the database details: DB_HOST = localhost DB_DATABASE = newsite DB_USERNAME = newsite DB_PASSWORD = password Laravel comes with a couple of database migration scripts, creating the users and password_resets tables respectively. A quick way to check that the database is correctly set up is to run these scripts, from the project's root: php artisan migrate If the migration is successful, you're all set! Reset the database if necessary: php artisan migrate : reset That's it! A few quick steps to get started on a new Laravel project with Homestead.","tags":"Laravel","url":"https://tech.osteel.me/posts/how-to-start-a-new-laravel5-project-with-homestead-quick-reference"},{"title":"Install and deploy a Pelican blog using Fabric - Part 4: workflow, extras and conclusion","text":"Alright! This was a bit of a long road, but we are finally getting there. In the previous part , we used Fabric to fully provision a server and pull our content from a Git repository. In this fourth and last part, we are going to review a complete worklow, take a few extra steps to complete our blog and conclude our journey. Summary Part 1: local environment Part 2: installation and configuration Part 3: Fabric Part 4: workflow, extras and conclusion Complete workflow Extras Images, favicon and other static files Google Analytics Sitemap Feeds Conclusion Sources Complete Workflow Let's have a look at what a complete workflow would look like: it will also summarize all that we have done so far. Here is mine: Create a new article directly under \"content/posts/\" , with \"Status: draft\" Edit the content (I personally use Mou , a Markdown editor for Mac) Generate and serve the blog locally: fab reserve Access http://localhost:8000/drafts and check the look of the article Edit and push the article to the Git repository as often as necessary until it is finished When ready to publish, create the right year/month/day folders under \"content/\" , move the article there and remove \"Status: draft\" Git commit and push fab publish Check the article on the live website ...and Bob's your uncle. Extras If you followed all the steps up to now, you already have a fully functional blog. But there are a few extra things you will probably want to add in. Some of them are coming from the tips and tricks page. Images, favicon and other static files Chances are you will want to add images to some of your articles. They will have to be stored somewhere and copied over at compilation. To that end, create a folder named \"images\" in \"content\" . I personally follow the same structure as for the articles, e.g. I place pictures under \"images/2015/02/22/\" for articles published on that day. To have this directory copied to \"output/\" with the rest of the content, open \"pelicanconf.py\" and add this section: STATIC_PATHS = [ 'images' ] This simply indicates to the generation script that this directory is to be copied as is under \"output/\" . And this is how you would embed images in your articles (Markdown syntax): ! [ \"Example image\" ]( / images / 2015 / 02 / 22 / example . jpg \"Example image\" ) How about a favicon? Create another directory under \"content/\" , named \"extra\" . Place your favicon in there, and edit the config file again: STATIC_PATHS = [ 'images' , 'extra' ] EXTRA_PATH_METADATA = { 'extra/favicon.ico' : { 'path' : 'favicon.ico' } } The \"EXTRA_PATH_METADATA\" allows to specify more precisely the path of specific files. Here, we basically say that we want \"favicon.ico\" from the \"extra\" directory to be copied at the root of the blog. You can add as many files as you wish in there, such as a \"robots.txt\" , for example. Google Analytics Pelican supports Google Analytics out of the box. All it takes is to add the following line to the \"publishconf.py\" file ( \"UA-XXXX-Y\" being your own tracking id): GOOGLE_ANALYTICS = \"UA-XXXX-Y\" Sitemap Under \"theme/templates/\" , add a \"sitemap.html\" file: < ? xml version = \" 1.0 \" encoding = \" UTF-8 \" ? > < urlset xmlns = \" http://www.sitemaps.org/schemas/sitemap/0.9 \" > { % for article in articles % } < url > < loc > {{ SITEURL }} / {{ article . url }} </ loc > < priority > 0 . 8 </ priority > </ url > { % for translation in article . translations % } < url > < loc > {{ SITEURL }} / {{ translation . url }} </ loc > < priority > 0 . 8 </ priority > </ url > { % endfor % } { % endfor % } { % for page in pages % } < url > < loc > {{ SITEURL }} / {{ page . url }} </ loc > < priority > 1 . 0 </ priority > </ url > { % for translation in page . translations % } < url > < loc > {{ SITEURL }} / {{ translation . url }} </ loc > < priority > 1 . 0 </ priority > </ url > { % endfor % } { % endfor % } </ urlset > Edit \"pelicanconf.py\" : DIRECT_TEMPLATES = (( 'index' , 'tags' , 'categories' , 'archives' , 'sitemap' )) SITEMAP_SAVE_AS = 'sitemap.xml' Feeds Are also supported by default. This is just a matter of configuration, which is all well explained in the documentation . Conclusion Well well well. Obviously this was way more complicated than just opening a Tumblr or a Wordpress. I assume that if you chose to go down that road, it was also for the educational aspects that were coming with it. So did I. And really, I have learned a lot putting all this together, and I hope you are taking away a little bit of new knowledge as well. So now what? Well first, tap yourself on the back and take a break, you deserved it. Anything unclear? Don't hesitate to refer to the result repository , it might be helpful. Or just drop a line in the comments, I'll be happy to help. Then, here are a few leads if you want to go further. The first and obvious one is to host everything on a real server. I personally use a DigitalOcean (referral link) droplet, because it is cheap and easy to setup (the most basic one is more than enough to serve a static HTML blog like Pelican). In any case, once you have got your server, all it should take is to update the corresponding section of the \"fabfile.py\" file, as described at the end of the \"provision\" section . You could also add a new Fabric function to speed up the process of starting a new article (have a look at the make version of this on the tips and tricks page for inspiration, or use it as is if you prefer). Finally, if you are using GitHub to host your repository, why not trying to use a webhook to make the publication even easier? Sources This tutorial is the result of the combination of many different sources. Apart from the official Pelican documentation and the Fabric one , here are the articles and people that were helpful: How to use Pelican, GitHub, and a DigitalOcean VPS to host a cool blog : this is the post that truly got me started. I was having a hard time finding a complete resource and I found all I needed to get the ball rolling in this excellent work How I built this website, using Pelican: Part 1 - Setup Howto Setup Comments with Disqus in Pelican Finally, thanks to Josh for having introduced me to Fabric","tags":"Blogging","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-4-workflow-extras-and-conclusion"},{"title":"Install and deploy a Pelican blog using Fabric - Part 3: Fabric","text":"In part 2 , we covered the installation and configuration of Pelican in our local environment. It is now time to provision our server and publish content using Fabric. But first, let's version our blog. Summary Part 1: local environment Part 2: installation and configuration Part 3: Fabric Versioning Deployment with Fabric fabfile.py Provision Publish Part 4: workflow, extras and conclusion Versioning The next stop in our journey is versioning. What we want to do is to be able to push our content to some repository so it can be pulled on the server later on (and it is always good to have a backup that we can clone and start working on locally in no time). Create a new repo on your favorite host (like GitHub or Bitbucket ) and set up your local one: git init git remote add origin your - repo - address Create a \".gitignore\" file ( \".vagrant/\" is not required if you don't use Vagrant): output /* *.py[cod] cache/ .vagrant/ We exclude the content of \"output/\" as the content will be generated directly on the live server, with its own settings. Make your first commit and push your code: git add - A git commit - m 'First commit' git push That's it! Deployment with Fabric Fabric is a command-line tool written in Python allowing to perform remote operations over SSH. It is often used for server provisioning and deployment. We installed it earlier with pip ( pip install Fabric ) and started using it already to build and serve our blog locally, using the fab build and fab serve commands among others. These commands are contained in a file named \"fabfile.py\" , at the root of your blog. We are now going to have a look at it and explain it a little bit. fabfile.py Let's open the file in and editor and dissect it section by section: from fabric.api import * import fabric.contrib.project as project import os import sys import SimpleHTTPServer import SocketServer Basically the imports of all the necessary libraries. SimpleHTTPServer is the one used by fab serve , for example. # Local path configuration ( can be absolute or relative to fabfile ) env . deploy_path = 'output' DEPLOY_PATH = env . deploy_path # Remote server configuration production = 'root@localhost:22' dest_path = '/var/www' # Rackspace Cloud Files configuration settings env . cloudfiles_username = 'my_rackspace_username' env . cloudfiles_api_key = 'my_rackspace_api_key' env . cloudfiles_container = 'my_cloudfiles_container' This section is about configuration. We will update the \"Remote server configuration\" bit soon. We are not using Rackspace in this tutorial so we can just ignore this part. The following function definitions (e.g. \"def clean()\" ) are all the commands you can run from the terminal (using \"fab command_name\" as you already know). You are already familiar with a few of them ( build , serve , reserve ) and I encourage you to take a look at the others and try them out (you might find that you are more comfortable with some of them for your workflow). The code for most of these commands is quite self-explanatory. Now let's have a look at the last one, \"publish\" : @ hosts ( production ) def publish (): local ( 'pelican -s publishconf.py' ) project.rsync_project ( remote_dir = dest_path , exclude = \".DS_Store\" , local_dir = DEPLOY_PATH.rstrip ( '/' ) + '/' , delete = True , extra_opts = '-c' , ) What's happenning here? First, we indicate that we want to ssh the host whose configuration is contained in the \"production\" variable mentioned above. Then, in the body of the function itself, we first generate the HTML locally using the live config ( \"publishconf.py\" ), and we synchronize the output with the remote server's destination directory defined by the \"dest_path\" variable and the \"DEPLOY_PATH\" environment variable. The synchronization is performed using project.rsync_project , which is a wrapper for the rsync command, allowing to upload newly modified files only. Basically, everything is there already for you to update a remote server with new content with a single command executed locally. And honestly that might be just enough for your needs. But it implies that the remote server already exists and is properly set up. And we'd rather see the new content being pulled from our Git repository instead. Now, you could just stop here for today. I won't be mad, promised. But if you are interested in seeing how to update the fabfile to both provision our server and publish our versionned content, stick with me. Provision Still there? Good. In our context, what's behind the word \"provisioning\" is the act of installing all the required software, packages, dependencies, etc for a project to work on a server. But before that, we are going to create the different error pages. By default, when trying to access a page that doesn't exist for example, the default HTTP server's 404 page will be displayed. And it's ugly. And even if we all agree that the true beauty comes from the inside, we don't want it to be ugly. Under \"content/pages/\" , create a new folder named \"errors/\" and the files \"403.md\" , \"404.md\" and \"50x.md\" in it (adapt the extensions to the format you chose). Here is the content of my 404 page as an example: Title : Hmm ... Slug : 404 Status : hidden Nope . Don ' t know what you ' re talking about , pal . [ Go home ]( / \"Back to home\" ). Not much new here, except for the \"hidden\" status, whose effect is to prevent it from being displayed along with the other pages ( \"about\" etc). Follow the same format for the two other pages. When you are done, create a new file called \"blog.conf\" under the \".provision/\" directory, with this content: server { listen 80 ; ## listen for ipv4; this line is default and implied listen [::]: 80 default ipv6only = on ; ## listen for ipv6 # Make site accessible from http : // my - blog . local . com server_name my - blog . local . com ; root / var / www / blog ; location = / { # Instead of handling the index , just # rewrite / to / index . html rewrite &#94; / index . html ; } location / { try_files $ uri . htm $ uri . html $ uri = 404 ; } access_log / var / log / blog / access . log ; error_log / var / log / blog / error . log ; # Redirect server error pages error_page 500 502 503 504 / pages / 50 x . html ; error_page 404 / pages / 404 . html ; error_page 403 / pages / 403 . html ; } You may have recognized the nginx format: it is indeed the HTTP server I am going to use (feel free to adapt this to your favorite one). This is a rather basic config: the website will be accessible at http://my-blog.local.com , its root will be \"/var/www/blog/\" on the remote server, the logs will be written under \"/var/log/blog/\" , and the errors will be redirected to the different pages you have just created. Now, open back \"fabfile.py\" and, before the \"publish\" function, add a \"provision\" one (it could be after as well, but the provisioning is supposed to come before the publication, right? But maybe that is just my OCD speaking): @ hosts ( production ) def provision () : if run ( ' nginx -v ' , warn_only = True ) . failed : sudo ( ' apt-get -y install nginx ' ) sudo ( ' rm /etc/nginx/sites-available/default ' ) sudo ( ' service nginx start ' ) put ( ' ./.provision/blog.conf ' , ' /etc/nginx/sites-available/blog.conf ' , use_sudo = True ) sudo ( ' rm -f /etc/nginx/sites-enabled/blog.conf ' ) sudo ( ' ln -s /etc/nginx/sites-available/blog.conf /etc/nginx/sites-enabled/blog.conf ' ) if run ( ' test -d %s/%s ' % ( log_path , sitename ) , warn_only = True ) . failed : sudo ( ' mkdir %s/%s ' % ( log_path , sitename )) if run ( ' test -d %s ' % root_path , warn_only = True ) . failed : sudo ( ' mkdir %s ' % root_path ) if run ( ' git -v ' , warn_only = True ) . failed : sudo ( ' apt-get install -y git-core ' ) if run ( ' pip --version ' , warn_only = True ) . failed : run ( ' wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py -P /tmp/ ' ) sudo ( ' python /tmp/get-pip.py ' ) run ( ' rm /tmp/get-pip.py ' ) if run ( ' fab --version ' , warn_only = True ) . failed : sudo ( ' pip install Fabric ' ) if run ( ' virtualenv --version ' , warn_only = True ) . failed : sudo ( ' pip install virtualenv ' ) sudo ( ' pip install virtualenvwrapper ' ) run ( ' echo \"export WORKON_HOME=$HOME/.virtualenvs\" >> /home/vagrant/.bashrc ' ) run ( ' echo \"source /usr/local/bin/virtualenvwrapper.sh\" >> /home/vagrant/.bashrc ' ) with prefix ( ' WORKON_HOME=$HOME/.virtualenvs ' ) : with prefix ( ' source /usr/local/bin/virtualenvwrapper.sh ' ) : run ( ' mkvirtualenv %s ' % sitename ) sudo ( ' service nginx restart ' ) Quite a few things here but, if you look closely, you will realize these are basically all the steps you have taken at the beginning of the tutorial. The pattern is almost always the same: test if the package is installed ( if run('package --version', warn_only=True).failed: ) install it if it is not The \"warn_only=True\" parameter allows Fabric not to exit in case of a command failure: this is exactly what we want, i.e. knowing if the command fails so we can install the missing package. \"with prefix\" allows to execute the subsequent commands in the context of the current one. If your version of Python is prior to 2.6, you will need to add from __future__ import with_statement at the top of the file. We will ssh the \"server\" box as the \"vagrant\" user, meaning the \"run\" commands will be executed with its permissions, and the \"sudo\" ones with the root permissions, just like we did throughout this tutorial. Back to the actual script: we first install nginx if necessary (and remove the default config), start it, copy the server config file we created earlier to the right location, and recreate the symlink. Then we ensure the destination directories for the logs and the blog's generated HTML exist, that Git is installed, then pip, Fabric, virtualenv and virtualenvwrapper, we create the \"blog\" virtual environment and, finally, we restart nginx. Again, if you followed this tutorial from the start, this should feel familiar. Now as I am using Ubuntu 14.04.1 LTS boxes, I know my \"remote\" server comes with Python preinstalled. If yours doesn't, well you will have to add the steps to install it :) Shall we test this now? Sure thing, but we need to do something first. Remember the configuration section of \"fabfile.py\" mentioned earlier? It is time to set up the details of our remote server. This is where the use of Vagrant comes in handy as, if you used the Vagrant config I gave you at the beginning of this post, then the \"remote server\" box is already there, reporting for duty. Open a new terminal on your host machine, go to the blog's root, and type this: vagrant up server Now update \"fabfile.py\" , changing the server configuration for this one: # Remote server configuration production = ' vagrant @192.168.72.3 : 22 ' env . key_filename = ' / home / vagrant / . ssh / insecure_private_key ' root_path = ' / var / www ' log_path = ' / var / log ' dest_path = ' ~/ dev ' sitename = ' blog ' symlink_folder = ' output ' The IP address is the one that was specified in the Vagrant config, and the private SSH key is the Vagrant one, copied over from the host machine, via this line of the config: # Copy the default Vagrant ssh private key over local . vm . provision \"file\" , source : \"~/.vagrant.d/insecure_private_key\" , destination : \"~/.ssh/insecure_private_key\" As mentioned earlier, we will ssh the box as the \"vagrant\" user. The rest of the variables are path/folder names used in both the \"provision\" and \"publish\" functions (next section). Now, from the \"local\" Vagrant machine: fab provision If everything is set up properly, you should see a series of text lines scrolling off the screen: your \"server\" box is being provisionned :) When it is done, you can ssh your \"server\" VM and play around to observe that everything was properly installed: vagrant ssh server Publish We are now able to provision a server with everything required to run our blog, and all it takes is to update a few lines of configuration in \"fabfile.py\" and running one command. Pretty cool, eh? Anyway, we are yet to update the \"publish\" function to automate the publication of new content, pulling it from our Git repository. Here is what it looks like: @ hosts ( production ) def publish () : if run ( ' cat ~/.ssh/id_rsa.pub ' , warn_only = True ) . failed : run ( ' ssh-keygen -N \"\" -f ~/.ssh/id_rsa ' ) key = run ( ' cat ~/.ssh/id_rsa.pub ' ) prompt ( \" Add this key to your Git repository and then hit return: \\n\\n %s \\n\\n \" % key ) if run ( ' test -d %s ' % dest_path , warn_only = True ) . failed : run ( ' mkdir %s ' % dest_path ) with cd ( dest_path ) : if run ( ' test -d %s ' % sitename , warn_only = True ) . failed : run ( ' mkdir %s ' % sitename ) with cd ( sitename ) : run ( ' git clone %s . ' % git_repository ) if run ( ' test -d %s ' % symlink_folder , warn_only = True ) . failed : run ( ' mkdir %s ' % symlink_folder ) sudo ( ' ln -s %s/%s/%s %s/%s ' % ( dest_path , sitename , symlink_folder , root_path , sitename )) with cd ( sitename ) : run ( ' git reset --hard HEAD ' ) run ( ' git pull origin master ' ) with prefix ( ' WORKON_HOME=$HOME/.virtualenvs ' ) : with prefix ( ' source /usr/local/bin/virtualenvwrapper.sh ' ) : run ( ' workon %s ' % sitename ) run ( ' pip install -r requirements.txt ' ) run ( ' fab preview ' ) First, we check if there is an existing SSH private key for the \"vagrant\" user (or whatever user you set up): we are going to pull the content from a Git repository over SSH, so we need one. If none is found, the script will generate one for us and display it so we can add it to our repo. Once this is done, just hit return to continue the execution. The destination path is then created if necessary: in our case, the repository is cloned and updated in \"~/dev/blog\" . From there, a symlink is created between \"~/dev/blog/output\" and \"/var/www/blog\" , so the generated HTML files alone are in \"/var/www/blog\" . Finally, the Virtual Environment is activated, the pip dependencies installed, and the content generated with the live config. Let's test our new function: fab publish Once the execution is over, you should be able to see your blog at the private IP address 192.168.72.3 . There is one last little step to take to access it from the server name as defined in the nginx config. Open the \"hosts\" file of your host machine and add the following line (change the domain for the one you chose, if different): 192 . 168 . 72 . 3 my - blog . local . com You can now access http://my-blog.local.com . That's it! You now know how to provision a server and publish your content using Fabric. In the next part , we will review a complete workflow, implement a few extra things and conclude this tutorial with a few openings on what to do to go further.","tags":"Blogging","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-3-fabric"},{"title":"Install and deploy a Pelican blog using Fabric - Part 2: installation and configuration","text":"In part 1 , we set up a local environment containing everything Pelican requires to run properly. Let's move on to the installation and configuration of Pelican itself. Summary Part 1: local environment Part 2: installation and configuration Pelican installation Writing content Theme Configuration URLs Plugins Comments Part 3: Fabric Part 4: workflow, extras and conclusion Pelican installation If you are using The Vagrant config provided in part 1, start the \"local\" box and ssh it if it is not already the case: vagrant up local vagrant ssh local Then, activate the Virtual Environment: workon blog Go to the directory you want your blog to reside in (if you are using Vagrant, this is \"/vagrant\" ) and type: pip install pelican Pelican allows to use either reStructuredText or Markdown formats for your articles. I personally use Markdown, but the choice is up to you. You can also use plain HTML if that's your thing. Install your weapon of choice: pip install Markdown Now is a good time to save our current list of dependencies. Type this: pip freeze This command will give you the list of the packages that are installed in your VE. It should more or less look like this: blinker == 1 . 3 docutils == 0 . 12 feedgenerator == 1 . 7 Jinja2 == 2 . 7 . 3 Markdown == 2 . 5 . 2 MarkupSafe == 0 . 23 pelican == 3 . 5 . 0 Pygments == 2 . 0 . 2 python - dateutil == 2 . 4 . 0 pytz == 2014 . 10 six == 1 . 9 . 0 Unidecode == 0 . 4 . 17 pip allows you to save this list into a file, in order to quickly reinstall its content if you need to (on another machine, for example): pip freeze > requirements . txt All you need to do to reinstall this environment somewhere else is: pip install - r requirements . txt Handy. Now let's set up the skeleton for your blog using the built-in wizard: pelican - quickstart Pelican will now ask you a series of questions. Some of them have a value between square brackets at the end: this is the default value you can select simply hitting return . I will only list the questions that might be a bit confusing here: Do you want to specify a URL prefix ? e . g ., http : // example . com ( Y / n ) Only if you already have a domain name that will point to your blog. You will be able to update this later directly in the publish config file. Do you want to generate a Fabfile / Makefile to automate generation and publishing ? ( Y / n ) Do you want an auto - reload & simpleHTTP script to assist with theme and site development ? ( Y / n ) Answer \"Yes\" to both, these are functionalities we are going to use. Then say \"No\" to all the different means to upload your blog. You might end up wanting to use one of the listed methods, but they are not covered in this article. Done . Your new project is available at / vagrant Sweet! Now let's have a glance at the default look of the blog. Type this: fab build Then: fab serve You've just launched a local webserver, that uses the port 8000 by default. Open your browser and navigate to http://localhost:8000 : the default skeleton and template should display (this also works with the Vagrant box because we activated the port forwarding option, cf the Vagrantfile in the Vagrant way section ). That was easy, wasn't it? Interrupt the server and regain control of your terminal typing ctrl + c . fab build and fab serve are Fabric commands. The first one generates the HTML content (more on this in the next section) and the second one creates the server. You can also use the shortcut command fab reserve that runs both ones in turn. Fabric is not the only way to generate content, spawn a HTTP server etc. You can read more about that in the online documentation . Here I choose to use Fabric simply because this is also what we are going to use for pubication later on. Better get familiar with it right now. I will give more details about it in part 3. Writing content How about actually writing something now? Create a file in the \"content\" folder, something like \"my-first-post.md\" (put the appropriate extension if you didn't go for Markdown). Add some content in it, following this format at the beginning: Author : osteel Title : My first post ! Date : 2015 - 02 - 22 Slug : my - first - post Category : test ## A subtitle Some ** Markdown ** content , with some * formatting *. A list : - Milk - Butter - Eggs Etc . Pelican will analyze the first lines to properly generate the post. Rebuild and serve your blog with fab reserve , and reload http://localhost:8000 : you should see your post. The generated HTML files are put into the \"output\" folder. A \"test\" category was automatically created and placed in the header (if you had specified none, it would have created a category called \"misc\" by default). This is basically how to write articles. Pelican also allows to create static pages that are not posts (typically, the \"about\" or \"contact\" pages). Simply add a \"pages\" folder under \"content\" , and edit a \"about.md\" file: Title : About Slug : about Amazing blog . Regenerate the content and refresh the web page: a new \"about\" entry has been placed in the header. These are the very basics of writing content. You probably wonder how to tweak the template to your taste now. Don't worry, we are getting there. But first, let's pick a theme! Theme The default theme is nice, but chances are you will want to change it. Pelican comes with a variety of themes to choose among the official repository ones or custom ones made by various people. I am not going to invent much here and will mostly follow the instructions available on the repository's page. First, clone all the themes in a local directory (if you are using the Vagrant box, you will probably want to install Git now - sudo apt-get install -y git-core ): git clone --recursive https://github.com/getpelican/pelican-themes ~/pelican-themes Then, open the \"pelicanconf.py\" file and add these lines at the end (change \"vagrant\" for the correct username if necessary): # Theme THEME = \"/home/vagrant/pelican-themes/mnmlist\" Rebuild and serve: fab reserve Reload http://localhost:8000 : you are now looking at the \"mnmlist\" theme! Test as many themes as you like until you find one that suits you. I personally went for the Octopress one, ported from Octopress by Maurizio Sambati. Once you picked one, copy its content in a new \"theme\" folder in your blog's directory: mkdir theme cp - rf ~/ pelican - themes / mnmlist /* theme/ Edit \"pelicanconf.py\" again and change the value of \"THEME\" for the new location: THEME = \"theme\" Rebuild and refresh to make sure it worked. You can now remove the other themes: rm - rf ~/ pelican - themes Most of the themes have their own settings. Just have look at the theme's own README file to know what they are (here is the \"mnmlist\" one, for example). Configuration We had a quick preview of the configuration in the previous section, when we changed the theme's path. Pelican actually has two configuration files: \"pelicanconf.py\" , with whom we made acquaintance already, and \"publishconf.py\" , which contains production-wise settings. The latter should contain settings that are relevant to your live environment only; we will see examples later on. Your \"pelicanconf.py\" file should currently look like something like that: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals AUTHOR = u 'osteel' SITENAME = u 'My Blog' SITEURL = '' PATH = 'content' TIMEZONE = 'Europe/Paris' DEFAULT_LANG = u 'en' # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None # Blogroll LINKS = (( 'Pelican' , 'http://getpelican.com/' ), ( 'Python.org' , 'http://python.org/' ), ( 'Jinja2' , 'http://jinja.pocoo.org/' ), ( 'You can modify those links in your config file' , '#' ),) # Social widget SOCIAL = (( 'You can add links in your config file' , '#' ), ( 'Another social link' , '#' ),) DEFAULT_PAGINATION = 10 # Uncomment following line if you want document-relative URLs when developing #RELATIVE_URLS = True # Theme THEME = \"theme\" Most of the parameters are pretty straightforward, and I invite you to have a look at the documentation that describes them all. I am going to focus on specific areas: URLs, plugins and comments. URLs For now, our posts' URLs look like so: http : // sitename . com / article - slug . html We are going to change this to adopt a format like the one I am using for my blog: http : // sitename . com / year / month / day / article - slug . html Obtaining this result is pretty easy. Open the \"pelicanconf.py\" file and add these lines: # URLs ARTICLE_URL = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' ARTICLE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' Now, let's move the post we created earlier in a folder reflecting this structure. Go to the \"content\" folder and create subfolders as following (change the date if you want to): cd content mkdir 2015 2015 / 02 2015 / 02 / 22 Then move the post: mv my - first - post . md 2015 / 02 / 22 Rebuild and refresh: URLs are now a bit more structured. Of course, this is just an example, and you are free to adopt any format you want. Plugins Pelican supports plugins since its version 3.0, allowing to add functionalities without having to touch the core. Plugins are maintained in a separate repository just like the themes. You will find a description of each one of them there, and we are going to manage them the same way we did for the themes. git clone them all locally, and copy over those you want in a new \"plugins\" folder in your blog's directory. If you want to use the Liquid Tags plugin for example, this is how you would do: git clone https : // github . com / getpelican / pelican - plugins ~/ pelican - plugins mkdir plugins cp - rf ~/ pelican - plugins / liquid_tags plugins / Open \"pelicanconf.py\" and add these lines at the end: # Plugins PLUGIN_PATHS = [ 'plugins' ] PLUGINS = [ 'liquid_tags' ] Pelican now knows where to find the plugins and which ones to load. Rebuild and refresh. Copy as many plugins as you like, update the config file accordingly and, when you are done, delete the other ones: rm - rf ~/ pelican - plugins Comments Pelican natively supports Disqus , a third party service that will take care of your commenting system for free, externally (nothing to host). Head to the website and create an account. Set it up as you like and note your sitename, which is just the string before \".disqus\" in the URL of your account. For example, mine is \"https://osteel.disqus.com\" , so my sitename is \"osteel\" . Now edit \"pelicanconf.py\" and add the following line (with the right sitename, of course): DISQUS_SITENAME = \"osteel\" Build, serve and refresh: you should have a nice comment box at the bottom of your article. Note: We added this config parameter to \"pelicanconf.py\" so you could see the result straight away, but this actually only relevant in the context of your live environment (Disqus won't recognize your local URL). This is typically one of the settings that should be in the other config file, \"publishconf.py\" . That's it for today. In part 3 , we will see how to use Fabric to automate the provisioning of a server and the publication of new content.","tags":"Blogging","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-2-installation-and-configuration"},{"title":"Install and deploy a Pelican blog using Fabric - Part 1: local environment","text":"This series of articles will walk you through the different steps to install, setup and deploy your first Pelican blog (just like this one). If its aim is to be accessible to most people, there are a couple of pre-requisites that would definitely facilitate your progression: Being comfortable with your OS's CLI Being comfortable with Git A developer background is preferable overall, even though there is no need to be familiar with Python. I knew nothing about this language when I started using Pelican. If you are not technical at all however, this is probably not for you. Pelican is a static website generator, meaning that it does not require any server logic (typically, a database), making it lightweight and easy to host. The script spits out HTML files which are then served to the client. Wanna play straight away? Head to the GitHub repository now and follow the instructions. If you are stuck at any point during this tutorial, don't hesitate to refer to it as well. In this first part, we are just going to set up our local environment to be able to install Pelican later on. Summary Part 1: local environment The Vagrant way Python, pip and virtualenv Python pip virtualenv Part 2: installation and configuration Part 3: Fabric Part 4: workflow, extras and conclusion The Vagrant way I am quite a fan of Vagrant , and I would advise to use an empty Vagrant box to play around safely. If you are not familiar with Vagrant and wish to learn more about it, take a look at this tutorial . This is completely optional tho, especially if you are on Mac OS. For Windows users however, this is strongly recommended (there is a dedicated tutorial here about how to use Vagrant on Windows), even though the team behind Pelican seems to have ensured the compatibility for this platform. Specific steps for Windows are not covered in this series of articles. The following tutorial was made using Ubuntu 14.04.1 LTS boxes, and the part that will cover the deployment with Fabric relies on one of Vagrant's features ( \"Multi-Machine\" ). If you choose not to use Vagrant, you will still learn how to deploy your blog using Fabric, but you won't be able to test it right away. If you choose to take the Vagrant way however, use this Vagrantfile: # -*- mode: ruby -*- # vi: set ft=ruby : # Vagrantfile API/syntax version. Don't touch unless you know what you're doing! VAGRANTFILE_API_VERSION = \"2\" Vagrant . configure ( VAGRANTFILE_API_VERSION ) do | config | # Used box config . vm . box = \"ubuntu/trusty32\" config . vm . define \"local\" do | local | # Accessing \"localhost:8000\" will access port 8000 on the guest machine local . vm . network :forwarded_port , guest : 8000 , host : 8000 , auto_correct : true # Copy the default Vagrant ssh private key over local . vm . provision \"file\" , source : \"~/.vagrant.d/insecure_private_key\" , destination : \"~/.ssh/insecure_private_key\" end config . vm . define \"server\" do | server | # Private IP server . vm . network :private_network , ip : \"192.168.72.3\" end end Don't worry too much if you don't understand everything at first, I will explain a bit more in due time. Just know that this config will allow you to run two different machines: one will be your local one (conveniently named \"local\" ) and the other one (named \"server\" ) will be used to simulate a remote server. For now, begin your journey with: vagrant up local vagrant ssh local Unless stated otherwise, all the steps covered in this part and the next ones need to happen on the \"local\" machine. Python, pip and virtualenv Python Pelican is written in Python and is installed via pip , the language's recommended tool to install packages (it is basically what npm is to JavaScript). Pelican works with Python 2.7.x, and chances are it is already available on your OS. To make sure of this, open a terminal and type: python --version If the command fails, please head to the installation guide and follow instructions for your platform. Pelican should just work with Python 3.3+ as well, but as our intent is to use Fabric for deployment, we will stick to 2.7.x in this tutorial. pip If the version of Python you have is 2.7.9+, you are in luck because pip is already included. If not, install it this way: wget https : // bootstrap . pypa . io / get - pip . py - P / tmp / sudo python / tmp / get - pip . py rm / tmp / get - pip . py The -P option allows to prefix the downloaded file with the destination folder ( \"/tmp/\" in our case). Let's take this opportunity of having pip handy to install Fabric straight away: sudo pip install Fabric We are going to use it soon enough. We could now install Pelican right away using pip, but we are going to take an extra step before that. virtualenv If you are a developer, you may have come across situations where different projects need different versions of the same language to run, or different versions of other dependencies. Python is no exception to this, and it came up with a solution to address it: the use of Virtual Environments. A Virtual Environment (VE) isolates a set of dependencies for a project. We are going to use virtualenv along with its buddy virtualenvwrapper to that purpose. First, we need to install them using pip: sudo pip install virtualenv sudo pip install virtualenvwrapper By default, virtualenv will create a folder named after the environment's directly inside the project's folder, and will do so for each project. virtualenvwrapper adds a set of functionalities on top of virtualenv to make its use easier, and placing all the VEs in the same folder is one of them (much cleaner IMHO and prevents us from having to add the folders in each project's .gitignore file). Let's define where virtualenwrapper should gather all the folders: echo 'export WORKON_HOME=$HOME/.virtualenvs' >> / home / vagrant / . bashrc echo 'source /usr/local/bin/virtualenvwrapper.sh' >> / home / vagrant / . bashrc Remember that I am using a Vagrant box here, hence \"vagrant\" as the username. Change this for the right value if necessary. I place these commands at the end of my .bashrc file so that WORKON_HOME is correctly initialized any time I start a new shell. Let's source our .bashrc file so the shell is updated with the new values: source ~/ . bashrc It is now time to create our first VE. As we are setting up a blog, let's call it \"blog\" : mkvirtualenv blog Your prompt should now look like this: ( blog ) vagrant @vagrant - ubuntu - trusty - 32 : / vagrant $ Mind the \"(blog)\" bit at the beginning: it indicates that you are currently using the blog VE. mkvirtualenv not only creates it, but activates it as well. This is how you leave it: deactivate Now let's check that the VE's folder was added to the right location: ls $ WORKON_HOME ls ~/ . virtualenvs Both commands should give the same list, and the \"blog\" folder should be among them (only if you set \".virtualenvs\" as the destination folder earlier, of course). To activate your VE, use the following command: workon blog Here you are, ready to move on to the installation of Pelican itself. Note that all the steps above is how you would prepare an environment for a Python project in general. Just sayin' :) In the next part , we will see how to install and configure Pelican in our shiny new environment.","tags":"Blogging","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-1-local-environment"},{"title":"How to use Vagrant for local web development","text":"Heads-up! While Vagrant served me well for years, I do not recommend using it for local development anymore and advise to use Docker instead, for which I wrote an entire [tutorial series](/posts/docker-for-local-web-development-introduction-why-should-you-care \"Docker for local web development, introduction: why should you care?\") which I invite you to read. This article shows how to quickly get up and running with Vagrant, to create and use local Virtual Machines as development environments, all with a single command. This is indeed written from a web developer's standing point, and I will not spend too much time describing how things work under the hood (not that I am an expert anyway). The point of Vagrant is precisely not to have to worry too much about it. The final result of this tutorial is available as a Github repository . In case of trouble, don't hesitate to refer to it. Summary Vagrant? Basic installation Picking a box Vagrantfile Port-forwarding Private IP Provisioning Shell scripts Server config Matching the private IP Tips .gitignore Handle port collisions Copy host git config GUI console Access the host machine when using a private network What now? Provisioning tools Vagrant Share Vagrant? Vagrant greatly simplifies the use of Virtual Machines to spawn development environments in no time (well, it's probably more like no effort than time). To understand what a Virtual Machine (VM) is, think of an emulator: you install it on your computer so you can then run software that believe they are running in the environment they were designed for. All inside your own machine (which is then called the host). That is essentially what a VM is. Vagrant is a VM manager , in the sense that it reduces the management and the configuration of VMs to a handful of commands. It relies on a VM provider , that deals with virtualization itself. As its support is shipped with Vagrant, we will use VirtualBox, but others exist . So what is actually the point? The main argument is the consistency of the environments among developers working on the same project, and more importantly that these environments reflect the production ones. Ship a Vagrant configuration with each project, and every developer will work on the same environment locally. No surprises when pushing the code live, no more \"it works on my machine\" . The other advantages I see is the fact that one can still use their editor of choice, as Vagrant folders are shared with the host by default. It also permits not to clutter up your computer with tons of different libraries and software that aren't used by every project. If something goes wrong, you don't screw your machine: you destroy your VM instance and recreate it instead. Easy. And safe. Basic installation First, download VirtualBox at https://www.virtualbox.org/wiki/Downloads ( \"platform packages\" ) and install it. Then, download Vagrant at https://www.vagrantup.com/downloads.html (v1.7.2 at the time of writing), install. Open up a terminal and type: vagrant version The version should be displayed. If not, log out your session, log back in and try again. Picking a box Vagrant's documentation uses a Ubuntu 12.04 LTS 32-bit server box. Now let's say you want Ubuntu 14.0 LTS 32-bit : go to the catalog of Vagrant boxes and type \"Ubuntu 14.04\" in the search field. Spot the Ubuntu one in the list: \"ubuntu/trusty32\" . Now get back to your terminal, browse to the directory you want your project to reside in, and type this: vagrant init ubuntu / trusty32 The terminal should display something like: A ` Vagrantfile ` has been placed in this directory . You are now ready to ` vagrant up ` your first virtual environment ! Please read the comments in the Vagrantfile as well as documentation on ` vagrantup . com ` for more information about using Vagrant . Good. Now type: vagrant up This is how you start your Virtual Machine. As you picked Ubuntu 14.04 , Vagrant will try to install it: as it won't find a corresponding box on your computer, it will fetch it directly from the catalog (this can take a while according to your connection speed), and boot it once the download is over. All you have to do now is: vagrant ssh BOOM! You are now in your Ubuntu 14.04 server. Note for Windows users: the vagrant ssh command might not work for you if SSH is not in your PATH variable. I invite you to take a look at this separate article and come back. What exactly happened here? vagrant init created a Vagrantfile file in the directory. This file contains the various settings Vagrant needs to spawn a VM. You can have a look at it now, it is basically full of commented examples (don't freak out, it is not as bad as it looks). You will find one uncommented line tho: config . vm . box = \"ubuntu/trusty32\" Yeah, you got it: as we invoked vagrant init followed by the box we wanted to use, Vagrant created its config file with the corresponding setting. While you are connected to your box, type this: ls / vagrant The Vagrantfile should be listed. This is the same Vagrantfile you have just updated: /vagrant is the folder that is shared between the host machine and the VM. You can simply leave your box typing ctrl + d . I won't explain how to shutdown it; just take a couple of minutes to read about the different available options in the doc , they are well explained. Just know that even if you destroy your box, files under /vagrant remain untouched. Vagrantfile I am not going to go through all the options here, only those I most often use. The Vagrantfiles are written in Ruby, but no Ruby background is required (I don't know much about Ruby myself). Let's look into accessing your VM from the host. Using your host machine's editor to update files on your VM via shared folders is nice, but at some point you will want to admire your work in a browser, which implies for it to have access to your Vagrant box somehow. The easiest way to achieve this is probably using port-forwarding. Port-forwarding First let's run a quick test. Open a terminal on your host machine and type the following command: telnet localhost 8080 Unless the port 8080 is already in use by something else (in which case change 8080 for whatever port you know is available), you should get something like: Trying 127 . 0 . 0 . 1 ... telnet : connect to address 127 . 0 . 0 . 1 : Connection refused Trying :: 1 ... telnet : connect to address :: 1 : Connection refused telnet : Unable to connect to remote host Edit your Vagrantfile and add the following: config . vm . network : forwarded_port , guest : 80 , host : 8080 Now reload your VM: vagrant reload or vagrant up if you had shut it down. Once it is booted, try the telnet command above again in the other terminal window. This is what you should read: Trying 127 . 0 . 0 . 1 ... Connected to localhost . Escape character is '&#94;]' . Port 8080 is now being listened to, and connections to it are forwarded to port 80 of the Vagrant box. This comes in handy when you use something like grunt-contrib-connect to quickly spawn a static HTTP server on a specific port. In our case, using port 80 in the Grunt config and accessing http://localhost:8080 from our host machine would display the website contained in the folder set with the base parameter. This is nice for quick prototyping for example, as no further server configuration is required. In most cases however, as we want to replicate a production environment as accurately as possible, we will want to use a proper server config. To do so, we are going to assign a private IP to our box. Private IP First, go back to the terminal and try: ping 192 . 168 . 68 . 8 You should get request timeout responses. Edit the Vagrantfile again and add: config . vm . network : private_network , ip : \"192.168.68.8\" Reload your VM and try again: you should now get responses. This is it for the options I mainly use; I rarely need the rest. Be curious and scan through the official documentation , it is well written and I am sure you will find something useful to you. On a side note, and to emphasize the usefulness of Vagrantfiles, I think their beauty resides in the fact that all it takes to make the environment available to other developers is to version them with your projects. Whenever you clone a project containing a Vagrantfile to a machine with Vagrant installed on it, you are at a vagrant up away from having it running locally. The rest of the process to get your website displayed using the private IP is covered in the next section. Provisioning Alright, so you've got your VM running, you can edit files from outside of it, and access it from the host machine using its private IP. How to properly display your work in a browser now? Let's cut to the chase here: it implies setting a server on your VM and matching its IP to the domain name you chose in the host machine's hosts file. As our Vagrant box is almost empty at this point, we need to install a HTTP server on it. We will go for Nginx here, but instead of installing it and setting up the server from the VM itself, we are going to use provisioning. Shell scripts Provisioning is achieved from the Vagrantfile as well and, if different means are available to do so, I will only cover the most basic one for now, i.e. using shell scripts (I will mention the other ways later on). Open the Vagrantfile in your editor, and add this: config . vm . provision : shell , : path => \".provision/bootstrap.sh\" Basically what we tell Vagrant is \"Use the shell script that you will find in .provision/bootstrap.sh to provision the box\" . Don't reload your box just yet, as we need to create this file first. Add a new folder named \".provision\" in your project (same level as the Vagranfile), and create a bootstrap.sh file in it. Here is the full content of this file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/env bash # nginx sudo apt-get -y install nginx sudo service nginx start # set up nginx server sudo cp /vagrant/.provision/nginx/nginx.conf /etc/nginx/sites-available/site.conf sudo chmod 644 /etc/nginx/sites-available/site.conf sudo ln -s /etc/nginx/sites-available/site.conf /etc/nginx/sites-enabled/site.conf sudo service nginx restart # clean /var/www sudo rm -Rf /var/www # symlink /var/www => /vagrant ln -s /vagrant /var/www Now, let's describe it step by step: 1 #!/usr/bin/env bash We are basically telling where to look for the bash program. # nginx sudo apt - get - y install nginx sudo service nginx start Install Nginx and start it. The -y option allows to automatically answer \"yes\" where user input is normally required. # set up nginx server sudo cp / vagrant / . provision / nginx / nginx . conf / etc / nginx / sites - available / site . conf sudo chmod 644 / etc / nginx / sites - available / site . conf sudo ln - s / etc / nginx / sites - available / site . conf / etc / nginx / sites - enabled / site . conf sudo service nginx restart We copy the server configuration from .provision/nginx/nginx.conf (yet to be written at this point, we're getting there) to Nginx's sites-available folder, ensure the permissions are right, then create the symlink from sites-enabled/site.conf to sites-available/site.conf , and restart Nginx to take this new config into account. Finally, we create a symbolic link from /var/www to /vagrant , after having removed the default files Nginx creates at installation: # clean / var / www sudo rm - Rf / var / www # symlink / var / www => / vagrant ln - s / vagrant / var / www /var/www is often where the code goes on a server (even though /srv/www/ might be more relevant, but that's another debate ). As mentioned earlier, on a Vagrant VM the shared folder is /vagrant . Using such a symlink allows to match your production server's settings (or staging or whatevs). This is completely optional tho. Server config Almost there! If you are attentive, you know that we now need the Nginx server config. Under the .provision folder, create a new folder named \"nginx\" and open a new nginx.conf file in it. Here is its content: server { listen 80 ; server_name vagrant - test . local . com ; access_log / var / log / nginx / access . log ; error_log / var / log / nginx / error . log ; root / var / www ; location / { } } Nothing too esoteric here, this is a very basic Nginx server config. Just note that our server name will be \"vagrant-test.local.com\" . Save your file, and reload the box using this command: vagrant reload --provision Normally, the provisioning is done only at the first boot of the VM. By specifying the --provision option, we force Vagrant to perform the provisioning again. The start up will take slightly longer than previously, and you should see quite a lot more instructions on the screen: the packages listed in the bootstrap.sh file are being installed. Matching the private IP We only need two little extra steps now: adding the private IP and the server name to the host machine's hosts file and creating a simple index.html file in our box to be displayed in the browser later on. On Mac OS, edit the hosts file with the following command: sudo vim / etc / hosts On Windows, you will find it under Windows/System32/Drivers/etc/ (you will have to edit it in admin mode). Add the following line: 192 . 168 . 68 . 8 vagrant - test . local . com Save and quit. Now add a little index.html file at your project's root: <!DOCTYPE html> < html > < head > < title > Vagrant test </ title > </ head > < body > < h1 > Oh hi! </ h1 > </ body > </ html > Alright ladies and gentlemen, drumroll please: open your favorite browser and navigate to http://vagrant-test.local.com . You should see the HTML page you have just created ＼(&#94;o&#94;)／ Phew! That was a rocky ride, eh! Well, yeah. But think about it this way: save this basic config somewhere (craft it to your needs, adding PHP or whatever floats your boat), and use it anytime you start a new project. You can now get up and running with a simple vagrant up command and a new line in the hosts file. I don't know about you, but it sounds quite nice to me. Tips .gitignore If you use Git, add \".vagrant/\" to your .gitignore file. Vagrant creates this folder when booting the VM: it contains auto-generated stuff you don't want to version. Handle port collisions When using port-forwarding, it can happen that a Vagrant box uses a port that is not available. It will tell you about it anyway, but if the host machine's port doesn't really matter to you, amend the corresponding line in the Vagrantfile this way: config . vm . network : forwarded_port , guest : 8080 , host : 80 , auto_correct : true The auto_correct parameter will allow Vagrant to automatically assign another port of the host machine in case of unavailability (and tell you about it, of course). Copy host git config I usually try to avoid having too many terminal windows open at the same time. That's why I like to install Git on my VMs so I don't need another terminal window only to perform the versioning operations. This implies having your Git config inside the Vagrant box, which is actually quite easy to do automatically. In your Vagrantfile, add this: config . vm . provision \"file\" , source : \"~/.gitconfig\" , destination : \"~/.gitconfig\" At the first boot of the VM, your Git config file will be copied over. GUI console Occasionaly, you may have some trouble booting your VM. This can notably happen when your box wasn't properly shut down the last time you used it. The boot process might just hang there, and the reason may not be obvious. In that case, add the following lines to your Vagrantfile: config . vm . provider : virtualbox do | vb | vb . gui = true end and try to boot again. What this does is it will open VirtualBox's GUI console, which displays what is happening behind the scenes, and thus should help you troubleshoot the issue. Access the host machine when using a private network Sometimes, you may need to access the host machine from the guest one. How to do so? When you set up a private network, 192.168.68.8 in our case, the host machine automatically takes 192.168.68.1 as its private IP address. What now? Well, that is quite a lot to digest already. And yet this is just the beginning, there are many more features to explore. Just to mention a couple of them tho: Provisioning tools In this tutorial, we used simple shell scripts. But it turns out Vagrant (supposedly) plays nicely with more advanced solutions such as Chef or Puppet . Admitedly tho, I tried to use Puppet something like a year ago, and completely failed to get it working with Vagrant. I am yet to give Chef a try. Vagrant Share Local development is good, but soon you will be confronted with the need to expose your work to the world, whether it is to show some progress to a client or to allow an API to reach your application. This is achievable with nice tools such as ngrok (hopefully I will soon publish a tutorial about it), but Vagrant unveiled a new service a few months ago, called \"Vagrant Share\" , that does just that. I am yet to try it out (a tutorial would probably follow), but it definitely looks promising.","tags":"DevOps","url":"https://tech.osteel.me/posts/how-to-use-vagrant-for-local-web-development"},{"title":"How to use Vagrant on Windows","text":"[UPDATE 2020/03/05]: While Vagrant served me well for years, I do not recommend using it for local development anymore and advise to use Docker instead, for which I wrote an entire tutorial series which I invite you to read. This article shows how to deal with Windows' specificities while trying to work with Vagrant. If you are not familiar at all with the latter, I suggest you go through this Vagrant tutorial first. Note: the following was tested on Windows 8, but the steps described below shouldn't change too much between the different versions. Vagrant ssh The first issue I came across was that vagrant ssh doesn't work out of the box. I was greeted with a constructive message: `ssh` executable not found in any directories in the % PATH % variable . Is an SSH client installed ? Try installing Cygwin , MinGW or Git , all of witch contain an SSH client . Or use your favorite SSH client with the following authentication information shown below : Host : 127 . 0 . 0 . 1 Port : 2222 Username : vagrant Private key : C : / path / to / project / . vagrant / machines / default / virtualbox / private_key Fine. Let's install Git , then (considering it is not already the case). Git install The key is when the \"Adjusting your PATH environment\" screen pops up: You want to select \"Use Git and optional Unix tools from the Windows Command Prompt\" . Now I know the message in red looks quite scary, but honestly, unless you are a hardcore user of the Windows console, there is not much to worry about. Basically it will override some of the commands and add a few others. Personally, it never caused me any trouble. If you are still a bit worried tho, be reassured: none of this is irreversible. All you would need to do is uninstall Git, or update the PATH variable removing the incriminated part. More on that in a minute. Try to vagrant ssh your VM again, this time it should do it (you might need to open a new terminal for the update to take effect, tho). What if Git is installed already? Well, it was the case for me as well. You could always remove it and install it again, but there is another way. You will have to do manually what the installation of Git could have done for you, but fortunately it is quite trivial: Open the Control Panel Go to System and Security Click on System , then on the Change Settings button Display the Advanced tab and click on Environment Variables... Look for the Path variable in the System variables list, select it then Edit... At the end of the string, add the path to Git's bin (something like \"C:\\Program Files\\Git\\bin\" ) (don't forget to add a semicolon first to separate it from the previous path): Validate and close the different menus. Try to vagrant ssh your box again, it should work (again, you might need to open a new terminal first). You probably guessed it already, but if you don't want Git's commands to override the Windows ones anymore, all you need to do is to remove that bit. You will need to find another way for ssh to work though! Ah, but wait. There is another way. PuTTY Remember that error message we initially got trying to ssh the box? Let's have a look at the second part of it: Or use your favorite SSH client with the following authentication information shown below : Host : 127 . 0 . 0 . 1 Port : 2222 Username : vagrant Private key : C : / path / to / project / . vagrant / machines / default / virtualbox / private_key I wasn't joking when I said it was constructive, because it really tells you what to do. The Windows console works ok but let's be honest, in the long run it is a real pain to use. It does the trick for a quick vagrant ssh but when the time comes to actually do some work on an Ubuntu server for example, a better shell is desirable. Enter PuTTY PuTTY is a very lightweight tool that allows to do a lot of cool stuff. Some of you are probably familiar with it already, and using it jointly with Vagrant is quite nice. We will use it to ssh our boxes, and rely on the info given by the message above to that purpose. First, download it if that is not the case already (the first putty.exe link will do). Download puttygen.exe as well, we are going to need it. PuTTY and PuTTYGen are stand-alone applications (no need to install them), so just double click on the .exe files. Let's open PuTTYGen first: PuTTY uses its own key format, and we need to convert Vagrant's one first. Click on File then Load private key , select the file indicated by the error message earlier (e.g. \"C:/path/to/project/.vagrant/machines/default/virtualbox/private_key\" ). Once selected, PuTTY is kind enough to tell us what to do with it: Ensure SSH-2 RSA is selected, and that the number in Number of bits in a generated key is 2048. Then click on Save private key (don't set a passphrase) and save it under your own user directory's .ssh folder, as \"vagrant_private_key\" . From now on, we will use this key for all the Vagrant boxes. Close PuTTYGen and open PuTTY. In the Hostname field, type 127.0.0.1 . In the Port one, 2222 . Ensure SSH is selected and, in the Saved Sessions field, type vagrant and click Save : Go to Connection then Data , and in the Auto-login username field, enter \"vagrant\" : Next, still under Connection , go to SSH then Auth . Browse for the key you generated earlier in the Private key file for authentication field. Now head back to the Session menu, save again the \"vagrant\" one. Now click on Open : if everything went alright, you should now be in your Vagrant box ＼(&#94;o&#94;)／ Using multiple Vagrant boxes simultaneously Now let's say you already have a box running, and you need to start a second one. You vagrant up it, the Virtual Machine boots and you want to SSH it. But all the boxes cannot use the same SSH port! It all happens when the box is being booted: See the highlighted line? Seeing port 2222 was busy already, Vagrant picked the port 2200 instead. Now to SSH it using PuTTY, open it, load the \"vagrant\" session, and in the Port field, replace \"2222\" with \"2200\" . Click Open : there you are, connected to the second box. Known limitations Shared folders and symlinks One of the fairly known limitations of using Vagrant on Windows with VirtualBox is that the latter won't let you create symlinks on the shared folders for security reasons. This quickly becomes problematic when dealing with npm packages, for example. One of the workarounds is to use the \"no bin link\" parameter (e.g. npm install --no-bin-link ), but this is not always enough. Fortunately, there is a way to bypass this restriction. In your Vagrantfile, add the following piece of config: config . vm . provider \" virtualbox \" do | v | v . customize [ \" setextradata \" , : id , \" VBoxInternal2/SharedFoldersEnableSymlinksCreate/v-root \" , \" 1 \" ] end As Windows won't let standard users create symlinks, you now need to start your Vagrant box in administrator mode (open a Windows terminal in admin mode before running vagrant up , for example). Make sure no other box is already running though, as it won't start if VirtualBox is already running in standard mode. Maximum path length Another recurring problem comes from the fact that Windows' maximum length for a path is 255 characters. Again, this is quickly an issue when dealing with npm packages, especially when they have dependencies, themselves having dependencies, etc. The solution in that case is to create a symbolic link between the \"node_modules\" directory and another directory outside of the shared folders. Which brings us to our practical example. Practical example: npm packages So you have this project relying on npm packages. You tried to install them using --no-bin-link but no luck, looks like some of the paths are too long. Fear not, Macless: update your Vagrant config as shown above to allow the creation of symlinks, boot your VM in admin mode, create a destination directory for your npm packages somewhere outside of the shared folder and create a symlink between it and the \"node_modules\" one: mkdir ~/ node_modules ln - s / home / vagrant / node_modules / vagrant / node_modules cd / vagrant npm install Et voilà. Note: this implies preventing the \"node_modules\" directory from being versionned. Conclusion Here you go, now using Vagrant on Windows in decent conditions. The process can look a bit convoluted, and really it is. It took me quite a while to put everything together, and if today I am rather satisfied, I am still a bit bugged about the multiple Vagrant boxes part. Having to check the SSH port and update it in the PuTTY session everytime is a bit annoying, even though dealing with several instances at the same time might be an edge case. Anyways, if you have any suggestions about that, don't hesitate to leave a comment.","tags":"DevOps","url":"https://tech.osteel.me/posts/how-to-use-vagrant-on-windows"},{"title":"I've got a blog","text":"Yeah, finally. It should have been started 8 years ago but well, better late than never, eh? Let's say that's my very own Duke Nukem Forever. So what will be in there? Posts about dev, surely. There should be one about launching a blog like this one very soon. A blog using Pelican , that is. For those of you who happen to speak French and want to read about a random dude's life, go check out the other part of this blog . Enjoy the ride!","tags":"Blogging","url":"https://tech.osteel.me/posts/ive-got-a-blog"}]}
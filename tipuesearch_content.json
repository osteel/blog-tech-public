{"pages":[{"title":"About","text":"Oh hi! I'm Yannick Chenot, a French web developer contractor based in London. I hope you enjoy my blog! You can also check out my past works, CV and other details on my portfolio , or some code on my GitHub account . Want to get in touch? Drop me a line","tags":"pages","url":"https://tech.osteel.me/pages/about"},{"title":"Docker for local web development, part 1: a basic LEMP stack","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack ⬅️ you are here Part 2: put your images on a diet (coming soon) Part 3: a three-tier architecture with frameworks (coming soon) Part 4: smoothing things out with Bash (coming soon) Part 5.1: HTTPS all the things with a self-signed certificate (coming soon) Part 5.2: HTTPS all the things with a trusted Certificate Authority (coming soon) Part 6: expose a local container to the Internet (coming soon) Part 7: using a multi-stage build to introduce a worker (coming soon) Conclusion: where to go from here (coming soon) Appendix A: Docker on Windows Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post The first steps Identifying the necessary containers Docker Compose Nginx PHP MySQL phpMyAdmin Domain name Environment variables Commands summary and cleaning up your environment Conclusion The first steps I trust you've already read the introduction to this series and are now ready for some action. The first thing to do is to head over to the Docker website and download and install Docker Desktop for Mac or PC, or head over here for installation instructions on various Linux distributions. The second thing you will need is a terminal. To Windows users: if you haven't already, I strongly recommend you read this appendix before proceeding any further. It will show you which terminal to use, among other things. Once both requirements are covered, you can either get the final result from the repository and follow this tutorial, or start from scratch and compare your code to the repository's whenever you get stuck. The latter is my recommended approach for Docker beginners, as the various concepts are more likely to stick if you write the code yourself. Note that this post is quite dense because of the large number of notions being introduced. I assume no prior knowledge of Docker and I try not to leave any detail unexplained. If you are a complete beginner, make sure you have some time ahead of you and grab yourself a hot drink: we're taking the scenic route. Identifying the necessary containers Docker recommends running only one process per container, which roughly means that each container should be running a single piece of software. Let's remind ourselves what the programs underlying the LEMP stack are: L is for Linux; E is for Nginx; M is for MySQL; P is for PHP. Linux is the operating system Docker runs on, so that leaves us with Nginx, MySQL and PHP. For convenience, we will also add phpMyAdmin into the mix. As a result, we now need the following containers: one container for Nginx; one container for PHP (PHP-FPM); one container for MySQL; one container for phpMyAdmin. This is fairly straightforward, but how do we get from here to setting up these containers, and how will they interact with each other? Docker Compose Docker Desktop comes with a tool called Docker Compose that allows you to define and run multi-container Docker applications. Docker Compose isn't absolutely necessary to manage multiple containers, as doing so can be achieved with Docker alone, but in practice it is very inconvenient to do so (it would be similar to doing long division while there is a calculator on the desk: while it is certainly not a bad skill to have, it is also a tremendous waste of time). The containers are described in a YAML configuration file and Docker Compose will take care of building the images and starting the containers, as well as some other useful things like automatically connecting the containers to an internal network. Don't worry if you feel a little confused; by the end of this post it will all make sense. Nginx The YAML configuration file will actually be our starting point: open your favourite text editor and add a new docker-compose.yml file to a directory of your choice on your local machine (your computer), with the following content: version: '3.7' # Services services: # Nginx Service nginx: image: nginx:1.17 ports: - 80:80 The version key at the top of the file indicates the version of Docker Compose we intend to use (3.7 is the latest version at the time of writing). It is followed by the services key, which is a list of the application's components. For the moment we only have the nginx service, with a couple of keys: image and ports . The former indicates which image to use to build our service's container; in our case, version 1.17 of the Nginx image . Open the link in a new tab: it will take you to Docker Hub, which is the largest registry for container images (think of it as the Packagist or PyPI of Docker). Why not use the latest tag? You will probably notice that all images have a latest tag corresponding to the most up-to-date version of the image. While it might be tempting to use it, you don't know how the image will evolve in the future – it is very likely that breaking changes will be introduced sooner or later. The same way you do a version freeze for an application's dependencies (via composer.lock for PHP or requirements.txt in Python, for example), using a specific version tag ensures your Docker setup won't break due to unforeseen changes. Much like a Github repository, image descriptions on Docker Hub usually do a good job at explaining how to use it and what the available versions are. Here, we are looking at Nginx's official image: it is provided for and maintained by Nginx. Docker has become so popular most companies provide their own official image, which I always use whenever possible. They are easily recognisable: their page mentions Docker Official Images at the top, and Docker Hub separates them clearly from the community images when doing a search: Note the \"Verified Content\" at the top Back to docker-compose.yml : under ports , 80:80 indicates that we want to map our local machine's port 80 (used by HTTP) to the container's. In other words, when we will access port 80 on our local machine (i.e. your computer), we will be forwarded to the port 80 of the Nginx container. Let's test this out. Save the docker-compose.yml file, open a terminal and change the current directory to your project's before running the following command: $ docker-compose up -d It might take a little while as the Nginx image will first be downloaded from Docker Hub. When it is done, open localhost in your browser, which should display Nginx's welcome page: Congratulations: you have just created your first Docker container. Let's break down that command: by running docker-compose up -d , we essentially asked Docker Compose to build and start the containers described in docker-compose.yml ; the -d option indicates that we want to run the containers in the background and get our terminal back. You can see which containers are currently running by executing the following command: $ docker-compose ps Which should display something similar to this: To stop the containers, simply run: $ docker-compose stop At this point, you might be wondering what the difference is between a service, an image and a container. A service is just one of your application's components, as listed in docker-compose.yml . Each service refers to an image, which is used to start and stop containers based on this image. To help you grasp the nuance, think of an image as a class, and of a container as an instance of that class. Speaking of OOP, how about we set up PHP? PHP By the end of this section, we will have Nginx serving a simple index.php file via PHP-FPM , which is the most widely used process manager for PHP. Note: as mentioned in the introduction , while PHP is used on the server side throughout this series, swapping it for another language should be fairly straightforward. Replace the content of docker-compose.yml with this one: version: '3.7' # Services services: # Nginx Service nginx: image: nginx:1.17 ports: - 80:80 volumes: - ./src:/var/www/php:ro - ./.docker/nginx/conf.d:/etc/nginx/conf.d:ro depends_on: - php # PHP Service php: image: php:7.4-fpm working_dir: /var/www/php volumes: - ./src:/var/www/php A few things going on here: let's forget about the Nginx service for a moment, and focus on the new PHP service instead. We start from the php:7.4-fpm image, corresponding to the tag 7.4-fpm of PHP's official image , featuring version 7.4 and PHP-FPM. Let's skip working_dir for now, and have a look at volumes . This section allows us to define volumes (basically, directories or single files) that we want to mount onto the container. This essentially means we can map local directories and files to directories and files on the container; in our case, we want Docker Compose to mount the src folder as the container's /var/www/php folder. What's in the src/ folder? Nothing yet, but that's where we are going to place our application code. Once it is mounted onto the container, any change we make to our code will be immediately available, without the need to restart the container. Create the src directory (at the same lvel as docker-compose.yml ) and add the following index.php file to it: <!DOCTYPE html> <html> <head> <meta charset=\"UTF-8\"> <title>Hello there</title> <style> .center { display: block; margin-left: auto; margin-right: auto; width: 50%; } </style> </head> <body> <img src=\"https://tech.osteel.me/images/2020/03/04/hello.gif\" alt=\"Hello there\" class=\"center\"> </body> </html> It only contains a little bit of HTML and CSS, but all we need for now is to make sure PHP files are correctly served. Back to the Nginx service: we added a volumes section to it as well, where we mount the directory containing our code just like we did for the PHP service (this is so Nginx gets a copy of index.php , without which it would return a 404 Not Found when trying to access the file), and this time we also want to import the Nginx server configuration that will point to our application code: - ./.docker/nginx/conf.d:/etc/nginx/conf.d As Nginx automatically reads files ending with .conf located in the /etc/nginx/conf.d directory, by mounting our own local conf.d directory in its place we make sure the configuration files it contains will be processed by Nginx on the container. You may have noticed the addition of :ro at the end of both volumes: this option stands for read only and indicates that the container shall not modify the content of these volumes in any way. Create the .docker/nginx/conf.d folder and add the following php.conf file to it: server { listen 80 default_server; listen [::]:80 default_server; root /var/www/php; index index.php; location ~* \\.php$ { fastcgi_pass php:9000; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; } } Note: placing Docker-related files under a .docker folder is a common practice. This is a minimalist PHP-FPM server configuration borrowed from Linode's website that doesn't have much to it; simply notice we point the root to /var/www/php , which is the directory onto which we mount our application code in both our Nginx and PHP containers, and that we set the index to index.php . The following line is also interesting: fastcgi_pass php:9000; It tells Nginx to forward requests for PHP files to the PHP container's port 9000, which is the default port PHP-FPM listens on. Internally, Docker Compose will automatically resolve the php keyword to whatever private IP address it assigned to the PHP container. This is another great feature of Docker Compose: at start-up, it will automatically set up an internal network on which each container is discoverable via its service's name. Note on networks: Docker Compose sets up a network with the bridge driver by default, but you can also specify the networks. I've personally never used any other network than the default one, but you can read about other options here . Finally, let's have a look at the last configuration section of the Nginx service: depends_on: - php Sometimes, the order in which Docker Compose starts the containers matters. As we want Nginx to forward PHP requests to the PHP container's port 9000, the following error might occur if Nginx happens to be ready before PHP: [emerg] 1#1: host not found in upstream \"php\" in /etc/nginx/conf.d/php.conf:7 nginx_1 | nginx: [emerg] host not found in upstream \"php\" in /etc/nginx/conf.d/php.conf:7 nginx_1 exited with code 1 This causes the Nginx process to stop, and as the Nginx container will only run for as long as the Nginx process is up, the container stops as well. The depends_on configuration ensures the PHP container will start before the Nginx one, saving us an embarrassing situation. Your directory and file structure should now look similar to this: docker-tutorial/ ├── .docker/ │ └── nginx/ │ └── conf.d/ │ └── php.conf ├── src/ │ └── index.php └── docker-compose.yml We are ready for another test. Go back to your terminal and run the same command again (this time, the PHP image will be downloaded): $ docker-compose up -d Refresh localhost : if everything went well you will be greeted by the man who can disappear in a bathrobe. Update index.php (modify the content of the <title> tag, for instance) and reload the page: the change should appear immediately. If you run docker-compose ps you will observe that you now have two containers running: nginx_1 and php_1 . Let's inspect the PHP container: $ docker-compose exec php bash By running this command, we ask Docker Compose to execute Bash on the PHP container. You should get a new prompt indicating that you are currently under /var/www/php : this is what the working_directory configuration we ran into earlier is for. Run a simple ls to list the content of the directory: you should see index.php , which is expected as we mounted our local src folder onto the container's /var/www/php folder. Run exit to leave the container. Before we move on to the next section, let me show you one last trick. Go back to your terminal and run the following command: $ docker-compose logs -f Wait for a few logs to display, and hit the return key a few times to add some empty lines. Refresh localhost again and take another look at your terminal, which should have printed some new lines: This command aggregates the logs of every container, which is extremely useful for debugging: if anything goes wrong, your first reflex should always be to look at the logs. It is also possible to display the information of a specific container simply by appending the name of the service (e.g. docker-compose logs -f nginx ). Hit ctrl+c to get your terminal back. MySQL The last key component of our LEMP stack is MySQL. Let's update docker-compose.yml again: version: '3.7' # Services services: # Nginx Service nginx: image: nginx:1.17 ports: - 80:80 volumes: - ./src:/var/www/php:ro - ./.docker/nginx/conf.d:/etc/nginx/conf.d:ro depends_on: - php # PHP Service php: build: ./.docker/php working_dir: /var/www/php volumes: - ./src:/var/www/php depends_on: - mysql # MySQL Service mysql: image: mysql:8 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: demo volumes: - ./.docker/mysql/my.cnf:/etc/mysql/conf.d/my.cnf:ro - mysqldata:/var/lib/mysql # Volumes volumes: mysqldata: The Nginx service is still the same, but the PHP one was slightly updated. We are already familiar with depends_on : this time, we indicate that the new MySQL service should be started before PHP. But before we dive into its configuration, let's take a look at the new build section of the PHP service, which seemingly replaced the image one. Instead of using the official PHP image as is, we tell Docker Compose to use the Dockerfile from .docker/php to build a new image. A Dockerfile is like the recipe to build an image: every image has one, even official ones (see for instance Nginx's ). Create the .docker/php folder and add the following Dockerfile file to it: FROM php:7.4-fpm RUN docker-php-ext-install pdo_mysql PHP needs the pdo_mysql extension in order to read from a MySQL database. Although it doesn't come with the official image, the Docker Hub description provides some instructions to install PHP extensions easily. At the top of our Dockerfile, we indicate that we start from the official image, and we proceed with installing pdo_mysql with a RUN command. And that's it! Next time we start our containers, Docker Compose will pick up the changes and build a new image based on the recipe we gave it. A lot more can be done with a Dockerfile, and while this is a very basic example some more advanced use cases will be covered in subsequent articles. For the time being, let's update index.php to leverage the new extension: <!DOCTYPE html> <html> <head> <meta charset=\"UTF-8\"> <title>Hello there</title> <style> body { font-family: \"Arial\", sans-serif; font-size: larger; } .center { display: block; margin-left: auto; margin-right: auto; width: 50%; } </style> </head> <body> <img src=\"https://tech.osteel.me/images/2020/03/04/hello.gif\" alt=\"Hello there\" class=\"center\"> <?php $connection = new PDO('mysql:host=mysql;dbname=demo;charset=utf8', 'root', 'root'); $query = $connection->query(\"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_SCHEMA = 'demo'\"); $tables = $query->fetchAll(PDO::FETCH_COLUMN); if (empty($tables)) { echo '<p class=\"center\">There are no tables in database <code>demo</code>.</p>'; } else { echo '<p class=\"center\">Database <code>demo</code> contains the following tables:</p>'; echo '<ul class=\"center\">'; foreach ($tables as $table) { echo \"<li>{$table}</li>\"; } echo '</ul>'; } ?> </body> </html> The main change is the addition of a few lines of PHP code to connect to a database that does not exist yet. Let's now have a look at the MySQL service in docker-compose.yml : the image section points to MySQL's official image for version 8, and it is followed by a section we haven't come across yet: environment . It contains a couple of keys, MYSQL_ROOT_PASSWORD and MYSQL_DATABASE , which are environment variables that will be set on the container upon creation. They are documented in the image's description and essentially allow us to set the root password and create a default database respectively. In other words, a demo database will automatically be created for us when the container starts. After the environment key is the now familiar volumes . The first volume is a configuration file (in read only mode) we will be using to set the character set to utf8mb4_unicode_ci by default, which is pretty standard nowadays. Create the .docker/mysql folder and add the following my.cnf file to it: [mysqld] collation-server = utf8mb4_unicode_ci character-set-server = utf8mb4 Password plugin error: some versions of PHP prior to 7.4 are incompatible with MySQL's new default password plugin introduced with version 8. If you require an older version of PHP, you might also need to add the following line to the configuration file: default-authentication-plugin = mysql_native_password The second volume looks a bit different than what we have seen so far: instead of pointing to a local folder, it refers to a named volume defined in a whole new volumes section which sits at the same level as services : # Volumes volumes: mysqldata: We need such a volume because without it, every time the mysql service container is destroyed the database is destroyed with it. To make it persistent, we basically tell the MySQL container to use the mysqldata volume to store the data locally, local being the default driver (just like networks, volumes come with various drivers and options which you can learn about here ). As a result, a local directory is mounted onto the container, the difference being that instead of specifying which one, we let Docker Compose pick a location. Go back to your terminal and run docker-compose up -d again. Once it is done downloading the MySQL image and all of the containers are up and running, refresh localhost . You should see this: Note: you might initially get the following error: Fatal error: Uncaught PDOException: SQLSTATE[HY000] [2002] Connection refused . The reason is that MySQL hasn't built the demo database just yet, even though we specifically instructed the MySQL container to be started before the PHP one. The problem is the MySQL container is indeed started first, but only then does it proceed with creating the database, which means the PHP container might be ready before the database is. Keep refreshing for a few seconds; the error will eventually disappear. We now have Nginx serving PHP files that can connect to a MySQL database, meaning our LEMP stack is pretty much complete. The next steps are about improving our setup, starting with seeing how we can interact with the database in a user-friendly way. phpMyAdmin When it comes to dealing with a MySQL database, phpMyAdmin remains a popular choice; conveniently, they provide a Docker image which is pretty straightforward to set up. Not using phpMyAdmin? If you are used to some other tool like Sequel Pro or MySQL Workbench , you can simply update the MySQL configuration in docker-compose.yml and add a ports section mapping your local machine's port 3306 to the container's: ... ports: - 3306:3306 ... From there, all you need to do is configure a database connection in your software of choice, setting localhost:3306 as the host and root , root as login and password to access the MySQL database while the container is running. If you choose to do the above, you can skip this section altogether and move on to the next one. Open docker-compose.yml one last time and add the following service configuration after MySQL's: # PhpMyAdmin Service phpmyadmin: image: phpmyadmin/phpmyadmin:5 ports: - 8080:80 environment: PMA_HOST: mysql depends_on: - mysql We start from version 5 of the image and we map the local machine's port 8080 to the container's port 80. We indicate the MySQL container should be started first with depends_on , and set the host that phpMyAdmin should connect to using the PMA_HOST environment variable (remember that Docker Compose will automatically resolve mysql to the private IP address it assigned to the container). Save the changes and run docker-compose up -d again. The image will be downloaded, then, once everything is up, visit localhost:8080 : Enter root / root as username and password, create a couple of tables under the demo database and refresh localhost to confirm they are correctly listed. And that's it! That one was easy, right? Let's move on to setting up a proper domain name for our application. Domain name We have come a long way already and all that's left for today mostly boils down to polishing up our setup. While accessing localhost is functional, it is not particularly user friendly. Replace the content of .docker/nginx/conf.d/php.conf with this one: server { listen 80; listen [::]:80; server_name php.test; root /var/www/php; index index.php; location ~* \\.php$ { fastcgi_pass php:9000; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; } } We essentially removed default_server (since the server will now be identified by a domain name) and added the server_name configuration, giving it the value php.test , which will be our application's address. There is one extra step we need to take for this to work: as php.test is not a real domain name (it is not registered anywhere), you need to edit your local machine's hosts file so it recognises it. Where to find the hosts file? On UNIX-based systems (essentially Linux distributions and macOS), it is located at /etc/hosts . On Windows, it should be located at c:\\windows\\system32\\drivers\\etc\\hosts . You will need to edit it as administrator ( this tutorial should help if you are unsure how to do that). Add the following line to your hosts file and save it: 127.0.0.1 php.test Since we haven't updated docker-compose.yml nor any Dockerfile, this time a simple docker-compose up -d won't be enough for Docker Compose to pick up the changes. We need to explicitly tell it to restart the containers so the Nginx process is restarted and the new configuration is taken into account: $ docker-compose restart Your application is now available at php.test , as well as localhost . Environment variables We are almost there, folks! The last thing I want to show you today is how to set environment variables for the whole Docker Compose project, rather than for a specific service like we have been doing so far (using the environment section in docker-compose.yml ). Before we do that, I would like you to list the current containers: $ docker-compose ps Notice how each container is prefixed by the name of your project directory (which would be docker-tutorial if you cloned the repository ): Now, before we proceed further, let's destroy our containers and volumes so we can start afresh: $ docker-compose down -v Create a .env file alongside 'docker-compose.yml , with the following content: COMPOSE_PROJECT_NAME=demo Save the file and run docker-compose up -d again, followed by docker-compose ps : each container is now prefixed with demo_ . Why is this important? By assigning a unique name to your project, you ensure that no name collision will happen with other ones. If there are multiple Docker-based projects on your system that share the same name or directory name, and more than one use a service called nginx , Docker may complain that another container named xxx_nginx already exists when you bring up a Docker environment. While this might not seem essential, it is an easy way to avoid potential hassle in the future, and provides some consistency across the team. Speaking of which: if you've dealt with .env files before, you probably know that they are not supposed to be versioned and pushed to a code repository. Assuming you are using Git, you should add .env to a .gitignore file, and create a .env.example file that will be shared with your coworkers. Here is what the final directory and file structure should look like: docker-tutorial/ ├── .docker/ │ ├── mysql/ │ │ └── my.cnf │ ├── nginx/ │ │ └── conf.d/ │ │ └── php.conf │ └── php/ │ └── Dockerfile ├── src/ │ └── index.php ├── .env ├── .env.example ├── .gitignore └── docker-compose.yml That is the extent to which we need environment variables for this article, but you can read more about them over here . Commands summary and cleaning up your environment Before we wrap up, I'd like to summarise all of the commands we have been using so far, and throw a few more in so you can clean up your environment if you wish to. This can be used as a reference you can easily come back to if need be, especially in the beginning. Remember that they need to be run from your project's directory. Start and run the containers in the background $ docker-compose up -d If you update docker-compose.yml , an image or a Dockerfile, running this command again will pick up the changes automatically. Restart the containers $ docker-compose restart Useful when some changes require a process to restart, e.g. restart Nginx to pick up some server configuration changes. List the containers $ docker-compose ps Tail the containers' logs $ docker-compose logs [service] Replace [service] with a service name (e.g. nginx ) to display this service's logs only. Stop the containers $ docker-compose stop Stop and/or destroy the containers $ docker-compose down Stop and/or destroy the containers and their volumes (including named volumes) $ docker-compose down -v Delete everything, including images $ docker-compose down -v --rmi all Conclusion Here is a summary of what we have covered today: what Docker Compose is; what the difference between a service, an image and a container is; how to search for images on Docker Hub; what running a single process per container means; how to split our application into different containers accordingly; how to describe services in a docker-compose.yml file; what a Dockerfile is; how to declare and use volumes; how Docker Compose makes containers discoverable on an internal network; how to assign a domain name to our application; how to set environment variables; a bunch of useful commands. That is an awful lot to digest. Congratulations if you made it this far, that must have been a real effort. The good news is that the next posts will be lighter, and the result of this one can already be used as a decent starting point for any web project. Don't worry if you feel a little bit confused or overwhelmed, that is perfectly normal. Docker is a strong case for practice makes perfect : it is only by using it regularly that its concepts eventually click. In the next part of this series, we will see how to choose and shrink the size of our images. Subscribe to email alerts below so you don't miss it, or follow me on Twitter where I will share my posts as soon as they are published.","tags":"Docker","url":"https://tech.osteel.me/posts/docker-for-local-web-development-part-1-a-basic-lemp-stack"},{"title":"Docker for local web development, appendix A: Docker on Windows","text":"In this series Introduction: why should you care? Part 1: a basic LEMP stack Part 2: put your images on a diet (coming soon) Part 3: a three-tier architecture with frameworks (coming soon) Part 4: smoothing things out with Bash (coming soon) Part 5.1: HTTPS all the things with a self-signed certificate (coming soon) Part 5.2: HTTPS all the things with a trusted Certificate Authority (coming soon) Part 6: expose a local container to the Internet (coming soon) Part 7: using a multi-stage build to introduce a worker (coming soon) Conclusion: where to go from here (coming soon) Appendix A: Docker on Windows ⬅️ you are here Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Why this appendix? Minimum requirements Virtualisation, Hyper-V and VirtualBox Terminal Local folder and disk sharing Ports Other resources and fixes Something missing? Why this appendix? You might have heard some rumours claiming Docker is a bit of a pain to run on Windows. The bad new is these rumours are mostly true. All is not lost however: most hurdles come with little tricks that, once known and applied, make the experience of Docker on Windows almost the same as a Unix-based system. This appendix is about those little tricks, which should allow you to complete the entirety of this tutorial series and run the final result smoothly on Microsoft's OS. Note that this appendix is not about Windows-based containers. Minimum requirements Docker cannot run on any version of Windows. The minimum requirements are listed in the Docker documentation and, at the time of writing, they indicate your system should run at least Windows 10 64-bit Pro, Enterprise, or Education , because of Docker's virtualisation needs. Virtualisation, Hyper-V and VirtualBox Docker requires hardware virtualisation capabilities to function properly, which on Windows are provided for by Hyper-V. If your system satisfies the aforementioned requirements, Hyper-V is probably already enabled; if Docker seems to think otherwise, head over here for instructions about how to activate it. Once Hyper-V is running, you might notice that VirtualBox doesn't work anymore: it is essentially because Hyper-V transforms Windows into a virtual machine on which VirtualBox cannot run. As a result, you can't use Docker and VirtualBox at the same time , and you will need a system restart to switch from one to the other (this also applies to other level 2 hypervisors like VMware). Having said this, you may not have to choose between the two: what you need VirtualBox for may also be compatible with Hyper-V (like Homestead , for instance). If you are interested in the subject, you can read more about virtualisation on Windows , and about the conflicts between Hyper-V and VirtualBox . Terminal The people at Docker seem to be focusing on Docker Desktop's UI at the moment, and I wouldn't be surprised if everything we currently do in a terminal ends up being available on the dashboard. For the time being, however, using a terminal is still (and will most likely always be, in my opinion) the easiest way to use Docker. To be able to follow this tutorial series in optimal conditions, you will need a command line interface featuring Bash, for which I recommend using Git Bash , whose name is rather indicative of what it ships with. You will then be able to run regular Docker commands and execute some Bash scripts when we get to that. Make sure to select \"Use Windows' default console window\" during the installation process ; if you don't, you will have to prepend any Docker command with winpty . This is a long standing issue that makes using the albeit nicer MinTTY a pain, although it seems Windows' console has improved with Windows 10 ( source ). If you also intend to clone the GitHub repository coming with the series, you can run Git commands from the same terminal, provided a SSH key is set up. If you already use a GIT GUI like GitKraken , you can skip this part; otherwise, open Git Bash and run the following command: $ ls -l ~/.ssh If you don't see a couple of files named id_rsa and id_rsa.pub or if the folder doesn't exist, run the following command and hit return at every prompt: $ ssh-keygen -o This will basically generate a public/private RSA key pair so Git Bash can interact with GitHub via SSH. Run the following command to display your public key and copy the result: $ cat ~/.ssh/id_rsa.pub Paste it as a new SSH key in your GitHub settings . Your Git Bash is now set! Local folder and disk sharing You will need to share the local drive containing your Docker projects with Docker, which you can do in the application's settings . Note that based on your Docker Desktop version, you might get prompted for your account's password and an empty string won't be accepted. Meaning if you don't have a password for your account, you will probably have to set one first. Ports The local setup needs port 80 to be available, but it might already be in use (which will most likely prevent the Nginx container from starting properly). A quick way to free up the port is to run the following command as admin (from Command Prompt, not from Git Bash): $ net stop http /y If you still have issues, have a look at this article . Based on the way you intend to access the database, you might also need to free up port 3306 (unless you plan on using phpMyAdmin). Other resources and fixes If you encounter other issues, some of these resources might help: Get started with Docker for Windows ; Logs and troubleshooting ; Extremely slow on Windows 10 ; Unable to bind ports . Something missing? This guide should be enough to get you going with this tutorial series; that being said, I work on a Mac and don't have regular access to a Windows machine, so if anything seemingly Microsoft-related happens I won't be of much assistance. If you come across an issue and find a way to solve it, please let me know in the comments and I will consider completing this guide. Pull requests are also always welcome . All set? You can now proceed with the first part of this series.","tags":"Docker","url":"https://tech.osteel.me/posts/docker-for-local-web-development-appendix-a-docker-on-windows"},{"title":"Docker for local web development, introduction: why should you care?","text":"In this series Introduction: why should you care? ⬅️ you are here Part 1: a basic LEMP stack Part 2: put your images on a diet (coming soon) Part 3: a three-tier architecture with frameworks (coming soon) Part 4: smoothing things out with Bash (coming soon) Part 5.1: HTTPS all the things with a self-signed certificate (coming soon) Part 5.2: HTTPS all the things with a trusted Certificate Authority (coming soon) Part 6: expose a local container to the Internet (coming soon) Part 7: using a multi-stage build to introduce a worker (coming soon) Conclusion: where to go from here (coming soon) Appendix A: Docker on Windows Subscribe to email alerts at the end of this article or follow me on Twitter to be informed of new publications. In this post In this series In this post Who this series is for \"Why should I listen to you?\" \"I am already using Vagrant/Homestead\" \"How about Laradock?\" \"Serverless is the way to go\" \"I'm on Windows\" \"It is not my job to deal with Docker\" Who this series is for The first version of Docker was released in 2013 and, since then, it has worked its way up to eventually becoming the industry standard for containers. Among developers, exposure to Docker ranges from having vaguely heard of the technology to using it on a daily basis, the latter category singing its praises while the former is sometimes still struggling with the sheer concept of containers. Wherever you are on your journey, as a developer there are many reasons why you might want to delve into this technology, including, but not limited to: \"I've read quite a bit about Docker and I am now looking for something more hands-on\"; \"I use a Vagrant-based solution like Homestead and it starts to feel like it's getting in the way\"; \"I already use a pre-configured Docker environment like Laradock and I want a deeper understanding of how things work under the hood\"; \"I want greater control over my development environment\"; \"I want to better understand application architecture\"; \"The project I am working on uses microservices\"; \"I am supposed to migrate a Wordpress website and I need a distraction\". Whatever your motive is, you might notice the emergence of a theme, revolving around understanding and being in control : in my opinion, Docker is about empowering the developer, from providing a safe environment to try out technologies easily, without messing up your local environment (yes, even Wordpress ), to opening the gates to the world of Ops. The aim of this tutorial series is to demystify a technology that can feel daunting at times, all the while providing most of the tools a developer might need to build a web project locally with Docker. It strictly focuses on the development environment and does not cover deploying Docker applications whatsoever , even though the end result can be used as part of a deployment process. No prior knowledge of Docker is required, but being comfortable around a terminal will help. We will start our journey with a very basic LEMP stack and gradually increase the environment's complexity as we work our way through the different parts of the series, covering more and more use cases along the way. While the focus is mostly put on PHP on the server side, the overall architecture can be adapted for any language. You might find yourself stopping at a certain point because all of your needs have been addressed already, only to come back later when you need more. Wherever that is, each part of the series comes with its own branch from this repository , which you can fork or download and use as a starting point for your own projects. That's it for the elevator pitch. Convinced already? Great, let's move on to the first part (or head over here if you intend to run Docker on Windows). If not, let's try to address some of the doubts you might still have, starting with a little background check. \"Why should I listen to you?\" I am a backend developer with more than a decade of experience, having worked with various types of companies in three different countries. I witnessed the evolution of the development environment landscape over time, from the early days of using WAMP and Notepad++ to migrating to full-fledged IDEs and custom virtual machines, before moving on to Vagrant and eventually, Homestead . Back in 2015, I explored the possibility of using Docker locally as a development environment and wrote about it in a couple of articles that were picked up by the official Docker newsletter , but eventually dropped the idea, mainly because of performance issues. A the beginning of 2018, however, I was forced to revisit that thought because of Vagrant's shortcomings vis à vis the growing complexity of the project I was working on at the time (more on that in the next section), and realised progress had been made both in terms of performance and adoption. After tinkering about for a few days and successive iterations, I ended up with a decent development environment running on Docker, capable of meeting the application's new scope and greatly simplifying the onboarding of new developers. Since then, I successfully implemented and improved upon a similar setup in another company. While the approach taken in this series is certainly not the only one and is most likely perfectible, it has proven to be reliable and a significant improvement compared to previous Vagrant-based setups. \"I am already using Vagrant/Homestead\" Homestead is great. I used Laravel 's pre-packaged Vagrant virtual machine (VM) for years, both for personal and clients' projects (if you have no clue what Vagrant is, please take a quick look at this post first, especially this short section ). Homestead features pretty much everything you need for a PHP application, and allows you to spin up new websites very quickly. So why did I make the switch? The short answer is microservices . Back at the beginning of 2018, I was tasked with exposing an API from a legacy monolith to serve a new SPA, and to progressively phase out the monolith by extracting all of its business logic into microservices. All of a sudden, I had to manage some legacy code running on PHP 5.x on the one hand, and some microservices running on PHP 7.x on the other. I initially got both versions of the language running on the same VM but it involved some dirty workarounds that made the overall user experience terrible. Besides, I would eventually end up with multiple microservices with different stacks, and managing them all on the same VM wasn't a realistic long-term solution. I briefly tried to give each microservice its own Vagrant box, but running everything together was far too heavy for my machine and managing things like intra-VM communication felt very cumbersome. I needed something else, and that something else was Docker. But how does it help the situation? One of the promises made by Docker is to provide isolated environments ( containers ) running on a single virtual machine, which starts in about five seconds. In my case, that meant replacing all of my heavy, Vagrant-based virtual machines with a single, super-fast virtual machine featuring Docker, and run all of my microservices on top of it, each in its own isolated container. Using Docker's very own logo in an attempt to illustrate this, it would be the equivalent of having a single whale per container instead of the same whale carrying all of the containers. If you replace \"whale\" with \"virtual machine\" and \"container\" with \"microservice\" in the previous sentence, you should get the idea. Imagine if every single container in the logo needed its own whale: beyond the ridiculous amount of plankton that would require, would it look efficient? While being an overly simplistic explanation, this right there is actually what made it click for me: as a developer, this use case made perfect sense as it related to the work I do every day, way more than trying to understand how virtualisation or shared operating systems work. Does that mean you should only use Docker locally if your application involves microservices? The answer is yes and no. The more complex the application, the more moving parts it is composed of, the more likely you will need a solution like Docker. But even if your application is rather simple, starting and stopping some containers is way faster than booting and halting a virtual machine, and in the eventuality of your application evolving in some unexpected ways (like they always do), adopting Docker from the get go gives you the confidence your setup is future-proof. Besides, instead of using a pre-packaged virtual machine like Homestead, featuring way more tools than any given application has any use for, using Docker the way I suggest ensures that you only ever install what you actually need, in a proactive way. You regain control of your environment. \"How about Laradock?\" Laradock is to Docker what Homestead is to Vagrant: a pre-packaged environment for PHP applications. While I don't personally use it, I have heard a lot of good things and it might just be enough for your needs. Their motto is Use Docker First - Then Learn About It Later , which is an excellent approach in my opinion. That being said, the purpose of this tutorial series is precisely to get a deeper understanding of how to make Docker work for you. If all you are interested in for the moment is using a Docker-based environment without the hassle of setting it up by yourself, by all means give Laradock a try, and feel free to come back whenever you want to create your own, tailored environment. \"Serverless is the way to go\" Serverless computing is on the rise and the word on the street is that it's coming for Docker . Meanwhile, others argue that the comparison is moot because the two technologies serve completely different purposes. I don't know what the future is made of. When it comes to technology, I like to consider myself a pragmatist: being a developer requires an awful lot of effort to stay up to date, and while I would love to explore the many technologies that arise pretty much on a daily basis, I simply don't have the time to do so. While the serverless movement has certainly been getting momentum, it is also still very much in its infancy, and I would like to see it mature further before I consider taking the plunge. If serverless somehow ends up \"eating the stack\" and that is where all the jobs go, fine, I will make the switch. But I don't see such a market shift happening for another few years (if ever), and for the time being I'd rather focus on what is likely to make my CV attractive today . Companies are already slow to migrate to containers despite high interest, and the industry only recently rallied behind Kubernetes, which is poised to become the standard of container orchestration. Serverless represents an even greater paradigm shift that is nowhere near such consolidation: thinking it will flip the market at the snap of a finger is foolish in my opinion. That doesn't mean I won't use a Lambda function where it seems relevant, or that serverless shouldn't be on your watchlist. On that note, if you wish to explore the subject further, I found this article by ZDNet to be informative. \"I'm on Windows\" Ah mate, I feel you. You work on Microsoft's operating system, and you read somewhere that running Docker on it is a bit of a pain? Well, the bad news is, this is still largely true. The good news, however, is that most of what is covered in this series has been successfully tested on Windows 10 at some point, even though it does require a few tweaks here and there. Nevertheless, running Docker on Microsoft's OS is tricky enough that I came up with a special appendix to help you get started. I recommend you read it after this introduction and before proceeding further, even though I regularly refer to it throughout the articles. A caveat: I work on a Mac and don't have regular access to a Windows machine, so if any Microsoft-related trickery seems to be at play I'm afraid I won't be of much assistance (this is also valid for Linux distributions, although from experience these don't tend to cause any trouble). I would also like to make a full disclosure: while it is true that everything covered in this series will also work on Windows, the overall experience is still likely to feel a bit brittle, especially from the moment Bash is involved. There is always a way to mitigate those frictions, but it generally takes a little extra TLC. \"It is not my job to deal with Docker\" Finally, some developers simply dismiss Docker as not being their problem. This is a fair argument, as Docker sits somewhere beyond the realm of pure application development. If anything, a sysadmin or DevOps engineer should set it up for you, right? Well, yes, you are entitled to feel that way. But if DevOps can refer to individuals with an interest for both system administration and coding, effectively acting as bridges between the two, it can also be interpreted as people from both sides meeting halfway. In all fairness, you don't need to know Docker to be a good developer. My point is simply that by ignoring it, you are missing out on an opportunity to get better. It won't improve your syntax nor teach you new design patterns, but it will help you to understand how the code you are writing fits into the bigger picture. It is a bit like playing an instrument without learning to read sheet music: you might sound good, but you are unlikely to write a symphony. Taking a step back to reflect on the application's architecture to understand how the different pieces fit together will give you invaluable perspective that will influence the technical choices you make for your application. Whatever your specialty – backend, frontend or fullstack – and to whatever extent you feel concerned with the DevOps movement, I promise that learning about the cogs that make your applications tick is worth your time, and that you will be a better developer for it. Now. Shall we get started ?","tags":"Docker","url":"https://tech.osteel.me/posts/docker-for-local-web-development-introduction-why-should-you-care"},{"title":"Talking about Collections at PHP Quebec","text":"Who knew Collections were so popular? After Laravel Montreal last month , I will be speaking about Collections again at PHP Quebec on July 4 . This talk will be less Laravel-centric, instead highlighting Tighten Co.'s standalone package , which extracted the Collection class from the web artisans' framework to make it available to any PHP project. So let's grab a beer and have a chat at Gorilla's office next Thursday! (If you can't make it but wish to know more about Collections nevertheless, I have put together a GitHub repository with a demo application containing all of the examples I will be showing, as well as the presentation's slides .)","tags":"PHP","url":"https://tech.osteel.me/posts/talking-about-collections-at-php-quebec"},{"title":"Talking about Collections at Laravel Montreal","text":"I have been a bit busy since I moved to Montreal back in February last year and one of the things I've been doing is regularly attending Laravel Montreal meetups. It is a small but active community that gathers every month or so around various subjects gravitating around the framework, and after sitting in the audience for a few months I in turn decided to stand before it and share a bit of my experience. I'll be talking about Laravel Collections tomorrow night (Weds May 29) at ctrlweb 's office from 6:30pm. If you live in Montreal don't hesitate to join, either tomorrow or at a future meetup - presentations are in French but you can ask questions in English. I have also put together a GitHub repository with a demo application containing all of the examples I'll be showing, as well as the presentation's slides (also in French, but there are a lot of code samples and with the demo app it should be fairly easy to follow). Happy coding!","tags":"Laravel","url":"https://tech.osteel.me/posts/talking-about-collections-at-laravel-montreal"},{"title":"Having issues with your cordless Logitech Unifying device on MacOS? Don't throw it away just yet","text":"TL;DR The cordless Logitech Unifying devices can be a bit funny when it comes to MacOS Sierra, be it the scrolling acting weird or the device not being detected altogether. In my case it was the latter, and it took me a ridiculous amount of time to find a solution, mostly because of borked software. This post is mainly for my own future reference, but if you are running into the same kind of troubles, hopefully following the quick steps below will help. Full story Having worked with a Magic Trackpad (version 1) for a while, then with a mouse again for one of my contracts, I realised I actually felt more comfortable using the latter. I decided I would get one for my personal use as well, and first peeked at Apple's horrendously (and unsurprisingly) expensive Magic Mouse , before I remembered that once upon a time I had a wireless mouse, when I was still a Windows user. I found it among old RJ45 cables and dusty keyboards: a Logitech Anywhere Mouse MX , using the Unifying technology - basically a tiny dongle to plug to one of the machine's USB ports, and supporting devices of the same product range. I inserted some batteries, plugged the dongle and switched the mouse on: nothing. I made a quick search online and everything seemed to point to the same piece of software, the Logitech Control Center . I downloaded it, installed it and rebooted my machine: the Logitech Control Center icon did appear in System Preferences , but my mouse did not show up and no matter what I did, clicking the Open Unifying Software button made the application crash (which was a bummer since my mouse obviously needed to be paired with the dongle again). I couldn't find a solution at the time and had better things to do with my life, so I gave up and moved on, and only decided to give it another shot a few weeks later, before I sold it on Gumtree and gave Apple more of my money to acquire their fancy Magic Mouse. I found an indirectly related GitHub thread where a dude mentioned another piece of software, Logitech Options . I first uninstalled the Logitech Control Center (search for \"LCC Uninstaller\", follow instructions, reboot to complete the removal of the driver), and went on installing Logitech Options. I'm not entirely sure what its purpose is but for some reason the version of the Unifying Software it comes with does work and allowed me to pair my mouse again. Hurray. Now there are multiple reports of weird scrolling behaviours concerning Logitech mouses on MacOS Sierra - if that's your case, it seems that installing the Logitech Control Center might help after all. I installed it again and I can configure my mouse properly - the application seems to happily live alongside the Logitech Options one in the System Preferences. I had enough emotions for one day though, and didn't dare to open the Unifying Software from the Control Center (will you?). Quick steps Uninstall the Logitech Control Center if already present on your system (search for \"LCC Uninstaller\", follow instructions, reboot to complete the removal of the driver) Install Logitech Options Open it and pair your device(s) (Re-)Install the Logitech Control Center if you wish to customise further your device(s)","tags":"Misc","url":"https://tech.osteel.me/posts/having-issues-with-your-cordless-logitech-unifying-device-on-macos-dont-throw-it-away-just-yet"},{"title":"UK Contractors: should you switch off the Flat Rate Scheme?","text":"TL;DR Probably. With the new fiscal year just a few days ahead, as a contractor you've probably heard about the Budget changes. And if you are like me up until a few days ago, you might not be sure whether you should take action or not. Let's try and bring some clarification to these changes. Note: if you are new to the contracting world and/or some of the terms below are unknown to you, you might want to read my UK tax breakdown as well. Table of contents A brief summary of the new Budget The Flat Rate Scheme's change The current state of things The new Limited Cost Trader category The Standard VAT Scheme Should you make the switch? Conclusion Sources A brief summary of the new Budget There are a few things that are effectively going to change between April 1st and April 6th 2017, some of which affect contractors, and which can be summarised in three bullet points: The income tax allowance is raised from £11,000 to £11,500 (April 6th) The Corporation Tax is reduced from 20% to 19% (April 1st) A new 16.5% VAT flat rate category is introduced ( Limited Cost Trader - April 1st) You might also have heard about the dividend tax allowance being reduced from £5,000 to £2,000, around which there seems to be some confusion. No need to worry about it just yet - it will actually come into effect in the 2018/19 tax year . There was also quite some fuss around an announced National Insurance Contributions increase, but the Government eventually dropped it . The Flat Rate Scheme's change The one big change that should get your attention here is the one related to VAT's Flat Rate Scheme (FRS). I will assume you already know what the scheme is, and if you don't, I invite you to read the section dedicated to VAT from my previous article about taxes. The current state of things The way the FRS currently works is, on an invoice of £2,000, you charge 20% of VAT, corresponding to an amount of £400, so a total of £2,400. Then, you must apply the flat rate corresponding to your trade sector to the total, the result being the VAT you pay back to HMRC. As an IT Consultant for example, the rate applying to my business is 14.5%, so I'd need to pay £2,400 * 0.145 = £348 back to HMRC. This is quite interesting, since the difference of VAT, called the VAT Credit , becomes part of my company's profit. Or to put it differently, I charge £400 VAT and pay back £348, keeping the remaining £52. The new Limited Cost Trader category In order to \"tackle aggressive abuse of the VAT Flat Rate Scheme\" (I won't go into details here, follow the link for more info), the Government is introducing a new category called Limited Cost Trader , with a fixed 16.5% rate for whichever business falls under it, regardless of the trade sector. To be affected by this new rate, your business expenditure must represent less than 2% of its VAT inclusive turnover, or less than £1,000 per annum if it is greater than 2%. Now if you are like me, looking at your business expenses you might think you are way above the threshold and thus not concerned by this at all. That was before I realised only certain kinds of expenses qualify. Basically, everything including food, transport, phone, computer ( capital expenditures ), etc doesn't qualify and, this list accounting for 95% of my expenses, it turns out my business automatically falls under the Limited Cost Trader category. Bummer. But how bad is it? If we look at the same example again, I now need to pay £2,400 * 0.165 = £396 back to HMRC. That's a VAT Credit of £4. The Standard VAT Scheme If your business falls under the new category, the VAT Credit it benefits from becomes very low. So should you switch to the Standard Scheme instead? Let's review how it works first. You still charge 20% VAT to your client, which you must entirely pay back to HMRC. However, you can claim the VAT on any VAT inclusive expense related to your business, when you could only do so for expenses greater than £2,000 with the FRS. Food and transport usually don't qualify, but capital expenditures do (computer, phone), and so do accountancy fees, for example. Let's take an example and consider your accountant costs your business £90 per month. They charge the VAT as well, which accounts for £90 - (£90 * 100 / 120) = £15. That means that you could claim £15 of VAT per month. Note that with the Standard Scheme, you need to keep track of the VAT you charge and the VAT you claim, which is a bit more work. Should you make the switch? To make that decision, you need to assess whether you'd be better off being able to claim VAT on VAT inclusive business expenditures (Standard Scheme) or applying the 16.5% rate and keeping the VAT Credit (Flat Rate Scheme's Limited Cost Trader category). You should be able to do so looking at how much you invoice in an average month as well as the nature of your usual expenses, and use the calculations above to determine what works best. If you are planning on buying hardware this year, it is very likely that the Standard Scheme would be better for you. Personally, with accountancy fees alone, I'm better off de-registering from the FRS. And since these fees are directly managed by my accountant, I don't even need to keep track of the claimed VAT, as they automatically do it for me. No extra work. Conclusion I hope this will help you assess whether you are affected by these changes at all, and if you need to do something about it. Bear in mind that I am not an accountant and that you should talk to yours before taking any action. If you do so and decide to switch off the FRS, you should also remember that you won't be able to register for it again for another twelve months . Sources Tax and tax credit rates and thresholds for 2017-18 Corporation Tax: main rate Income Tax: dividend allowance reduction 'This is grossly unfair': self-employed readers react to NICs increase Budget U-Turn As Chancellor Scraps National Insurance Hike On Self-Employed Tackling aggressive abuse of the VAT Flat Rate Scheme - technical note What are capital expenditures? VAT Flat Rate Scheme - Join or leave the scheme","tags":"Contracting","url":"https://tech.osteel.me/posts/uk-contractors-should-you-switch-off-the-flat-rate-scheme"},{"title":"Contracting in the UK: a tax breakdown","text":"Edit 26/03/17: The 2017/18 tax year is almost here! Read about the changes here . When I started contracting about a year and a half ago, I absolutely didn't want to have to deal with anything accounting-related. Which is fine really, as getting an accountant is highly recommended anyway - for various good reasons - and there is a plethora of companies on the market that do just that. I picked one that had been recommended to me by a fellow contractor and they did everything for me, from opening my limited company and business bank account to dealing with VAT registration, among other boring things. All I had to do was to sign papers and pretend to understand what they were telling me over the phone. Easy. Earlier this year though, as I had been in the contracting business for a little more than a year, I started to feel like I should try and understand a bit more what was what. Needless to say, it didn't turn out to be an easy task. I started googling around and realised the first few result pages were hogged by accounting companies that were all more or less providing the same content - enough to get a global grasp, but nothing breaking down things in detail. I kept digging nevertheless and, with my findings and some answers provided by my accounting team, I eventually got to a point which I think is a basic but decent understanding of what's going on, now all compiled below in a post I wrote while enjoying a well-deserved end-of-year holiday. What could be better than an article about taxes to put you in the Christmas mood anyway? Summary Disclaimer Tax Typology 1. Corporation Tax 2. PAYE and National Insurance contributions 3. Tax on dividends 4. VAT Example Yearly projection Conclusion Sources Disclaimer What follows is closely related to my own contracting experience as an IT consultant, to the way I operate on a daily basis as the director of my limited company. While this post certainly isn't exhaustive, I think it covers most of the questions that will arise for the average contractor at some point, and should allow you to make a projection for your own company based on your personal situation. The aim here is not for you to deal with your own company accounts, but more to understand the stuff your accountant is sending you, beginning with your personal illustration when you're just getting started. I should also point out that what is described below only applies if you can prove that you operate outside of IR35, which is a whole other subject that I won't get into here. You can read more about it over there if you wish to. Unless stated otherwise, all the info provided is valid for the 2016-17 financial year. Tax typology Taxes come in different flavours and the four main ones are: Corporation Tax, PAYE and National Insurance contributions, tax on dividends, and VAT. In the United Kingdom, the department responsible for collecting taxes is HMRC (Her Majesty's Revenue & Customs). 1. Corporation Tax Corporation Tax applies to the company's profits and is paid annually by the company, the deadline usually being nine months and a day after the end of your accounting period (which normally corresponds to the fiscal year - from the 6th of April to the 5th of April the following year). The tax is 20%. 2. PAYE and National Insurance contributions PAYE Income tax or PAYE (Pay As You Earn) is based on the employee's income (meaning the director) (meaning you) and is retained and paid for by the employer monthly or quarterly. There is a yearly Personal Allowance of £11,000 on standard income, which essentially means that the first £11,000 of income are tax free. Here are the tax bands for the 2016-17 fiscal year: Allowance (0%) Up to £11,000 Basic rate (20%) From £11,001 to £43,000 Higher rate (40%) From £43,001 to £150,000 Additional rate (45%) Over £150,000 Expenses I should probably drop a quick note about expenses at this point. These are the purchases you as the employee make that are related to your professional activity (transport, meals, hardware, etc). You declare these purchases as business expenses so your company can pay you back as part of your income, but that amount is tax free. To put it differently, say you file £65 of expenses in a certain week, you will get this amount back as part of your income but it will not be deducted from your Personal Allowance. National Insurance contributions You pay National Insurance contributions so you can benefit from advantages like the State Pension or the Jobseeker's Allowance (the full list of benefits is available here ). There are different classes and category letters corresponding to different rates, but as a company director yours will most likely be class 1 and category letter A (unless specific circumstances as listed in the two previous links). National Insurance contributions are paid by both the employee and the employer, based on rates and only above a certain yearly threshold, and the employer pays the total amount to HMRC along with the income tax. Employee: 0% From £112 (minimum wage) to £155 per week 12% From £155.01 to £827 per week 2% Over £827 per week The employee's threshold is £8,060, meaning the employee will start to see NI contributions deducted from their income once it meets £8,060 for the current fiscal year. Employer: 0% From £112 (minimum wage) to £156 per week 13.8% From £156.01 to £827 per week 13.8% Over £827 per week The employer's threshold is £8,112, meaning the employer will start to pay NI contributions once the employee's income meets £8,112 for the current fiscal year. 3. Tax on dividends Being a company director allows you to pay yourself with dividends (since you own 100% of the company's shares) as well as the standard income. Just like the latter, there is a Personal Allowance under which dividends are not taxed. Its amount is £5,000, which means that the first £5,000 of dividends are tax free. Allowance (0%) Up to £5,000 Basic rate (7.5%) From £5,001 to £32,000 Higher rate (32.5%) From £32,001 to £150,000 Additional rate (38.1%) Over £150,000 Tax on dividends is paid annually while you complete your tax return. We can already clearly see that standard income tax rates are significantly higher than the dividend ones - that's why contractors usually pay themselves the standard income's Personal Allowance only (so it remains tax free), and the rest with dividends. 4. VAT The VAT (Value-Added Tax) is basically a tax that is paid every time goods or services are sold. As a business, you must register for VAT only if your annual VAT-taxable turnover is over a certain threshold (£83,000 for 2016-17, usually updated every year), but you can also voluntarily do so even if your turnover is less than that amount ( here are some reasons why you would do such a thing ). When you pay for it depends on the chosen VAT scheme, which are quite numerous and vary according to your activity. The standard way of dealing with VAT is to keep track of the VAT you invoice (and to pay the corresponding amount to HMRC whether your client paid the invoice or not) as well as the VAT of your business expenditure (which you can likewise claim even if you haven't paid the invoice yet), and to report these amounts to HMRC quarterly. Depending on the side of your business and its turnover, however, different options are available to you. Going into details for each of them is much like sticking your head down the rabbit hole, so I will only briefly describe some of them and linger for a bit longer on the Flat Rate Scheme, since that's the one I personally registered for (and seems to often be picked by contractors - or is advised by accountants anyway, since it also makes their life easier as we will see). Annual Accounting Scheme This scheme allows you to submit your VAT return once a year instead of quarterly, for a fixed amount based on the previous return (or an estimate for the first year you register for that scheme). You then pay for or get back the difference at the end of the year. Cash Accounting Scheme The difference between this scheme and the standard one is that you pay and claim VAT for invoices that were actually paid rather than from the moment you send or receive them. Margin schemes The VAT corresponds to 16.67% of the difference between the price you paid for an object (antiques, works of art...) and the price you sold it for. Flat Rate Scheme At first glance the Flat Rate Scheme is quite appealing, since it greatly simplifies things and in most cases will actually save you some dough (via the VAT credit you get by charging more VAT than your company actually pays for - see the example below ). The way it works is that you charge 20% VAT to your clients as usual, but you only pay a fixed (lower) percentage of VAT to HMRC in return. That percentage depends on your activity (see the table on this page for comparison) and is reduced by 1% the first year - e.g. as an IT consultant my rate is 14.5% (and was 13.5% the first year). That means that I charge a 20% VAT to my clients and pay 14.5% back to HMRC - the difference is the VAT credit, added to my company profit. This is quite simple indeed since there is no need to keep track of the VAT for each business expenditure. The Flat Rate Scheme doesn't allow you to claim VAT on such expenses, apart for purchases above £2,000 . While it might seem like a no-brainer, there is slightly more to this: the scheme certainly simplifies the process around VAT, but this simplification also benefits to your accountant and HMRC. You will most certainly get VAT credit, which is a good thing, but whether it is more beneficial to you than say the Cash Accounting Scheme should be estimated first, based on your expected business expenses. At the end of the day which option is best for you really depends on your personal situation, but in any case you should ask your accountant for comparative simulations between the different schemes to make sure you pick what works best for your business, not only what works best for them. There is obviously much more to VAT than the above (specific rules for builders , charities , retailers ... or even depending on whether you trade internationally), and there's quite some literature about it on HMRC's website if you need more information. Example Alright, enough with the theory, it all truly starts to make sense once applied concretely. Let's have a look at an example, in which we'll calculate the amount of take-home based on a weekly invoice of £1,500 plus VAT (+20%), so five days at £300 plus VAT per day. In order to be tax-efficient, say we only want to pay ourselves the Personal Allowance of £11,000 mentioned earlier, which over a year (52 weeks) is £11,000 / 52 = £210 per week (approximately). This will allow us not to pay any tax on income, meaning we will pay ourselves with dividends instead, which are taxed at a lower rate as we already saw. Note: I am using a weekly average for the sake of simplicity, but then again this is just an example, YMMV. VAT credit Since our daily rate is £300 plus VAT and we charge 20% VAT to the client, we send a weekly invoice of: £1,500 + VAT = £1,500 + (£1,500 * 0.2) = £1,500 + £300 = £1,800 Say we registered for the Flat Rate Scheme which, as an IT consultant, means we have to pay a 14.5% rate. 14.5% of £1,800 is £261, which is the amount we owe to the taxman. Since the amount of VAT charged to the client is £300, in effect that's a VAT credit of £300 - £261 = £39. Your company essentially keeps the difference between the VAT it charges and the VAT it pays (the VAT credit). In our example, that means our company profit is now £1,500 + £39 = £1,539. PAYE, NI contributions and expenses We already saw that since we'll pay ourselves the Personal Allowance only, we won't pay any PAYE tax on our income. But as this allowance (£11,000) is more than the NI contributions thresholds (£8,060 and £8,112 for the employee and the employer respectively), we will have to pay some NI contributions eventually. Assuming you would pay yourself the same weekly income throughout the year (52 weeks), the NI contributions could be calculated like so, as a weekly average: The NI contributions rate is 0% up to £155, and 12% of the remaining £55, so £55 * 0.12 = £6.6 paid by the employee per week. NI contributions are 0% up to £156, and 13.8% of the remaining £54, so £54 * 0.138 = £7.45 paid by the employer per week. In practice however, NI deductions would only appear on your payslips once the employee's threshold is met, and the company's liability in this regard would only start to be accounted for once the employer's threshold is met. For the sake of simplicity however, and to show you how they are accounted for, let's consider that we take these NI contributions into account on a weekly basis. Our company pays us (as the employee) £210, plus £7.45 of NI contributions to HMRC, so £217.45 in total. The company profit is now £1,539 - £217.45 = £1,321.55. As we saw earlier, however, as a contractor chances are you will have expenses to file, such as transport costs, meals, books, etc. These expenses are paid to you by your company as tax-free income, so if you spend say £50 on that week, the company will pay you back £50. Hence the company profit becomes £1,321.55 - £50 = 1,271.55. You as the employee are paid £210 - £6.6 of NI contributions = £203.4. We don't count the £50 of expenses here, since you paid for it in the first place - your company is just paying you back. It's cancelled out. (Don't worry if you start to feel a bit schizophrenic at this point, that's perfectly normal.) Corporation Tax It is on this remaining company profit that the Corporation Tax is applied (20%): £1,271.55 * 0.2 = £254.31 £254.28 is thus what you should keep in your company's bank account for the taxman. That also means that the remaining £1,017.24 (£1,271.55 - £254.31) can be paid to you as dividends. The total take-home is thus £1,017.24 + £203.4 = £1,220.64. Wrapping it up Let's go over our initial assumptions again and sum it all up: our daily rate is £300 + VAT we work five days a week we pay ourselves £210 per week throughout the year (that's 52 weeks) we registered for the Flat Rate Scheme With that in mind, the weekly take-home would be £1,220.64 out of the £1,500 we invoice or, to put it differently, a whopping 81% of the initial amount. But then again that's only an average amount, as in practice you wouldn't pay for the NI contributions until the threshold is reached and it is very unlikely you would work five days a week for 52 weeks (it's actually impossible, if only because of bank holidays). You could probably still pay yourself £210 a week (even when you don't work), but not as much in dividends. Let's consider it could be the case anyway, just for the sake of the example: does that mean you could earn 52 * £1,220.64 = £63,473.28 a year? Well, not quite. We are not paying any tax on income, that's true, but let's not forget that dividends are subject to their own tax too. That's why it now makes sense to think on a yearly scale, now that you understand how things work on a weekly one. Yearly projection We know that the Personal Allowance for the current fiscal year (2016-17) is £11,000, which is tax free. As for dividends, the allowance is £5,000, then the basic rate is 7.5% up to £32,000, and then 32.5% above that (and 38.1% for the higher tax band). If we decide to stay beneath the first band for tax-efficiency purposes, that means that £32,000 - £5,000 = £27,000 is taxed at a 7.5% rate, so we would pay £27,000 * 0.075 = £2,025 of tax on dividends. Meaning that over the year, we could get £32,000 - £2,025 = £29,975 as dividends after tax. £11,000 is above the NI contributions thresholds of both the employee and the employer, but we're mainly interested in the take-home here so we can ignore the latter. As we saw earlier the employee's threshold for NI contributions is £8,060 for the current financial year, and anything above that is taxed at a 12% rate up to £43,004 (£827 by 52 weeks). This means that £11,000 - £8,060 = £2,940 is taxed at a 12% rate, so we would pay £2,940 * 0.12 = £352.8 of NI contributions over the year. Combining all of the above, we could get an income of £11,000 - £352.8 = £10,647.2, plus £29,975 of dividends, so a total of £40,622.2 after tax over the year. With the same assumptions we used in the previous example, we would need to work £40,622.2 / £1,220.64 = 33.28, basically a bit more than 33 weeks over the year to reach that amount. Conclusion I hope this will shed some light on an obscure income statement or help your decision if you are considering taking the plunge and becoming a contractor. I am not by any measure an accountant so there might be inaccuracies here and there, or I might have left out elements I'm not aware of (if that's the case please let me know about it in the comments). Then again, the aim here is not for you to do your own accounting, but rather to give you the keys so you can make a projection for your own company based on your personal situation. Finally, if you are contemplating becoming a contractor in the near future and are not sure which accountant to go for, I can hook you up with mine - just drop me a line . Sources Corporation Tax Corporation Tax Corporation Tax rates and reliefs PAYE Income Tax Income Tax rates and Personal Allowances NI contributions National Insurance National Insurance for company directors National Insurance rates and categories Tax on dividends Tax on dividends VAT VAT VAT registration The Advantages Of Voluntary VAT Registration For Small Business VAT Schemes for Small Businesses VAT Flat Rate Scheme IT contractor guide to Flat Rate VAT Other related sources Preparing for the inevitable: What taxes will I have to pay? Rates and thresholds for employers 2016 to 2017","tags":"Contracting","url":"https://tech.osteel.me/posts/contracting-in-the-uk-a-tax-breakdown"},{"title":"How to enable NFS on Laravel Homestead","text":"I currently work on a Laravel project composed of multiple microservices that I run locally using Homestead (box v0.4.0 at the time of writing). As I started tinkering around I noticed that requesting the different APIs was super slow - up to 20s per request, which was really unexpected (and annoying to say the least). Googling around it appeared that most fingers were pointing to the same suspect: VirtualBox's shared folders system. Most people were also advising to use NFS instead, and as a matter of fact there is a whole section of Vagrant's documentation on the subject . I tried different things found on various forums over the Internet (such as this topic on Laracast ) and, once I got it set up, the time per request dropped to around 1s. This, you will agree, was quite an improvement. If it originally required a few tweaks in Homestead's configuration, it turns out that with the recent versions of the box, almost all the work is already done. The following is of course assuming that you are already using Homestead locally. Note for Windows users: You may have noticed that the Vagrant documentation states that NFS is not available on Windows. If you are a PC user, you might want to have a look at Vagrant WinNFSd whose promise is to add support for NFS on Windows (I didn't try it myself). Note for Ubuntu users: You might need to install the NFS server: apt-get install nfs-kernel-server In ~/.homestead/Homestead.yaml , under the folders section, add a type option under the folders you wish to map using NFS, as such: folders: - map: ~/Work/www/homestead to: /home/vagrant/projects type: \"nfs\" Now, stop Homestead if it was running ( vagrant halt from ~/Homestead or however you usually do it) and start it again, forcing the provisioning step, e.g.: vagrant up —-provision You will be asked to enter your Mac OS user password before Vagrant mounts the folder(s). That's it! You may wonder why isn't NFS activated by default, then: the reason is because it won't work out of the box on all operating systems, as seen in the notes above.","tags":"Laravel","url":"https://tech.osteel.me/posts/how-to-enable-nfs-on-laravel-homestead"},{"title":"From Vagrant to Docker: How to use Docker for local web development","text":"[UPDATE 2020/03/05]: This is old content. I published an up to date, more complete tutorial series about using Docker for local web development, which I invite you to read instead. If you are somewhat following what's happening in the tech world, you must have heard of Docker. If you haven't, first, get out of your cave, and then, here is a short description of the thing, borrowed from Wikipedia : Docker is an open-source project that automates the deployment of applications inside software containers, by providing an additional layer of abstraction and automation of operating-system-level virtualization on Linux. \"Wow, cool. What the hell does that mean tho?\" I hear you say. Hang on! It goes on, saying that: [it allows] independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines. Hmm ok. It starts to sound familiar and vaguely useful. I don't know about you, but working on a Mac (and previously on a Windows machine) (shush), I set up a Vagrant box for almost every single project I work on (if you have no clue what Vagrant is, please take a look at this post first, especially this short bit ). Whenever I need to work on any of these projects, I run a quick vagrant up and get it running in its own isolated virtual machine (VM) in a matter of minutes. Pretty handy. But that's still a few minutes to get up and running, and having a VM for each project quickly ends up taking a shitload of resources and space on the disk. You could take Laravel's initial approach with Homestead and run several projects on the same VM, but it kinda defeats the purpose of having isolated environments. So what does Docker have to do with this? Well, the promise behind it is to provide isolated environments running on a same virtual machine (with Boot2Docker ) that starts in about five seconds. \"So... I could replace all my Vagrant boxes with a single super-fast VM running Docker?\" Exactamundo. And that's what we are going to do today, step by step. Table of contents Installation Setup Nginx PHP Data Volumes and data containers MySQL phpMyAdmin Handling multiple projects Troubleshooting Conclusion Sources Installation As mentioned earlier, I'm working on a Mac, so this tutorial will be written from a Mac user point of view. That being said, in practice, once the installation is done, the way Docker is used shouldn't differ much (if at all). So, whether you're on a Mac or a PC, just head over here , download and install Docker Toolbox and follow the instructions for your platform ( Mac OS , Windows ) (or, if you're a Linux user, go straight to the specific installation guide - no need to get Docker Toolbox). As stated on the website, Docker Toolbox will install all the stuff you need to get started. Follow the steps of the Get Started with Docker guide all the way down to the Tag, push, and pull your image section: it's all very clear and well written and will even help you get your bearings with the terminal if you're not familiar with it. I'll take it from there. In the meantime, here is a dancing Tyrion: Note: You might get this kind of error when trying to complete the first step: Network timed out while trying to connect to https://index.docker.io/v1/repositories/library/hello-world/images. You may want to check your internet connection or if you are behind a proxy. In that case, simply run docker-machine restart default and try again. All good? Well done. By now, you should have a virtual machine running with VirtualBox (the virtualisation software used by default under the hood) and know how to find, run, build, push and pull images, and have a better idea of what Docker is and how it works. That's pretty cool already, but not exactly concrete. How do you get to the point where you open your browser and display the website you are currently building and interact with a database and everything? Stick with me. Setup Everything we'll do under this section is available as a GitHub repository you can refer to if you get stuck at any time (you can also directly use it as is if you want). Also, as you may have noticed I already started to use the terms virtual machine and VM to refer to the same thing. I will sometimes mention a Docker machine as well. Don't get confused: they are all the same. Now that this is clear, let's decide on the technologies. I usually work with the LEMP stack , and I would like to get my hands on PHP7, so let's go for a Linux/PHP7/Nginx/MySQL stack (we'll see how to throw a framework into the mix in another post). As we want to have the different parts of the stack to run in separate containers, we need a way to orchestrate them: that's when Docker Compose comes in. A lot of tutorials will teach you how to set up a single container first, or a couple of and how to link them together using Docker commands, but in real life it is very unlikely that only one or two containers are going to be needed, and using simple Docker commands to link more containers to each other can quickly become a pain in the bottom. Docker Compose allows you to describe your stack specifying the different containers that will compose it through a YAML config file. As the recommended practice is to have one process per container, we will separate things as follows: a container for Nginx a container for PHP-FPM a container for MySQL a container for phpMyAdmin a container to make MySQL data persistent a container for the application code Another common thing among the tutorials and articles I came through is that their authors often use their own images, which I find somewhat confusing for the newcomers, especially as they rarely explain why they do so. Here, we'll use the official images and extra Dockerfiles to extend them. Nginx But first, let's start with an extremely basic configuration to make sure everything is working properly and that we're all on the same page (this will also allow you to familiarise with a few commands in the process). Create a folder for your project (I named mine docker-tutorial ) and add a docker-compose.yml file into it, with this content: nginx: image: nginx:latest ports: - 80:80 Save it and, from that same folder in your terminal, run: $ docker-compose up -d It might take a little while as the Nginx image needs to be pulled first. When it is done, run: $ docker-machine ip default Copy the IP address that displays and paste it in the address bar of your favourite browser. You should see Nginx's welcome page: Nice! So what did we do here? First we told Docker Compose that we wanted a container named nginx to use the latest official Nginx image and publish its port 80 (the standard port used by HTTP) on the port 80 of our host machine (that's my Mac in my case). Then we asked Docker Compose to build and start the containers described in docker-compose.yml (just one so far) with docker-compose up . Option -d allows to have the containers running in the background and gives the terminal back. Finally, we displayed the private IP address of the virtual machine created by Docker and named default (you can check this running docker-machine ls , which will give you the list of running machines). As we published the port 80 of this virtual machine, we can access it from our host machine. One last thing to observe before we move on: in your terminal, type docker ps . You should see something like that: That's the list of all the running containers and the images they use. For now we only have one container, using the official Nginx image. Here its name is dockertutorial_nginx_1 : Docker Compose came up with it using the name of the current directory and the image's and appended a digit to prevent name collisions. PHP Still following? Good. Now let's try and add PHP and a custom index.php file to be displayed when accessing the machine's private IP. Replace the content of docker-compose.yml with this one: nginx: build: ./nginx/ ports: - 80:80 links: - php volumes: - ./www/html:/var/www/html php: image: php:7.0-fpm expose: - 9000 volumes: - ./www/html:/var/www/html A few things here: we added a new container named php , which will use the official PHP image, and more specifically the 7.0-fpm tag. As this image doesn't expose the port 9000 by default, we specify it ourselves. At this point you might be wondering what is the difference between expose and ports : the former allows to expose some ports to the other containers only , and the latter makes them accessible to the host machine . We also added a volumes key. What we're saying here is that the directory ./www/html must be mounted inside the container as its /var/www/html directory. To simplify, it means that the content of ./www/html on our host machine will be in sync with the container's /var/www/html directory. It also means that this content will be persistent even if we destroy the container. More on that later. Note: If you have trouble mounting a local folder inside a container, please have a look at the corresponding documentation . The nginx container's config has been slightly modified as well: it got the same volumes key as the php one (as the Nginx container needs an access to the content to be able to serve it), and a new links key appeared. We are telling Docker Compose that the nginx container needs a link to the php one (don't worry if you are confused, this will make sense soon). Finally, we replaced the image key for a build one, pointing to a nginx/ directory inside the current folder. Here, we tell Docker Compose not to use an existing image but to use the Dockerfile from nginx/ to build a new image. If you followed the get started guide, you should already have an idea of what a Dockerfile is. Basically, it is a file allowing to describe what must be installed on the image, what commands should be run on it, etc. Here is what ours looks like (create a Dockerfile file with this content under a new nginx/ directory): FROM nginx:latest COPY ./default.conf /etc/nginx/conf.d/default.conf Not much to see, eh! We start from the official Nginx image we have already used earlier and we replace the default configuration it contains with our own (might be worth noting that by default the official Nginx image will only take files named following the pattern *.conf and under conf.d/ into account - a mere detail but it drove me crazy for almost three hours at the time). Let's add this default.conf file into nginx/ : server { listen 80 default_server; root /var/www/html; index index.html index.php; charset utf-8; location / { try_files $uri $uri/ /index.php?$query_string; } location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } access_log off; error_log /var/log/nginx/error.log error; sendfile off; client_max_body_size 100m; location ~ \\.php$ { fastcgi_split_path_info &#94;(.+\\.php)(/.+)$; fastcgi_pass php:9000; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off; fastcgi_buffer_size 16k; fastcgi_buffers 4 16k; } location ~ /\\.ht { deny all; } } It is a very basic Nginx server config. What is interesting to note here however is this line: fastcgi_pass php:9000; We are asking Nginx to proxy the requests to the port 9000 of our php container: that's what the links key from the config for the nginx container in the docker-compose.yml file was for! We just need one more file - index.php , inside www/html under the current directory: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <title>Hello World!</title> </head> <body> <img src=\"https://tech.osteel.me/images/2015/12/18/docker-tutorial2.gif\" alt=\"Hello World!\" /> </body> </html> Yes, it only contains HTML but we just want to make sure that PHP files are correctly served. Here is what your tree should look like by now: Go back to your terminal and run docker-compose up -d again. Docker Compose will detect the configuration changes and build and start the containers again (it will also pull the PHP image): Browse the virtual machine's private IP again ( docker-machine ip default if you closed the tab): you should be greeted by a famous Doctor. Now type docker ps in your terminal to display the list of running containers again: We can see a new one appeared, using the official PHP image, and the Nginx one looks a bit different from the previous one: Docker Compose used the Dockerfile to automatically build a new image from the official Nginx one, and used it for the container. Now if you remember, I said earlier that the current directory is in sync with the containers' (because of the volumes key in docker-compose.yml ). Let's check if I'm not a liar: open index.php and change the page title for \"Hello Universe!\" , for example. Save and reload the page. See the change? Sweet. Now we have got two containers for Nginx and PHP, talking to each other and serving files we can update from our host machine and see the result instantly. Time to add some database madness! Data Volumes and data containers Before we actually dive into the configuration of MySQL, let's have a closer look at this volumes thing. Both the nginx and php containers have the same directory mounted inside, and it's common practice to use what is called a data container to hold this kind of data. In other words, it's a way to factorise the access to this data by other containers. Change the content of docker-compose.yml for the following: nginx: build: ./nginx/ ports: - 80:80 links: - php volumes_from: - app php: image: php:7.0-fpm expose: - 9000 volumes_from: - app app: image: php:7.0-fpm volumes: - ./www/html:/var/www/html command: \"true\" Several things happened: first, we added a new container named app , using the same volumes parameter as the nginx and php ones. The purpose of this container is solely to hold the application code: when Docker Compose will create it, it is going to be stopped at once as it doesn't do anything apart from executing the command \"true\" . This is not a problem as for the volume to be accessible, the container needs to exist but doesn't need to be running, also preventing the pointless use of extra resources. Besides, you'll notice that we're using the same PHP image as the php container's: this is a good practice as this image already exists and reusing it doesn't take any extra space (as opposed to using a data-only image such as busybox , as you may see in other tutorials out there). The other change we made is volumes was replaced with volumes_from in nginx and php 's configurations and both are pointing to this new app container. This is quite self-explanatory, but basically we are telling Docker Compose to mount the volumes from app in both these containers. Run docker-compose up -d again and make sure you can still access the virtual machine's private IP properly. Running docker ps now should display this: \"Wait a minute. Where's the app container?\" I'm glad you asked. If you recall I've just said that the container was stopped right after its creation, and docker ps only displays the running containers. Now run docker ps -a : There it is! If you're interested in reading more about data containers and volumes (and I encourage you to do so), I'd suggest this article by Adrian Mouat which gives a good overview (you will also find all the sources I used at the end of this article). MySQL Alright! Enough digression, back to MySQL. Open docker-compose.yml again and add this at the end: mysql: image: mysql:latest volumes_from: - data environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: project MYSQL_USER: project MYSQL_PASSWORD: project data: image: mysql:latest volumes: - /var/lib/mysql command: \"true\" And update the config for the php container to add a link to the mysql one and use a Dockerfile to build the image: php: build: ./php/ expose: - 9000 links: - mysql volumes_from: - app You already know what the purpose of the links parameter is, so let's have a look at the new Dockerfile: FROM php:7.0-fpm RUN docker-php-ext-install pdo_mysql Again, not much in there: we simply install the pdo_mysql extension so we can connect to the database (see How to install more PHP extensions from the image's doc ). Put this file in a new php/ directory. Moving on to the MySQL configuration: we start from the official MySQL image , and as you can see there is a new environment key we haven't met so far: it allows to declare some environment variables that will be accessible in the container. More specifically here, we set the root password for MySQL, and a name ( project ), a user and a password for a database to be created (all the available variables are listed in the image's documentation ). Following the same principle as exposed earlier, we also declare a data container whose aim is only to hold the MySQL data present in /var/lib/mysql on the container (and reusing the same MySQL image to save disk space). You might have noticed that, unlike what we were doing so far, we do not declare a specific directory on the host machine to be mounted into /var/lib/mysql (normally specified before the colon): we don't need to know where this directory is, we just want its content to persist, so we let Docker Compose handle this part. Although that does not mean we have no idea where this folder sits - but we'll have a look at this later. One thing worth noting right now tho, is that if this volume already contains MySQL data, the MYSQL_ROOT_PASSWORD variable will be ignored and if the MYSQL_DATABASE already exists, it will remain untouched. In order to be able to test the connection to the database straight away, let's update the index.php file a bit: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <title>Hello World!</title> </head> <body> <img src=\"https://tech.osteel.me/images/2015/12/18/docker-tutorial2.gif\" alt=\"Hello World!\" /> <?php $database = $user = $password = \"project\"; $host = \"mysql\"; $connection = new PDO(\"mysql:host={$host};dbname={$database};charset=utf8\", $user, $password); $query = $connection->query(\"SELECT TABLE_NAME FROM information_schema.TABLES WHERE TABLE_TYPE='BASE TABLE'\"); $tables = $query->fetchAll(PDO::FETCH_COLUMN); if (empty($tables)) { echo \"<p>There are no tables in database \\\"{$database}\\\".</p>\"; } else { echo \"<p>Database \\\"{$database}\\\" has the following tables:</p>\"; echo \"<ul>\"; foreach ($tables as $table) { echo \"<li>{$table}</li>\"; } echo \"</ul>\"; } ?> </body> </html> This script will try to connect to the database and list the tables it contains. We're all set! Run docker-compose up -d in your terminal again, followed by docker ps -a (again, it might take a little while as the MySQL image needs to be pulled). You should see five containers, two of which are exited: Now refresh the browser tab: \"There are no tables in database 'project'\" should appear. Event though the next point is about phpMyAdmin and you will be able to use it to edit your databases, I am now going to show you how to access the running MySQL container and use the MySQL command line interface. From the result of the previous command, copy the running MySQL container ID ( 5207587d116b in our case) and run: $ docker exec -it 5207587d116b /bin/bash You are now running an interactive shell in this container (you can also use its name instead of its ID). docker exec allows to execute a command in a running container, -t attaches a terminal and -i makes it interactive. Finally, /bin/bash is the command that is run and creates a bash instance inside the container. Of course, you can use the same command for other containers too. From there, all you need to do is run mysql -uroot -psecret to enter the MySQL CLI. List the databases running show databases; : Change the current database for project and create a new table: $ mysql> use project $ mysql> CREATE TABLE users (id int); Refresh the project page: the table users should now be listed. You can exit the MySQL CLI entering \\q and the container with ctrl + d . There is one question about the MySQL data we haven't answered yet: where does it sit on the host machine? Earlier we set up the data container in docker-compose.yml as follows: data: image: mysql:latest volumes: - /var/lib/mysql command: \"true\" That means we let Docker Compose mount a directory of its choice from the host machine into /var/lib/mysql . So where is it? Run docker ps -a again, copy the ID of the exited MySQL container this time ( 7970b851b07a in our case) , and run : $ docker inspect 7970b851b07a Some JSON data should appear on your screen. Look for the Mounts section: ..., \"Mounts\": [ { \"Name\": \"0cd0f26f7a41e40437019d9e5514b237e492dc72a6459da88d36621a9af2599f\", \"Source\": \"/mnt/sda1/var/lib/docker/volumes/0cd0f26f7a41e40437019d9e5514b237e492dc72a6459da88d36621a9af2599f/_data\", \"Destination\": \"/var/lib/mysql\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true } ], ... The data contained in the volume sits in the \"Source\" directory. \"Hmm I see. But… what happens to the volumes if we remove the containers that hold them?\" Excellent question! Well, they actually stay around, taking disk space for nothing. Two solutions for this. First, we can make sure to remove the volumes along with the container using the -v option: $ docker rm -v containerid Or, if some containers were removed without the -v option, resulting in dangling volumes: $ docker volume rm $(docker volume ls -qf dangling=true) This command uses the docker volume ls command with the q and f options to respectively list the volumes' names only and keep the dangling ones ( dangling=true ). Check the Docker command line documentation for more info. phpMyAdmin Being able to access a container and deal with MySQL using the command line interface is a good thing, but sometimes it is convenient to have a more friendly user interface. PHPMyAdmin is arguably the de facto choice when it comes to MySQL, so let's set it up using Docker! Open docker-compose.yml again and add the following: phpmyadmin: image: phpmyadmin/phpmyadmin ports: - 8080:80 links: - mysql environment: PMA_HOST: mysql Once again, we start from the official phpMyAdmin image . We publish its port 80 to the virtual machine's port 8080, we link it to the mysql container (obviously) and we set it as the host using the PMA_HOST environment variable. Save the changes and run docker-compose up -d again. The image will be downloaded and, once everything is up, visit the project page again, appending :8080 to the private IP (that's how we access the VM's port 8080): Enter root / secret as credentials and you're in ( project / project will work too, giving access to the project database only, as defined in the mysql container configuration). That one was easy, right? That's actually it for the setup! This is a lot to digest already, so taking a break might be a good idea. Don't forget to read on eventually though, as the next couple of sections will most likely clarify a few points. Again, the setup is also available as a GitHub repository . Feel free to clone it and play around. Handling multiple projects One big advantage of using Docker rather than say, multiple Vagrant boxes, is if several containers use the same base images, the disk space usage will only increase by the read-write layer of each container, which is usually only a few megabytes. To put it differently, if you use the same stack for most of your projects, each new project will basically only take a few extra megabytes to run (not taking into account the size of the codebase here, obviously). Say you want to use the same stack except you need PostgreSQL instead of MySQL. All you need to do is change the database container image for your new project, and all the other containers will reuse the images you already have locally. Pretty neat. But how do you concretely deal with several projects using the same virtual machine? First thing to consider is a Docker machine has only one private IP address, so unless you decide to use a different port per project (which should be absolutely fine), you won't be able to have multiple web projects running on the port 80 at the same time. That's not really an issue as you can easily stop containers. You might have noticed that we used docker and docker-compose commands in turn, which might be a tad confusing. To simplify, let's say docker-compose allows to run the same commands as docker , but on all the containers defined in the docker-compose.yml file at once, or for these containers only. Let's take a couple of examples: docker ps -a You already know this command: it displays all the containers of the Docker machine, be they running or not. All of them. However: docker-compose ps will do the same, but for the containers defined in the docker-compose.yml file of the current directory only (you will notice that -a is not necessary and that the order of the displayed info is slightly different). It comes in handy when you begin to have a lot of containers managed by the same VM. But there is more: docker-compose stop will stop all the containers described in the current docker-compose.yml file. Basically when you are done with a project, run this command to stop all its related containers. To connect the dots with what I said above, that will also free the port 80 for another project if need be. Start them again using the now familiar docker-compose up -d . You can also delete all the stopped containers of the current project running: docker-compose rm Just like its equivalent docker rm , you can add the option -v if you want to remove the corresponding volumes as well (if you don't, you might end up with dangling volumes as already mentioned earlier). Check the docker and docker-compose references for more details. Troubleshooting Obviously, not everything is always going to work at once. Docker Compose might refuse to build an image or start a container and what is displayed in the console is not always very helpful. When something goes wrong, run: docker-compose logs This will display the logs for the containers of the current docker-compose.yml file. You can also run docker-compose ps and check the State column: if there is an exit code different than 0 , there was a problem with the container. Display the logs specific to a container with: docker logs containerid The container's name will work too. Conclusion Docker is an amazing technology and of course there is much more to it (I'm not even started with deployment and how to use it into production and believe me, it is very promising). Here we used Docker Toolbox and Boot2Docker for simplicity, because it sets up a lot of things for us automatically, but we could have used a Vagrant box just the same and install Docker on it. There is no obligation whatsoever. Putting all this together was no trivial exercise, especially as I knew very little about Docker before I started to write this article. It actually took me a couple of days of tinkering around before I started to make any sense of it. Docker is evolving super quickly and there are many resources out there from which it's not always easy to separate the wheat from the chaff. I believe I only kept the wheat, but then again I'm still a humble beginner so if you spot some chaff, you are very welcome to let me know about it in the comments. Sources Discovering Docker (e-book) How I develop in PHP with CoreOS and Docker Docker for PHP Developers Understanding Volumes in Docker Manage data in containers Tips for Deploying NGINX (Official Image) with Docker Use the Docker command line Compose CLI reference Dockerfile reference","tags":"Docker","url":"https://tech.osteel.me/posts/from-vagrant-to-docker-how-to-use-docker-for-local-web-development"},{"title":"How to use the fork of a repository with Composer","text":"When using packages maintained by other developers, you often find yourself waiting for a fix, an update or the merge of a PR that will come with the next release but you need it now. A workaround is to fork the corresponding repository (which you may have done already if you are the author of the PR), make the updates you need and then use your fork in your project instead of the original package. If I wanted to use a fork of Guzzle for example, I would edit composer.json like so ( osteel is my GitHub username): { \"name\": \"osteel/myproject\", \"description\": \"My project.\", \"license\": \"MIT\", \"type\": \"project\", \"repositories\": [ { \"type\": \"vcs\", \"url\": \"https://github.com/osteel/guzzle\" } ], \"require\": { \"laravel/lumen-framework\": \"5.1.*\", \"guzzlehttp/guzzle\": \"dev-master\" } } This would use the master branch of my fork (replace master with whichever branch name you want to use). Don't forget to watch the original repository to be aware of new releases, or use http://gh-release-watch.com if you don't want to get notifications for all the issues, conversations, etc (while GitHub is considering the possibility of watching for releases only ).","tags":"PHP","url":"https://tech.osteel.me/posts/how-to-use-the-fork-of-a-repository-with-composer"},{"title":"Handling CORS with Nginx","text":"[UPDATE 2015/08/02] As @OtaK_ pointed out , in most cases CORS should be handled directly by the app as it should return the allowed verbs by endpoint, instead of all of them being allowed by Nginx. This config should only be used for quick development, of a prototype or PoC for example, or if you are certain that the same verbs are allowed for all the endpoints (that would be the case for the assets returned by a CDN, for instance). [/UPDATE] With the always wider adoption of API-driven architecture, chances are you already had to deal with cross-origin resource sharing at some point. Whilst it is possible to deal with it from the code and you will find many packages or snippets to do so, we can remove the CORS handling from our app and let the HTTP server take care of it. The Enable CORS website contains useful resources to this end, but when I tried to use their Nginx config for my own projects it didn't quite work as expected. The following examples are based on the Nginx server configurations generated by Homestead , but the steps won't change much even if you are not using Laravel's dev environment. nginx-extras First of all, Nginx's traditional add_header directive doesn't work with 4xx responses. As we still want to add custom headers to them, we need to install the ngx_headers_more module to be able to use the more_set_headers directive, which also works with 4xx responses. While the documentation suggests to build the Nginx source with the module, if you are on a Debian distro you can actually easily install it with the nginx-extras package : sudo apt-get install nginx-extras The server configuration Here is what a typical server config of a Laravel project looks like, without the CORS bit (I am voluntarily omitting the SSL part to keep the post short, but it works exactly the same): server { listen 80; server_name example-site.com; root \"/home/vagrant/projects/example-site/public\"; index index.html index.htm index.php; charset utf-8; location / { try_files $uri $uri/ /index.php?$query_string; } location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } access_log off; error_log /var/log/nginx/example-site.com-error.log error; sendfile off; client_max_body_size 100m; location ~ \\.php$ { fastcgi_split_path_info &#94;(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off; fastcgi_buffer_size 16k; fastcgi_buffers 4 16k; } location ~ /\\.ht { deny all; } } Now, with the CORS handling: server { listen 80; server_name example-site.com; root \"/home/vagrant/projects/example-site/public\"; index index.html index.htm index.php; charset utf-8; more_set_headers 'Access-Control-Allow-Origin: $http_origin'; more_set_headers 'Access-Control-Allow-Methods: GET, POST, OPTIONS, PUT, DELETE, HEAD'; more_set_headers 'Access-Control-Allow-Credentials: true'; more_set_headers 'Access-Control-Allow-Headers: Origin,Content-Type,Accept,Authorization'; location / { if ($request_method = 'OPTIONS') { more_set_headers 'Access-Control-Allow-Origin: $http_origin'; more_set_headers 'Access-Control-Allow-Methods: GET, POST, OPTIONS, PUT, DELETE, HEAD'; more_set_headers 'Access-Control-Max-Age: 1728000'; more_set_headers 'Access-Control-Allow-Credentials: true'; more_set_headers 'Access-Control-Allow-Headers: Origin,Content-Type,Accept,Authorization'; more_set_headers 'Content-Type: text/plain; charset=UTF-8'; more_set_headers 'Content-Length: 0'; return 204; } try_files $uri $uri/ /index.php?$query_string; } location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } access_log off; error_log /var/log/nginx/example-site.com-error.log error; sendfile off; client_max_body_size 100m; location ~ \\.php$ { fastcgi_split_path_info &#94;(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_intercept_errors off; fastcgi_buffer_size 16k; fastcgi_buffers 4 16k; } location ~ /\\.ht { deny all; } } And that is pretty much it. All you need to do now is to reload your Nginx confs: sudo service nginx reload Extra considerations Note that this allows any domain to access your app, and while this is most likely enough for local development, on a production server you might want to fine-tune this configuration to allow specific domains only ( Access_Control_Allow_Origin ). More generally, all the headers' values are examples and you can modify them as you see fit. You could also put the global and options-related snippets into separate files (in /etc/nginx/shared/ , for example) and import them with the Nginx's include directive.","tags":"Nginx","url":"https://tech.osteel.me/posts/handling-cors-with-nginx"},{"title":"Database management with Adminer (and how to install on Homestead)","text":"For quite some time now I have been prefering accessing databases from the CLI, but sometimes it can feel overkill when wanting to quickly check or update something, say. In this kind of case I usually look to PHPMyAdmin, but as I was looking for an equivalent for PostgreSQL I stumbled upon Adminer . It is basically a single file PHP script allowing to connect to and manage different DBMSs (namely MySQL, PostgreSQL, SQLite, MS SQL, Oracle, Firebird, SimpleDB, Elasticsearch and MongoDB) super easily. Here are a few simple steps to use it on Homestead, but the process wouldn't differ much whatever your development environment is. First, download adminer.php , rename it index.php and move it to an adminer folder under your usual Homestead projects directory. Add a new site to your Homestead.yaml file (adapt the path to your own config if necessary): sites: - map: adminer.local to: /home/vagrant/projects/adminer Re-provision your box: $ cd ~/Homestead $ vagrant provision Edit your local hosts file to match the Homestead box IP address to the domain (on a Mac that's /etc/hosts ): 192.168.10.10 adminer.local You should now be able to access http://adminer.local : To access the MySQL databases, simply select \"MySQL\" in the \"System\" dropdown, type \"localhost\" in \"Server\" (there is a placeholder that looks like the value is entered by default but you actually need to type it in), and the default \"homestead\" and \"secret\" as \"Username\" and \"Password\" respectively (you can also tick the \"Permanent login\" box so the details are added as a link on the left column for quick access). The default PostgreSQL user doesn't have a password so you need to set one first. ssh your Homestead box and run: $ sudo -u postgres psql You should be connected to PostgreSQL (notice postgres=# at the beginning of the prompt). Now type: $ postgres=# alter user postgres password 'secret'; You've just set up a password for the default user ( \"secret\" , just like for MySQL). Quit this interface typing \\q and try to access the PostgreSQL databases from Adminer (select \"PostgreSQL\" in the \"System\" dropdown, type \"localhost\" in \"Server\" , and \"postgres\" and \"secret\" as \"Username\" and \"Password\" respectively). I personnally don't like the default theme, and there are a few other designs available from the website . Download whichever you fancy (I went for \"pappu687\" ) and place the adminer.css file at the same level as the index.php file you moved earlier. Refresh http://adminer.local . You're done!","tags":"Laravel","url":"https://tech.osteel.me/posts/database-management-with-adminer-and-how-to-install-on-homestead"},{"title":"Laravel Homestead: debug an API with Xdebug and cURL in Sublime Text","text":"Foreword There are a few tutorials out there about how to set up Sublime Text and Xdebug so they play nice together. The good news is that in our case, Homestead has covered the configuration of Xdebug for us: the tool is already available and reporting for duty. You will find its settings in /etc/php5/fpm/conf.d/20-xdebug.ini , which should look like this: zend_extension=xdebug.so xdebug.remote_enable = 1 xdebug.remote_connect_back = 1 xdebug.remote_port = 9000 xdebug.max_nesting_level = 250 I won't go into more detail about this, but you can have a look at this post by sitepoint for a more complete explanation. Might be worth mentioning that my host machine runs Mac OS, and the steps below might slightly differ if you are on a different OS. Prerequisites This is assuming Homestead is installed on your machine and that you've got a basic knowledge of it. If you are confused by the documentation, this free Laracast might help. I also wrote a short guide on how to set up a basic PHP/MySQL project on Homestead . We will use a small Laravel project (v5.0.31) for testing purpose, which you can clone from this repository , inside a directory of your choice (you probably already have one containing your Laravel projects): git clone git@github.com:osteel/xdebug-api-blog-tutorial.git xdebug-api Add the site in your ~/.homestead/Homestead.yaml file: sites: - map: xdebug-api.local to: /home/vagrant/path/to/xdebug-api/public Where /path/to/ should be replaced with the path to the project on your Homestead box. Match the Homestead IP address with \"xdebug-api.local\" in your local hosts file ( /etc/hosts on a Mac) (of course, adapt the IP address if you changed the default one): 192.168.10.10 xdebug-api.local You should now re-provision your Homestead box. Go to ~/Homestead/ and run vagrant provision if the box is already running, or vagrant up --provision if it needs to be started. You should now be able to access http://xdebug-api.local , which should display the Laravel welcome page. Sublime Text We first need to get the Xdebug Client package for Sublime. If you've never installed any package before, the easiest way is to install package control first. Follow the steps, then hit shift + cmd + p in Sublime (or shift + ctrl + p on Windows) and type \"install\" until the \"Package Control: Package Install\" entry appears: Hit enter and wait for the packages input to display. Type \"xdebug\" and hit enter after having selected the Xdebug Client one to install it. We now need to set up a Sublime Text project. Open the project folder in Sublime, then go to the \"Project\" menu, \"Save Project As...\". Name it \"xdebug-api.sublime-project\" and save it at the root. Open it and replace its content with: { \"folders\": [ { \"follow_symlinks\": true, \"path\": \".\" } ], \"settings\": { \"xdebug\": { \"url\": \"http://xdebug-api.local/\", \"path_mapping\": { \"/home/vagrant/path/to/xdebug-api/\" : \"/path/to/xdebug-api/\" } } } } where the two occurrences of /path/to/ should be replaced with the path to the project on the Homestead box and the path to the project on your host machine respectively. Using Xdebug Let's make sure everything is correctly set up. The project is extremely simple and offers a couple of endpoints, both in the WelcomeController.php file and declared in routes.php . One will receive GET requests and the other POST ones (corresponding to the get and post methods in WelcomeController.php ). Route::get('/', 'WelcomeController@index'); Route::get('/get', 'WelcomeController@get'); Route::post('/post', 'WelcomeController@post'); Since we installed the package, there is a new \"Xdebug\" menu available under \"Tools\": Add a breakpoint in the index method of WelcomeController.php (notice the circle on the left): Now start the debugging mode (\"Tools\", \"Xdebug\", \"Start Debugging (Launch Browser)\"): your default browser will open http://xdebug-api.local/?XDEBUG_SESSION_START=sublime.xdebug . The page should just hang in there, and switching back to Sublime Text you should see something like that: The script execution stopped at your breakpoint, and Xdebug displays information about the different objects available at this point and their values in the panels at the bottom. Now you might think that you could basically test all the GET endpoints of your API this way, and you would not be entirely wrong. But how about other methods (POST, PUT, etc.)? And what if you need a specific header, such as a bearer token? That's where cURL comes in handy. cURL First let's add another breakpoint, in the get method this time: Open a terminal on your host machine and run (cURL is available on Mac OS by default, but you might need to install it if you use another platform): curl http://xdebug-api.local/get You should get an array of pizzas in JSON format. The breakpoint was ignored, the reason being Xdebug needs a cookie to be read so its session can be started (that's what happens when you append \"?XDEBUG_SESSION_START=sublime.xdebug\" to the URL: a cookie is created). What we are going to do is to \"add\" this cookie to the cURL call. Now run: curl -b \"XDEBUG_SESSION=sublime.xdebug\" http://xdebug-api.local/get The script execution should stop at the breakpoint and Sublime Text should look like: What happened here is we told cURL to pass a cookie to the request, created from a string ( \"XDEBUG_SESSION=sublime.xdebug\" ). Now let's try a POST request. Create a new breakpoint in the post method: And run: curl -b \"XDEBUG_SESSION=sublime.xdebug\" -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"Pepperoni\"}' http://xdebug-api.local/post Displaying back Sublime Text; you should get something like: To explain the command a little bit: -X allows to specify the method ( GET by default) -H allows to specify headers -d allows to pass some data (here, some JSON) There are many things you can do with cURL, that should cover pretty much all the cases presented by your API. I encourage you to have a look at its documentation . Extra considerations Alias You will probably agree that having to type -b \"XDEBUG_SESSION=sublime.xdebug\" every time is somewhat annoying. To speed up the process a little bit, I use an alias command I named curlx . To add it on a Mac for example, edit ~/.bash_profile : vim ~/.bash_profile Add this line: alias curlx='curl -b \"XDEBUG_SESSION=sublime.xdebug\"' Then source the file so the new alias is taken into account: source ~/.bash_profile And run: curlx http://xdebug-api.local/get If the breakpoint previously set in the get method is still active, your script should stop there. cURL command handy You may also have noticed that I put a cURL command in the description of each method in WelcomeController.php : I do so so I can test them quickly, with a terminal open on the side. How to use Xdebug Finally, this tutorial doesn't actually explain how you might use Xdebug. For concrete examples, you can watch a couple of Laracast videos (free upon opening an account): Xdebug (from 5'20'') Xdebug and Laravel These videos were shot using PHPStorm, but the described processes apply all the same.","tags":"Laravel","url":"https://tech.osteel.me/posts/laravel-homestead-debug-an-api-with-xdebug-and-curl-in-sublime-text"},{"title":"Extending Homestead: how to customize Laravel's Virtual Machine (the example of Apache)","text":"Homestead offers a nice pre-packaged environment. But as a project grows in complexity, there will be a time where extra packages will be necessary. How to install them properly, and not to lose everything any time we need to recreate the box? How does one extend Homestead? Foreword Impatient kids can skip to the next section . The more I use Laravel, the more I enjoy it. I am also quite a fan of Vagrant . That's actually one of the things that got me interested in the framework in the first place: that they were offering a pre-packaged Vagrant box (namely Homestead). The fact they are using such a cool tool can only be a good sign, right? But then I realized they were implicitely advocating using the same box for all sorts of PHP projects. I understand that most of them wouldn't even use all of the included features , but it nonetheless feels like it goes against the principle of Vagrant, that is isolating project environments and more specifically mimicking the production server as closely as possible. It's like leaving the window open for the \"it works on my machine\" syndrom when the door has been carefully locked. It also poses another problem: if the included software is pretty good as it is, chances are developers will need extra packages at some point. There will be those who don't really know how to install them, as everything was set up for them from the start, and those who are comfortable enough to do it from the CLI, but then what if they need to recreate the Vagrant box for some reason? All the extra stuff is gone. I stumbled upon Yitzchak Schaffer's post on the subject, where I got introduced to the \"Laragarden\" term, which defines the feeling quite well. He explains his concerns in a more detailed way than I do, and if you are interested in the subject I invite you to read his post. Now those familiar with Vagrant will probably have thought of provisioning by now, and that was my thinking too. I started to look for a clean way to customize Homestead, which led me to this post , in which the author advocates creating a bash script to run any time the Homestead box is (re)created. This looked clean enough to me, except that I couldn't be bothered executing the script manually every time, hence my decision to edit Homestead's VagrantFile instead. That's when I realised the guys behind Laravel had that covered already. More on that in a bit. Prerequisites This is assuming you already have homestead running on your machine . If you are confused by the documentation, this free Laracast might help. Might be worth mentioning that my host machine runs Mac OS, and the steps below might slightly differ if you are on a different OS. Custom provisioning Looking at the VagrantFile , I noticed a few interesting lines: afterScriptPath = File.expand_path(\"~/.homestead/after.sh\") and, later on: if File.exists? afterScriptPath then config.vm.provision \"shell\", path: afterScriptPath end And here is the content of ~/.homestead/after.sh : # If you would like to do some extra provisioning you may # add any commands you wish to this file and they will # be run after the Homestead machine is provisioned. Gotta love Laravel... What it means is that after all the base Homestead set up is done, the script will look for a file named after.sh in the .homestead/ folder under your user's directory, and run its content if it exists. This file is created along with the Homestead.yaml file when you first run bash init.sh after cloning the Homestead repository. As ~/.homestead/ is an independent folder, and if you work with other developers on the same project(s), it might be a good idea to version it for consistency across the team. From there, all that is left to do is actually to put the necessary install scripts in after.sh . Handy! The example of Apache Let's take a simple example here. Homestead comes with Nginx preinstalled (nothing bad about that, quite the opposite actually), but you might have some projects that need to be run on Apache. If both servers cannot run at the same time, there is no harm in having them both installed and starting one or the other according to the current need. From your host machine, open a terminal window and edit the after.sh mentioned above: vim ~/.homestead/after.sh Add these lines at the end of it: sudo apt-get update sudo apt-get install -y apache2 What this will do is it will update the package lists to make sure we'll grab the latest version of the Apache one, and then install it ( -y is there to answer \"yes\" by default where the user is normally prompted). Save the file and go to ~/Homestead/ , then run: vagrant provision This will force Homestead to re-run the provision scripts, and install the Apache server using the script you've just added. ssh your Homestead box and run: sudo service apache status If everything went alright, you should get something like that: * apache2 is not running This is expected, as it means that 1. Apache has been correctly installed and 2. Nginx is already running and we can't have both running at the same time. What we need to do now is to stop Nginx first and then start Apache: sudo service nginx stop sudo service apache2 start Accessing your Homestead machine from the browser should now display the default Apache screen (chances are the address is http://192.168.10.10 ): That being said and done, you should be aware that Homestead will not create the Apache Virtual Hosts for you (as it does for the Nginx server confs). You will have to add them yourself in /etc/apache2/ . This also means that if you recreate the Homestead box, these Virtual Hosts will be gone (it is not that difficult to use provisioning to automate this, but this is beyond the scope of this article - more on that in the conclusion). Conclusion Here is how to extend your Homestead box in a clean and simple way (all it takes is to update an existing file, after all). Of course it implies using bash scripts and that might be a bit frustrating for Chef or Puppet users (even though I don't think there would be any harm in updating the Vagrantfile directly), but I'd say it is a good start already, and it kinda shows that Laravel's intent is not necessarily to lock you up in their Laragarden (even though you won't find any mention of after.sh in the official documentation, at the time of writing). To my opinion, Homestead is definitely a good kick-start and a nice first exposure to Vagrant to some users, but as a project grows in complexity, using a dedicated Vagrant box might be a good idea. By the way, if you want to learn more about Vagrant, I wrote an article explaining how to set up a Vagrant box for local development step by step . It will also show you how to import a Nginx server config, which can be adapted to Apache Virtual Hosts. Just sayin' ;)","tags":"Laravel","url":"https://tech.osteel.me/posts/extending-homestead-how-to-customize-laravels-virtual-machine-the-example-of-apache"},{"title":"How to start a new Laravel 5 project with Homestead - quick reference","text":"I wrote this short get-started guide mainly for my own use, to have a reference handy to quickly set up a new Laravel project with a MySQL database. But as I felt the need to write it, one might find some interest in it as well. [UPDATE 11/05/2015]: As the use of Composer to install Homestead has been deprecated (at least it disappeared from the doc), this post is not using the homestead commands anymore. Prerequisites This is assuming homestead is installed . If it is not the case, please follow the instructions first: once completed, you will have a fully provisioned virtual machine run by Vagrant and containing everything necessary to develop with Laravel in the best conditions (if you are having trouble following the different steps, here is a free Laracast video that might explain it better). If you have no idea what Vagrant is and why you need to install it along with something like VirtualBox, feel free to read the article I wrote on the subject . A note about ssh Laravel's documentation suggests to add an alias to ssh the Homestead box more quickly. I personnally prefer to use an ssh config to that purpose, as it is intended to. Update or create a config file in the .ssh folder of your home directory: vim ~/.ssh/config Add the following content: # Homestead Host homestead HostName 127.0.0.1 Port 2222 User vagrant You can now access the Homestead machine from anywhere running: ssh homestead Create the project First add a new site to the ~/.homestead/Homestead.yaml file: vim ~/.homestead/Homestead.yaml Something like: sites: - map: site1.local to: /home/vagrant/projects/site1 - map: new-site.local to: /home/vagrant/projects/new-site/public Here we want to start a new Laravel project (arbitrarily called \"new-site\") so we point the root to the public/ directory. There is also another method, using the serve script, allowing to add a new project without having to edit Homestead.yaml nor to provision the box again. My preference goes to the first method because I like keep track of my existing projects in Homestead.yaml . I don't specify the database in this file, because I don't want the databases to be reset every time I provision the Vagrant box. Speaking of which, we now need to run this command, from ~/Homestead/ : vagrant provision or: vagrant up —-provision if your box isn't running yet. This will basically create the right Nginx config on the Vagrant box and restart Nginx in there (and, like I said, reset the databases that are declared in the Homestead.yaml file, so be careful. Simply remove the databases for which you don't want that to happen under databases from that file). Now, edit the hosts file of your host machine ( sudo vim /etc/hosts on MacOS) to match the new domain to the box's IP: 192.168.10.10 new-site.local Then, ssh the box and create your new project using Composer (in my config, all my projects are under the ~/projects/ directory of the Vagrant box) (this will probably take a little while): ssh homestead cd projects composer create-project laravel/laravel new-site This will copy Laravel and all its dependencies in the new-site directory. If everything went fine, you should now be able to access http://new-site.local and see the default Laravel screen: Set up the database From the Homestead box, connect to MySQL and create the database with the right user (the password for homestead is \"secret\"): mysql -uhomestead -p create database newsite; grant usage on *.* to newsite@localhost identified by 'password'; grant all privileges on newsite.* to newsite@localhost; flush privileges; \\q Be sure to note the chosen password and edit the .env file of your project, changing the values for the database details: DB_HOST=localhost DB_DATABASE=newsite DB_USERNAME=newsite DB_PASSWORD=password Laravel comes with a couple of database migration scripts, creating the users and password_resets tables respectively. A quick way to check that the database is correctly set up is to run these scripts, from the project's root: php artisan migrate If the migration is successful, you're all set! Reset the database if necessary: php artisan migrate:reset That's it! A few quick steps to get started on a new Laravel project with Homestead.","tags":"Laravel","url":"https://tech.osteel.me/posts/how-to-start-a-new-laravel5-project-with-homestead-quick-reference"},{"title":"Install and deploy a Pelican blog using Fabric - Part 4: workflow, extras and conclusion","text":"Alright! This was a bit of a long road, but we are finally getting there. In the previous part , we used Fabric to fully provision a server and pull our content from a Git repository. In this fourth and last part, we are going to review a complete worklow, take a few extra steps to complete our blog and conclude our journey. Summary Part 1: local environment Part 2: installation and configuration Part 3: Fabric Part 4: workflow, extras and conclusion Complete workflow Extras Images, favicon and other static files Google Analytics Sitemap Feeds Conclusion Sources Complete Workflow Let's have a look at what a complete workflow would look like: it will also summarize all that we have done so far. Here is mine: Create a new article directly under \"content/posts/\" , with \"Status: draft\" Edit the content (I personally use Mou , a Markdown editor for Mac) Generate and serve the blog locally: fab reserve Access http://localhost:8000/drafts and check the look of the article Edit and push the article to the Git repository as often as necessary until it is finished When ready to publish, create the right year/month/day folders under \"content/\" , move the article there and remove \"Status: draft\" Git commit and push fab publish Check the article on the live website ...and Bob's your uncle. Extras If you followed all the steps up to now, you already have a fully functional blog. But there are a few extra things you will probably want to add in. Some of them are coming from the tips and tricks page. Images, favicon and other static files Chances are you will want to add images to some of your articles. They will have to be stored somewhere and copied over at compilation. To that end, create a folder named \"images\" in \"content\" . I personally follow the same structure as for the articles, e.g. I place pictures under \"images/2015/02/22/\" for articles published on that day. To have this directory copied to \"output/\" with the rest of the content, open \"pelicanconf.py\" and add this section: STATIC_PATHS = [ 'images' ] This simply indicates to the generation script that this directory is to be copied as is under \"output/\" . And this is how you would embed images in your articles (Markdown syntax): ![\"Example image\"](/images/2015/02/22/example.jpg \"Example image\") How about a favicon? Create another directory under \"content/\" , named \"extra\" . Place your favicon in there, and edit the config file again: STATIC_PATHS = [ 'images', 'extra' ] EXTRA_PATH_METADATA = { 'extra/favicon.ico': {'path': 'favicon.ico'} } The \"EXTRA_PATH_METADATA\" allows to specify more precisely the path of specific files. Here, we basically say that we want \"favicon.ico\" from the \"extra\" directory to be copied at the root of the blog. You can add as many files as you wish in there, such as a \"robots.txt\" , for example. Google Analytics Pelican supports Google Analytics out of the box. All it takes is to add the following line to the \"publishconf.py\" file ( \"UA-XXXX-Y\" being your own tracking id): GOOGLE_ANALYTICS = \"UA-XXXX-Y\" Sitemap Under \"theme/templates/\" , add a \"sitemap.html\" file: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> {% for article in articles %} <url> <loc>{{ SITEURL }}/{{ article.url }}</loc> <priority>0.8</priority> </url> {% for translation in article.translations %} <url> <loc>{{ SITEURL }}/{{ translation.url }}</loc> <priority>0.8</priority> </url> {% endfor %} {% endfor %} {% for page in pages %} <url> <loc>{{ SITEURL }}/{{ page.url }}</loc> <priority>1.0</priority> </url> {% for translation in page.translations %} <url> <loc>{{ SITEURL }}/{{ translation.url }}</loc> <priority>1.0</priority> </url> {% endfor %} {% endfor %} </urlset> Edit \"pelicanconf.py\" : DIRECT_TEMPLATES = (('index', 'tags', 'categories', 'archives', 'sitemap')) SITEMAP_SAVE_AS = 'sitemap.xml' Feeds Are also supported by default. This is just a matter of configuration, which is all well explained in the documentation . Conclusion Well well well. Obviously this was way more complicated than just opening a Tumblr or a Wordpress. I assume that if you chose to go down that road, it was also for the educational aspects that were coming with it. So did I. And really, I have learned a lot putting all this together, and I hope you are taking away a little bit of new knowledge as well. So now what? Well first, tap yourself on the back and take a break, you deserved it. Anything unclear? Don't hesitate to refer to the result repository , it might be helpful. Or just drop a line in the comments, I'll be happy to help. Then, here are a few leads if you want to go further. The first and obvious one is to host everything on a real server. I personally use a DigitalOcean (referral link) droplet, because it is cheap and easy to setup (the most basic one is more than enough to serve a static HTML blog like Pelican). In any case, once you have got your server, all it should take is to update the corresponding section of the \"fabfile.py\" file, as described at the end of the \"provision\" section . You could also add a new Fabric function to speed up the process of starting a new article (have a look at the make version of this on the tips and tricks page for inspiration, or use it as is if you prefer). Finally, if you are using GitHub to host your repository, why not trying to use a webhook to make the publication even easier? Sources This tutorial is the result of the combination of many different sources. Apart from the official Pelican documentation and the Fabric one , here are the articles and people that were helpful: How to use Pelican, GitHub, and a DigitalOcean VPS to host a cool blog : this is the post that truly got me started. I was having a hard time finding a complete resource and I found all I needed to get the ball rolling in this excellent work How I built this website, using Pelican: Part 1 - Setup Howto Setup Comments with Disqus in Pelican Finally, thanks to Josh for having introduced me to Fabric","tags":"Blog","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-4-workflow-extras-and-conclusion"},{"title":"Install and deploy a Pelican blog using Fabric - Part 3: Fabric","text":"In part 2 , we covered the installation and configuration of Pelican in our local environment. It is now time to provision our server and publish content using Fabric. But first, let's version our blog. Summary Part 1: local environment Part 2: installation and configuration Part 3: Fabric Versioning Deployment with Fabric fabfile.py Provision Publish Part 4: workflow, extras and conclusion Versioning The next stop in our journey is versioning. What we want to do is to be able to push our content to some repository so it can be pulled on the server later on (and it is always good to have a backup that we can clone and start working on locally in no time). Create a new repo on your favorite host (like GitHub or Bitbucket ) and set up your local one: git init git remote add origin your-repo-address Create a \".gitignore\" file ( \".vagrant/\" is not required if you don't use Vagrant): output/* *.py[cod] cache/ .vagrant/ We exclude the content of \"output/\" as the content will be generated directly on the live server, with its own settings. Make your first commit and push your code: git add -A git commit -m 'First commit' git push That's it! Deployment with Fabric Fabric is a command-line tool written in Python allowing to perform remote operations over SSH. It is often used for server provisioning and deployment. We installed it earlier with pip ( pip install Fabric ) and started using it already to build and serve our blog locally, using the fab build and fab serve commands among others. These commands are contained in a file named \"fabfile.py\" , at the root of your blog. We are now going to have a look at it and explain it a little bit. fabfile.py Let's open the file in and editor and dissect it section by section: from fabric.api import * import fabric.contrib.project as project import os import sys import SimpleHTTPServer import SocketServer Basically the imports of all the necessary libraries. SimpleHTTPServer is the one used by fab serve , for example. # Local path configuration (can be absolute or relative to fabfile) env.deploy_path = 'output' DEPLOY_PATH = env.deploy_path # Remote server configuration production = 'root@localhost:22' dest_path = '/var/www' # Rackspace Cloud Files configuration settings env.cloudfiles_username = 'my_rackspace_username' env.cloudfiles_api_key = 'my_rackspace_api_key' env.cloudfiles_container = 'my_cloudfiles_container' This section is about configuration. We will update the \"Remote server configuration\" bit soon. We are not using Rackspace in this tutorial so we can just ignore this part. The following function definitions (e.g. \"def clean()\" ) are all the commands you can run from the terminal (using \"fab command_name\" as you already know). You are already familiar with a few of them ( build , serve , reserve ) and I encourage you to take a look at the others and try them out (you might find that you are more comfortable with some of them for your workflow). The code for most of these commands is quite self-explanatory. Now let's have a look at the last one, \"publish\" : @hosts(production) def publish(): local('pelican -s publishconf.py') project.rsync_project( remote_dir=dest_path, exclude=\".DS_Store\", local_dir=DEPLOY_PATH.rstrip('/') + '/', delete=True, extra_opts='-c', ) What's happenning here? First, we indicate that we want to ssh the host whose configuration is contained in the \"production\" variable mentioned above. Then, in the body of the function itself, we first generate the HTML locally using the live config ( \"publishconf.py\" ), and we synchronize the output with the remote server's destination directory defined by the \"dest_path\" variable and the \"DEPLOY_PATH\" environment variable. The synchronization is performed using project.rsync_project , which is a wrapper for the rsync command, allowing to upload newly modified files only. Basically, everything is there already for you to update a remote server with new content with a single command executed locally. And honestly that might be just enough for your needs. But it implies that the remote server already exists and is properly set up. And we'd rather see the new content being pulled from our Git repository instead. Now, you could just stop here for today. I won't be mad, promised. But if you are interested in seeing how to update the fabfile to both provision our server and publish our versionned content, stick with me. Provision Still there? Good. In our context, what's behind the word \"provisioning\" is the act of installing all the required software, packages, dependencies, etc for a project to work on a server. But before that, we are going to create the different error pages. By default, when trying to access a page that doesn't exist for example, the default HTTP server's 404 page will be displayed. And it's ugly. And even if we all agree that the true beauty comes from the inside, we don't want it to be ugly. Under \"content/pages/\" , create a new folder named \"errors/\" and the files \"403.md\" , \"404.md\" and \"50x.md\" in it (adapt the extensions to the format you chose). Here is the content of my 404 page as an example: Title: Hmm... Slug: 404 Status: hidden Nope. Don't know what you're talking about, pal. [Go home](/ \"Back to home\"). Not much new here, except for the \"hidden\" status, whose effect is to prevent it from being displayed along with the other pages ( \"about\" etc). Follow the same format for the two other pages. When you are done, create a new file called \"blog.conf\" under the \".provision/\" directory, with this content: server { listen 80; ## listen for ipv4; this line is default and implied listen [::]:80 default ipv6only=on; ## listen for ipv6 # Make site accessible from http://my-blog.local.com server_name my-blog.local.com; root /var/www/blog; location = / { # Instead of handling the index, just # rewrite / to /index.html rewrite &#94; /index.html; } location / { try_files $uri.htm $uri.html $uri =404; } access_log /var/log/blog/access.log; error_log /var/log/blog/error.log; # Redirect server error pages error_page 500 502 503 504 /pages/50x.html; error_page 404 /pages/404.html; error_page 403 /pages/403.html; } You may have recognized the nginx format: it is indeed the HTTP server I am going to use (feel free to adapt this to your favorite one). This is a rather basic config: the website will be accessible at http://my-blog.local.com , its root will be \"/var/www/blog/\" on the remote server, the logs will be written under \"/var/log/blog/\" , and the errors will be redirected to the different pages you have just created. Now, open back \"fabfile.py\" and, before the \"publish\" function, add a \"provision\" one (it could be after as well, but the provisioning is supposed to come before the publication, right? But maybe that is just my OCD speaking): @hosts(production) def provision(): if run('nginx -v', warn_only=True).failed: sudo('apt-get -y install nginx') sudo('rm /etc/nginx/sites-available/default') sudo('service nginx start') put('./.provision/blog.conf', '/etc/nginx/sites-available/blog.conf', use_sudo=True) sudo('rm -f /etc/nginx/sites-enabled/blog.conf') sudo('ln -s /etc/nginx/sites-available/blog.conf /etc/nginx/sites-enabled/blog.conf') if run('test -d %s/%s' % (log_path, sitename), warn_only=True).failed: sudo('mkdir %s/%s' % (log_path, sitename)) if run('test -d %s' % root_path, warn_only=True).failed: sudo('mkdir %s' % root_path) if run('git -v', warn_only=True).failed: sudo('apt-get install -y git-core') if run('pip --version', warn_only=True).failed: run('wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py -P /tmp/') sudo('python /tmp/get-pip.py') run('rm /tmp/get-pip.py') if run('fab --version', warn_only=True).failed: sudo('pip install Fabric') if run('virtualenv --version', warn_only=True).failed: sudo('pip install virtualenv') sudo('pip install virtualenvwrapper') run('echo \"export WORKON_HOME=$HOME/.virtualenvs\" >> /home/vagrant/.bashrc') run('echo \"source /usr/local/bin/virtualenvwrapper.sh\" >> /home/vagrant/.bashrc') with prefix('WORKON_HOME=$HOME/.virtualenvs'): with prefix('source /usr/local/bin/virtualenvwrapper.sh'): run('mkvirtualenv %s' % sitename) sudo('service nginx restart') Quite a few things here but, if you look closely, you will realize these are basically all the steps you have taken at the beginning of the tutorial. The pattern is almost always the same: test if the package is installed ( if run('package --version', warn_only=True).failed: ) install it if it is not The \"warn_only=True\" parameter allows Fabric not to exit in case of a command failure: this is exactly what we want, i.e. knowing if the command fails so we can install the missing package. \"with prefix\" allows to execute the subsequent commands in the context of the current one. If your version of Python is prior to 2.6, you will need to add from __future__ import with_statement at the top of the file. We will ssh the \"server\" box as the \"vagrant\" user, meaning the \"run\" commands will be executed with its permissions, and the \"sudo\" ones with the root permissions, just like we did throughout this tutorial. Back to the actual script: we first install nginx if necessary (and remove the default config), start it, copy the server config file we created earlier to the right location, and recreate the symlink. Then we ensure the destination directories for the logs and the blog's generated HTML exist, that Git is installed, then pip, Fabric, virtualenv and virtualenvwrapper, we create the \"blog\" virtual environment and, finally, we restart nginx. Again, if you followed this tutorial from the start, this should feel familiar. Now as I am using Ubuntu 14.04.1 LTS boxes, I know my \"remote\" server comes with Python preinstalled. If yours doesn't, well you will have to add the steps to install it :) Shall we test this now? Sure thing, but we need to do something first. Remember the configuration section of \"fabfile.py\" mentioned earlier? It is time to set up the details of our remote server. This is where the use of Vagrant comes in handy as, if you used the Vagrant config I gave you at the beginning of this post, then the \"remote server\" box is already there, reporting for duty. Open a new terminal on your host machine, go to the blog's root, and type this: vagrant up server Now update \"fabfile.py\" , changing the server configuration for this one: # Remote server configuration production = 'vagrant@192.168.72.3:22' env.key_filename = '/home/vagrant/.ssh/insecure_private_key' root_path = '/var/www' log_path = '/var/log' dest_path = '~/dev' sitename = 'blog' symlink_folder = 'output' The IP address is the one that was specified in the Vagrant config, and the private SSH key is the Vagrant one, copied over from the host machine, via this line of the config: # Copy the default Vagrant ssh private key over local.vm.provision \"file\", source: \"~/.vagrant.d/insecure_private_key\", destination: \"~/.ssh/insecure_private_key\" As mentioned earlier, we will ssh the box as the \"vagrant\" user. The rest of the variables are path/folder names used in both the \"provision\" and \"publish\" functions (next section). Now, from the \"local\" Vagrant machine: fab provision If everything is set up properly, you should see a series of text lines scrolling off the screen: your \"server\" box is being provisionned :) When it is done, you can ssh your \"server\" VM and play around to observe that everything was properly installed: vagrant ssh server Publish We are now able to provision a server with everything required to run our blog, and all it takes is to update a few lines of configuration in \"fabfile.py\" and running one command. Pretty cool, eh? Anyway, we are yet to update the \"publish\" function to automate the publication of new content, pulling it from our Git repository. Here is what it looks like: @hosts(production) def publish(): if run('cat ~/.ssh/id_rsa.pub', warn_only=True).failed: run('ssh-keygen -N \"\" -f ~/.ssh/id_rsa') key = run('cat ~/.ssh/id_rsa.pub') prompt(\"Add this key to your Git repository and then hit return:\\n\\n%s\\n\\n\" % key) if run('test -d %s' % dest_path, warn_only=True).failed: run('mkdir %s' % dest_path) with cd(dest_path): if run('test -d %s' % sitename, warn_only=True).failed: run('mkdir %s' % sitename) with cd(sitename): run('git clone %s .' % git_repository) if run('test -d %s' % symlink_folder, warn_only=True).failed: run('mkdir %s' % symlink_folder) sudo('ln -s %s/%s/%s %s/%s' % (dest_path, sitename, symlink_folder, root_path, sitename)) with cd(sitename): run('git reset --hard HEAD') run('git pull origin master') with prefix('WORKON_HOME=$HOME/.virtualenvs'): with prefix('source /usr/local/bin/virtualenvwrapper.sh'): run('workon %s' % sitename) run('pip install -r requirements.txt') run('fab preview') First, we check if there is an existing SSH private key for the \"vagrant\" user (or whatever user you set up): we are going to pull the content from a Git repository over SSH, so we need one. If none is found, the script will generate one for us and display it so we can add it to our repo. Once this is done, just hit return to continue the execution. The destination path is then created if necessary: in our case, the repository is cloned and updated in \"~/dev/blog\" . From there, a symlink is created between \"~/dev/blog/output\" and \"/var/www/blog\" , so the generated HTML files alone are in \"/var/www/blog\" . Finally, the Virtual Environment is activated, the pip dependencies installed, and the content generated with the live config. Let's test our new function: fab publish Once the execution is over, you should be able to see your blog at the private IP address 192.168.72.3 . There is one last little step to take to access it from the server name as defined in the nginx config. Open the \"hosts\" file of your host machine and add the following line (change the domain for the one you chose, if different): 192.168.72.3 my-blog.local.com You can now access http://my-blog.local.com . That's it! You now know how to provision a server and publish your content using Fabric. In the next part , we will review a complete workflow, implement a few extra things and conclude this tutorial with a few openings on what to do to go further.","tags":"Blog","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-3-fabric"},{"title":"Install and deploy a Pelican blog using Fabric - Part 2: installation and configuration","text":"In part 1 , we set up a local environment containing everything Pelican requires to run properly. Let's move on to the installation and configuration of Pelican itself. Summary Part 1: local environment Part 2: installation and configuration Pelican installation Writing content Theme Configuration URLs Plugins Comments Part 3: Fabric Part 4: workflow, extras and conclusion Pelican installation If you are using The Vagrant config provided in part 1, start the \"local\" box and ssh it if it is not already the case: vagrant up local vagrant ssh local Then, activate the Virtual Environment: workon blog Go to the directory you want your blog to reside in (if you are using Vagrant, this is \"/vagrant\" ) and type: pip install pelican Pelican allows to use either reStructuredText or Markdown formats for your articles. I personally use Markdown, but the choice is up to you. You can also use plain HTML if that's your thing. Install your weapon of choice: pip install Markdown Now is a good time to save our current list of dependencies. Type this: pip freeze This command will give you the list of the packages that are installed in your VE. It should more or less look like this: blinker==1.3 docutils==0.12 feedgenerator==1.7 Jinja2==2.7.3 Markdown==2.5.2 MarkupSafe==0.23 pelican==3.5.0 Pygments==2.0.2 python-dateutil==2.4.0 pytz==2014.10 six==1.9.0 Unidecode==0.4.17 pip allows you to save this list into a file, in order to quickly reinstall its content if you need to (on another machine, for example): pip freeze > requirements.txt All you need to do to reinstall this environment somewhere else is: pip install -r requirements.txt Handy. Now let's set up the skeleton for your blog using the built-in wizard: pelican-quickstart Pelican will now ask you a series of questions. Some of them have a value between square brackets at the end: this is the default value you can select simply hitting return . I will only list the questions that might be a bit confusing here: Do you want to specify a URL prefix? e.g., http://example.com (Y/n) Only if you already have a domain name that will point to your blog. You will be able to update this later directly in the publish config file. Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) Do you want an auto-reload & simpleHTTP script to assist with theme and site development? (Y/n) Answer \"Yes\" to both, these are functionalities we are going to use. Then say \"No\" to all the different means to upload your blog. You might end up wanting to use one of the listed methods, but they are not covered in this article. Done. Your new project is available at /vagrant Sweet! Now let's have a glance at the default look of the blog. Type this: fab build Then: fab serve You've just launched a local webserver, that uses the port 8000 by default. Open your browser and navigate to http://localhost:8000 : the default skeleton and template should display (this also works with the Vagrant box because we activated the port forwarding option, cf the Vagrantfile in the Vagrant way section ). That was easy, wasn't it? Interrupt the server and regain control of your terminal typing ctrl + c . fab build and fab serve are Fabric commands. The first one generates the HTML content (more on this in the next section) and the second one creates the server. You can also use the shortcut command fab reserve that runs both ones in turn. Fabric is not the only way to generate content, spawn a HTTP server etc. You can read more about that in the online documentation . Here I choose to use Fabric simply because this is also what we are going to use for pubication later on. Better get familiar with it right now. I will give more details about it in part 3. Writing content How about actually writing something now? Create a file in the \"content\" folder, something like \"my-first-post.md\" (put the appropriate extension if you didn't go for Markdown). Add some content in it, following this format at the beginning: Author: osteel Title: My first post! Date: 2015-02-22 Slug: my-first-post Category: test ##A subtitle Some **Markdown** content, with some *formatting*. A list: - Milk - Butter - Eggs Etc. Pelican will analyze the first lines to properly generate the post. Rebuild and serve your blog with fab reserve , and reload http://localhost:8000 : you should see your post. The generated HTML files are put into the \"output\" folder. A \"test\" category was automatically created and placed in the header (if you had specified none, it would have created a category called \"misc\" by default). This is basically how to write articles. Pelican also allows to create static pages that are not posts (typically, the \"about\" or \"contact\" pages). Simply add a \"pages\" folder under \"content\" , and edit a \"about.md\" file: Title: About Slug: about Amazing blog. Regenerate the content and refresh the web page: a new \"about\" entry has been placed in the header. These are the very basics of writing content. You probably wonder how to tweak the template to your taste now. Don't worry, we are getting there. But first, let's pick a theme! Theme The default theme is nice, but chances are you will want to change it. Pelican comes with a variety of themes to choose among the official repository ones or custom ones made by various people. I am not going to invent much here and will mostly follow the instructions available on the repository's page. First, clone all the themes in a local directory (if you are using the Vagrant box, you will probably want to install Git now - sudo apt-get install -y git-core ): git clone --recursive https://github.com/getpelican/pelican-themes ~/pelican-themes Then, open the \"pelicanconf.py\" file and add these lines at the end (change \"vagrant\" for the correct username if necessary): # Theme THEME = \"/home/vagrant/pelican-themes/mnmlist\" Rebuild and serve: fab reserve Reload http://localhost:8000 : you are now looking at the \"mnmlist\" theme! Test as many themes as you like until you find one that suits you. I personally went for the Octopress one, ported from Octopress by Maurizio Sambati. Once you picked one, copy its content in a new \"theme\" folder in your blog's directory: mkdir theme cp -rf ~/pelican-themes/mnmlist/* theme/ Edit \"pelicanconf.py\" again and change the value of \"THEME\" for the new location: THEME = \"theme\" Rebuild and refresh to make sure it worked. You can now remove the other themes: rm -rf ~/pelican-themes Most of the themes have their own settings. Just have look at the theme's own README file to know what they are (here is the \"mnmlist\" one, for example). Configuration We had a quick preview of the configuration in the previous section, when we changed the theme's path. Pelican actually has two configuration files: \"pelicanconf.py\" , with whom we made acquaintance already, and \"publishconf.py\" , which contains production-wise settings. The latter should contain settings that are relevant to your live environment only; we will see examples later on. Your \"pelicanconf.py\" file should currently look like something like that: #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals AUTHOR = u'osteel' SITENAME = u'My Blog' SITEURL = '' PATH = 'content' TIMEZONE = 'Europe/Paris' DEFAULT_LANG = u'en' # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None # Blogroll LINKS = (('Pelican', 'http://getpelican.com/'), ('Python.org', 'http://python.org/'), ('Jinja2', 'http://jinja.pocoo.org/'), ('You can modify those links in your config file', '#'),) # Social widget SOCIAL = (('You can add links in your config file', '#'), ('Another social link', '#'),) DEFAULT_PAGINATION = 10 # Uncomment following line if you want document-relative URLs when developing #RELATIVE_URLS = True # Theme THEME = \"theme\" Most of the parameters are pretty straightforward, and I invite you to have a look at the documentation that describes them all. I am going to focus on specific areas: URLs, plugins and comments. URLs For now, our posts' URLs look like so: http://sitename.com/article-slug.html We are going to change this to adopt a format like the one I am using for my blog: http://sitename.com/year/month/day/article-slug.html Obtaining this result is pretty easy. Open the \"pelicanconf.py\" file and add these lines: # URLs ARTICLE_URL = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' ARTICLE_SAVE_AS = 'posts/{date:%Y}/{date:%m}/{date:%d}/{slug}.html' Now, let's move the post we created earlier in a folder reflecting this structure. Go to the \"content\" folder and create subfolders as following (change the date if you want to): cd content mkdir 2015 2015/02 2015/02/22 Then move the post: mv my-first-post.md 2015/02/22 Rebuild and refresh: URLs are now a bit more structured. Of course, this is just an example, and you are free to adopt any format you want. Plugins Pelican supports plugins since its version 3.0, allowing to add functionalities without having to touch the core. Plugins are maintained in a separate repository just like the themes. You will find a description of each one of them there, and we are going to manage them the same way we did for the themes. git clone them all locally, and copy over those you want in a new \"plugins\" folder in your blog's directory. If you want to use the Liquid Tags plugin for example, this is how you would do: git clone https://github.com/getpelican/pelican-plugins ~/pelican-plugins mkdir plugins cp -rf ~/pelican-plugins/liquid_tags plugins/ Open \"pelicanconf.py\" and add these lines at the end: # Plugins PLUGIN_PATHS = ['plugins'] PLUGINS = ['liquid_tags'] Pelican now knows where to find the plugins and which ones to load. Rebuild and refresh. Copy as many plugins as you like, update the config file accordingly and, when you are done, delete the other ones: rm -rf ~/pelican-plugins Comments Pelican natively supports Disqus , a third party service that will take care of your commenting system for free, externally (nothing to host). Head to the website and create an account. Set it up as you like and note your sitename, which is just the string before \".disqus\" in the URL of your account. For example, mine is \"https://osteel.disqus.com\" , so my sitename is \"osteel\" . Now edit \"pelicanconf.py\" and add the following line (with the right sitename, of course): DISQUS_SITENAME = \"osteel\" Build, serve and refresh: you should have a nice comment box at the bottom of your article. Note: We added this config parameter to \"pelicanconf.py\" so you could see the result straight away, but this actually only relevant in the context of your live environment (Disqus won't recognize your local URL). This is typically one of the settings that should be in the other config file, \"publishconf.py\" . That's it for today. In part 3 , we will see how to use Fabric to automate the provisioning of a server and the publication of new content.","tags":"Blog","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-2-installation-and-configuration"},{"title":"Install and deploy a Pelican blog using Fabric - Part 1: local environment","text":"This series of articles will walk you through the different steps to install, setup and deploy your first Pelican blog (just like this one). If its aim is to be accessible to most people, there are a couple of pre-requisites that would definitely facilitate your progression: Being comfortable with your OS's CLI Being comfortable with Git A developer background is preferable overall, even though there is no need to be familiar with Python. I knew nothing about this language when I started using Pelican. If you are not technical at all however, this is probably not for you. Pelican is a static website generator, meaning that it does not require any server logic (typically, a database), making it lightweight and easy to host. The script spits out HTML files which are then served to the client. Wanna play straight away? Head to the GitHub repository now and follow the instructions. If you are stuck at any point during this tutorial, don't hesitate to refer to it as well. In this first part, we are just going to set up our local environment to be able to install Pelican later on. Summary Part 1: local environment The Vagrant way Python, pip and virtualenv Python pip virtualenv Part 2: installation and configuration Part 3: Fabric Part 4: workflow, extras and conclusion The Vagrant way I am quite a fan of Vagrant , and I would advise to use an empty Vagrant box to play around safely. If you are not familiar with Vagrant and wish to learn more about it, take a look at this tutorial . This is completely optional tho, especially if you are on Mac OS. For Windows users however, this is strongly recommended (there is a dedicated tutorial here about how to use Vagrant on Windows), even though the team behind Pelican seems to have ensured the compatibility for this platform. Specific steps for Windows are not covered in this series of articles. The following tutorial was made using Ubuntu 14.04.1 LTS boxes, and the part that will cover the deployment with Fabric relies on one of Vagrant's features ( \"Multi-Machine\" ). If you choose not to use Vagrant, you will still learn how to deploy your blog using Fabric, but you won't be able to test it right away. If you choose to take the Vagrant way however, use this Vagrantfile: # -*- mode: ruby -*- # vi: set ft=ruby : # Vagrantfile API/syntax version. Don't touch unless you know what you're doing! VAGRANTFILE_API_VERSION = \"2\" Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| # Used box config.vm.box = \"ubuntu/trusty32\" config.vm.define \"local\" do |local| # Accessing \"localhost:8000\" will access port 8000 on the guest machine local.vm.network :forwarded_port, guest: 8000, host: 8000, auto_correct: true # Copy the default Vagrant ssh private key over local.vm.provision \"file\", source: \"~/.vagrant.d/insecure_private_key\", destination: \"~/.ssh/insecure_private_key\" end config.vm.define \"server\" do |server| # Private IP server.vm.network :private_network, ip: \"192.168.72.3\" end end Don't worry too much if you don't understand everything at first, I will explain a bit more in due time. Just know that this config will allow you to run two different machines: one will be your local one (conveniently named \"local\" ) and the other one (named \"server\" ) will be used to simulate a remote server. For now, begin your journey with: vagrant up local vagrant ssh local Unless stated otherwise, all the steps covered in this part and the next ones need to happen on the \"local\" machine. Python, pip and virtualenv Python Pelican is written in Python and is installed via pip , the language's recommended tool to install packages (it is basically what npm is to JavaScript). Pelican works with Python 2.7.x, and chances are it is already available on your OS. To make sure of this, open a terminal and type: python --version If the command fails, please head to the installation guide and follow instructions for your platform. Pelican should just work with Python 3.3+ as well, but as our intent is to use Fabric for deployment, we will stick to 2.7.x in this tutorial. pip If the version of Python you have is 2.7.9+, you are in luck because pip is already included. If not, install it this way: wget https://bootstrap.pypa.io/get-pip.py -P /tmp/ sudo python /tmp/get-pip.py rm /tmp/get-pip.py The -P option allows to prefix the downloaded file with the destination folder ( \"/tmp/\" in our case). Let's take this opportunity of having pip handy to install Fabric straight away: sudo pip install Fabric We are going to use it soon enough. We could now install Pelican right away using pip, but we are going to take an extra step before that. virtualenv If you are a developer, you may have come across situations where different projects need different versions of the same language to run, or different versions of other dependencies. Python is no exception to this, and it came up with a solution to address it: the use of Virtual Environments. A Virtual Environment (VE) isolates a set of dependencies for a project. We are going to use virtualenv along with its buddy virtualenvwrapper to that purpose. First, we need to install them using pip: sudo pip install virtualenv sudo pip install virtualenvwrapper By default, virtualenv will create a folder named after the environment's directly inside the project's folder, and will do so for each project. virtualenvwrapper adds a set of functionalities on top of virtualenv to make its use easier, and placing all the VEs in the same folder is one of them (much cleaner IMHO and prevents us from having to add the folders in each project's .gitignore file). Let's define where virtualenwrapper should gather all the folders: echo 'export WORKON_HOME=$HOME/.virtualenvs' >> /home/vagrant/.bashrc echo 'source /usr/local/bin/virtualenvwrapper.sh' >> /home/vagrant/.bashrc Remember that I am using a Vagrant box here, hence \"vagrant\" as the username. Change this for the right value if necessary. I place these commands at the end of my .bashrc file so that WORKON_HOME is correctly initialized any time I start a new shell. Let's source our .bashrc file so the shell is updated with the new values: source ~/.bashrc It is now time to create our first VE. As we are setting up a blog, let's call it \"blog\" : mkvirtualenv blog Your prompt should now look like this: (blog)vagrant@vagrant-ubuntu-trusty-32:/vagrant$ Mind the \"(blog)\" bit at the beginning: it indicates that you are currently using the blog VE. mkvirtualenv not only creates it, but activates it as well. This is how you leave it: deactivate Now let's check that the VE's folder was added to the right location: ls $WORKON_HOME ls ~/.virtualenvs Both commands should give the same list, and the \"blog\" folder should be among them (only if you set \".virtualenvs\" as the destination folder earlier, of course). To activate your VE, use the following command: workon blog Here you are, ready to move on to the installation of Pelican itself. Note that all the steps above is how you would prepare an environment for a Python project in general. Just sayin' :) In the next part , we will see how to install and configure Pelican in our shiny new environment.","tags":"Blog","url":"https://tech.osteel.me/posts/install-and-deploy-a-pelican-blog-using-fabric-part-1-local-environment"},{"title":"How to use Vagrant for local web development","text":"[UPDATE 2020/03/05]: While Vagrant served me well for years, I do not recommend using it for local development anymore and advise to use Docker instead, for which I wrote an entire tutorial series which I invite you to read. This article shows how to quickly get up and running with Vagrant, to create and use local Virtual Machines as development environments, all with a single command. This is indeed written from a web developer's standing point, and I will not spend too much time describing how things work under the hood (not that I am an expert anyway). The point of Vagrant is precisely not to have to worry too much about it. The final result of this tutorial is available as a Github repository . In case of trouble, don't hesitate to refer to it. Summary Vagrant? Basic installation Picking a box Vagrantfile Port-forwarding Private IP Provisioning Shell scripts Server config Matching the private IP Tips .gitignore Handle port collisions Copy host git config GUI console Access the host machine when using a private network What now? Provisioning tools Vagrant Share Vagrant? Vagrant greatly simplifies the use of Virtual Machines to spawn development environments in no time (well, it's probably more like no effort than time). To understand what a Virtual Machine (VM) is, think of an emulator: you install it on your computer so you can then run software that believe they are running in the environment they were designed for. All inside your own machine (which is then called the host). That is essentially what a VM is. Vagrant is a VM manager , in the sense that it reduces the management and the configuration of VMs to a handful of commands. It relies on a VM provider , that deals with virtualization itself. As its support is shipped with Vagrant, we will use VirtualBox, but others exist . So what is actually the point? The main argument is the consistency of the environments among developers working on the same project, and more importantly that these environments reflect the production ones. Ship a Vagrant configuration with each project, and every developer will work on the same environment locally. No surprises when pushing the code live, no more \"it works on my machine\" . The other advantages I see is the fact that one can still use their editor of choice, as Vagrant folders are shared with the host by default. It also permits not to clutter up your computer with tons of different libraries and software that aren't used by every project. If something goes wrong, you don't screw your machine: you destroy your VM instance and recreate it instead. Easy. And safe. Basic installation First, download VirtualBox at https://www.virtualbox.org/wiki/Downloads ( \"platform packages\" ) and install it. Then, download Vagrant at https://www.vagrantup.com/downloads.html (v1.7.2 at the time of writing), install. Open up a terminal and type: vagrant version The version should be displayed. If not, log out your session, log back in and try again. Picking a box Vagrant's documentation uses a Ubuntu 12.04 LTS 32-bit server box. Now let's say you want Ubuntu 14.0 LTS 32-bit : go to the catalog of Vagrant boxes and type \"Ubuntu 14.04\" in the search field. Spot the Ubuntu one in the list: \"ubuntu/trusty32\" . Now get back to your terminal, browse to the directory you want your project to reside in, and type this: vagrant init ubuntu/trusty32 The terminal should display something like: A `Vagrantfile` has been placed in this directory. You are now ready to `vagrant up` your first virtual environment! Please read the comments in the Vagrantfile as well as documentation on `vagrantup.com` for more information about using Vagrant. Good. Now type: vagrant up This is how you start your Virtual Machine. As you picked Ubuntu 14.04 , Vagrant will try to install it: as it won't find a corresponding box on your computer, it will fetch it directly from the catalog (this can take a while according to your connection speed), and boot it once the download is over. All you have to do now is: vagrant ssh BOOM! You are now in your Ubuntu 14.04 server. Note for Windows users: the vagrant ssh command might not work for you if SSH is not in your PATH variable. I invite you to take a look at this separate article and come back. What exactly happened here? vagrant init created a Vagrantfile file in the directory. This file contains the various settings Vagrant needs to spawn a VM. You can have a look at it now, it is basically full of commented examples (don't freak out, it is not as bad as it looks). You will find one uncommented line tho: config.vm.box = \"ubuntu/trusty32\" Yeah, you got it: as we invoked vagrant init followed by the box we wanted to use, Vagrant created its config file with the corresponding setting. While you are connected to your box, type this: ls /vagrant The Vagrantfile should be listed. This is the same Vagrantfile you have just updated: /vagrant is the folder that is shared between the host machine and the VM. You can simply leave your box typing ctrl + d . I won't explain how to shutdown it; just take a couple of minutes to read about the different available options in the doc , they are well explained. Just know that even if you destroy your box, files under /vagrant remain untouched. Vagrantfile I am not going to go through all the options here, only those I most often use. The Vagrantfiles are written in Ruby, but no Ruby background is required (I don't know much about Ruby myself). Let's look into accessing your VM from the host. Using your host machine's editor to update files on your VM via shared folders is nice, but at some point you will want to admire your work in a browser, which implies for it to have access to your Vagrant box somehow. The easiest way to achieve this is probably using port-forwarding. Port-forwarding First let's run a quick test. Open a terminal on your host machine and type the following command: telnet localhost 8080 Unless the port 8080 is already in use by something else (in which case change 8080 for whatever port you know is available), you should get something like: Trying 127.0.0.1... telnet: connect to address 127.0.0.1: Connection refused Trying ::1... telnet: connect to address ::1: Connection refused telnet: Unable to connect to remote host Edit your Vagrantfile and add the following: config.vm.network :forwarded_port, guest: 80, host: 8080 Now reload your VM: vagrant reload or vagrant up if you had shut it down. Once it is booted, try the telnet command above again in the other terminal window. This is what you should read: Trying 127.0.0.1... Connected to localhost. Escape character is '&#94;]'. Port 8080 is now being listened to, and connections to it are forwarded to port 80 of the Vagrant box. This comes in handy when you use something like grunt-contrib-connect to quickly spawn a static HTTP server on a specific port. In our case, using port 80 in the Grunt config and accessing http://localhost:8080 from our host machine would display the website contained in the folder set with the base parameter. This is nice for quick prototyping for example, as no further server configuration is required. In most cases however, as we want to replicate a production environment as accurately as possible, we will want to use a proper server config. To do so, we are going to assign a private IP to our box. Private IP First, go back to the terminal and try: ping 192.168.68.8 You should get request timeout responses. Edit the Vagrantfile again and add: config.vm.network :private_network, ip: \"192.168.68.8\" Reload your VM and try again: you should now get responses. This is it for the options I mainly use; I rarely need the rest. Be curious and scan through the official documentation , it is well written and I am sure you will find something useful to you. On a side note, and to emphasize the usefulness of Vagrantfiles, I think their beauty resides in the fact that all it takes to make the environment available to other developers is to version them with your projects. Whenever you clone a project containing a Vagrantfile to a machine with Vagrant installed on it, you are at a vagrant up away from having it running locally. The rest of the process to get your website displayed using the private IP is covered in the next section. Provisioning Alright, so you've got your VM running, you can edit files from outside of it, and access it from the host machine using its private IP. How to properly display your work in a browser now? Let's cut to the chase here: it implies setting a server on your VM and matching its IP to the domain name you chose in the host machine's hosts file. As our Vagrant box is almost empty at this point, we need to install a HTTP server on it. We will go for Nginx here, but instead of installing it and setting up the server from the VM itself, we are going to use provisioning. Shell scripts Provisioning is achieved from the Vagrantfile as well and, if different means are available to do so, I will only cover the most basic one for now, i.e. using shell scripts (I will mention the other ways later on). Open the Vagrantfile in your editor, and add this: config.vm.provision :shell, :path => \".provision/bootstrap.sh\" Basically what we tell Vagrant is \"Use the shell script that you will find in .provision/bootstrap.sh to provision the box\" . Don't reload your box just yet, as we need to create this file first. Add a new folder named \".provision\" in your project (same level as the Vagranfile), and create a bootstrap.sh file in it. Here is the full content of this file: #!/usr/bin/env bash # nginx sudo apt-get -y install nginx sudo service nginx start # set up nginx server sudo cp /vagrant/.provision/nginx/nginx.conf /etc/nginx/sites-available/site.conf sudo chmod 644 /etc/nginx/sites-available/site.conf sudo ln -s /etc/nginx/sites-available/site.conf /etc/nginx/sites-enabled/site.conf sudo service nginx restart # clean /var/www sudo rm -Rf /var/www # symlink /var/www => /vagrant ln -s /vagrant /var/www Now, let's describe it step by step: #!/usr/bin/env bash We are basically telling where to look for the bash program. # nginx sudo apt-get -y install nginx sudo service nginx start Install Nginx and start it. The -y option allows to automatically answer \"yes\" where user input is normally required. # set up nginx server sudo cp /vagrant/.provision/nginx/nginx.conf /etc/nginx/sites-available/site.conf sudo chmod 644 /etc/nginx/sites-available/site.conf sudo ln -s /etc/nginx/sites-available/site.conf /etc/nginx/sites-enabled/site.conf sudo service nginx restart We copy the server configuration from .provision/nginx/nginx.conf (yet to be written at this point, we're getting there) to Nginx's sites-available folder, ensure the permissions are right, then create the symlink from sites-enabled/site.conf to sites-available/site.conf , and restart Nginx to take this new config into account. Finally, we create a symbolic link from /var/www to /vagrant , after having removed the default files Nginx creates at installation: # clean /var/www sudo rm -Rf /var/www # symlink /var/www => /vagrant ln -s /vagrant /var/www /var/www is often where the code goes on a server (even though /srv/www/ might be more relevant, but that's another debate ). As mentioned earlier, on a Vagrant VM the shared folder is /vagrant . Using such a symlink allows to match your production server's settings (or staging or whatevs). This is completely optional tho. Server config Almost there! If you are attentive, you know that we now need the Nginx server config. Under the .provision folder, create a new folder named \"nginx\" and open a new nginx.conf file in it. Here is its content: server { listen 80; server_name vagrant-test.local.com; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; root /var/www; location / { } } Nothing too esoteric here, this is a very basic Nginx server config. Just note that our server name will be \"vagrant-test.local.com\" . Save your file, and reload the box using this command: vagrant reload --provision Normally, the provisioning is done only at the first boot of the VM. By specifying the --provision option, we force Vagrant to perform the provisioning again. The start up will take slightly longer than previously, and you should see quite a lot more instructions on the screen: the packages listed in the bootstrap.sh file are being installed. Matching the private IP We only need two little extra steps now: adding the private IP and the server name to the host machine's hosts file and creating a simple index.html file in our box to be displayed in the browser later on. On Mac OS, edit the hosts file with the following command: sudo vim /etc/hosts On Windows, you will find it under Windows/System32/Drivers/etc/ (you will have to edit it in admin mode). Add the following line: 192.168.68.8 vagrant-test.local.com Save and quit. Now add a little index.html file at your project's root: <!DOCTYPE html> <html> <head> <title>Vagrant test</title> </head> <body> <h1>Oh hi!</h1> </body> </html> Alright ladies and gentlemen, drumroll please: open your favorite browser and navigate to http://vagrant-test.local.com . You should see the HTML page you have just created ＼(&#94;o&#94;)／ Phew! That was a rocky ride, eh! Well, yeah. But think about it this way: save this basic config somewhere (craft it to your needs, adding PHP or whatever floats your boat), and use it anytime you start a new project. You can now get up and running with a simple vagrant up command and a new line in the hosts file. I don't know about you, but it sounds quite nice to me. Tips .gitignore If you use Git, add \".vagrant/\" to your .gitignore file. Vagrant creates this folder when booting the VM: it contains auto-generated stuff you don't want to version. Handle port collisions When using port-forwarding, it can happen that a Vagrant box uses a port that is not available. It will tell you about it anyway, but if the host machine's port doesn't really matter to you, amend the corresponding line in the Vagrantfile this way: config.vm.network :forwarded_port, guest: 8080, host: 80, auto_correct: true The auto_correct parameter will allow Vagrant to automatically assign another port of the host machine in case of unavailability (and tell you about it, of course). Copy host git config I usually try to avoid having too many terminal windows open at the same time. That's why I like to install Git on my VMs so I don't need another terminal window only to perform the versioning operations. This implies having your Git config inside the Vagrant box, which is actually quite easy to do automatically. In your Vagrantfile, add this: config.vm.provision \"file\", source: \"~/.gitconfig\", destination: \"~/.gitconfig\" At the first boot of the VM, your Git config file will be copied over. GUI console Occasionaly, you may have some trouble booting your VM. This can notably happen when your box wasn't properly shut down the last time you used it. The boot process might just hang there, and the reason may not be obvious. In that case, add the following lines to your Vagrantfile: config.vm.provider :virtualbox do |vb| vb.gui = true end and try to boot again. What this does is it will open VirtualBox's GUI console, which displays what is happening behind the scenes, and thus should help you troubleshoot the issue. Access the host machine when using a private network Sometimes, you may need to access the host machine from the guest one. How to do so? When you set up a private network, 192.168.68.8 in our case, the host machine automatically takes 192.168.68.1 as its private IP address. What now? Well, that is quite a lot to digest already. And yet this is just the beginning, there are many more features to explore. Just to mention a couple of them tho: Provisioning tools In this tutorial, we used simple shell scripts. But it turns out Vagrant (supposedly) plays nicely with more advanced solutions such as Chef or Puppet . Admitedly tho, I tried to use Puppet something like a year ago, and completely failed to get it working with Vagrant. I am yet to give Chef a try. Vagrant Share Local development is good, but soon you will be confronted with the need to expose your work to the world, whether it is to show some progress to a client or to allow an API to reach your application. This is achievable with nice tools such as ngrok (hopefully I will soon publish a tutorial about it), but Vagrant unveiled a new service a few months ago, called \"Vagrant Share\" , that does just that. I am yet to try it out (a tutorial would probably follow), but it definitely looks promising.","tags":"Vagrant","url":"https://tech.osteel.me/posts/how-to-use-vagrant-for-local-web-development"},{"title":"How to use Vagrant on Windows","text":"[UPDATE 2020/03/05]: While Vagrant served me well for years, I do not recommend using it for local development anymore and advise to use Docker instead, for which I wrote an entire tutorial series which I invite you to read. This article shows how to deal with Windows' specificities while trying to work with Vagrant. If you are not familiar at all with the latter, I suggest you go through this Vagrant tutorial first. Note: the following was tested on Windows 8, but the steps described below shouldn't change too much between the different versions. Vagrant ssh The first issue I came across was that vagrant ssh doesn't work out of the box. I was greeted with a constructive message: `ssh` executable not found in any directories in the %PATH% variable. Is an SSH client installed? Try installing Cygwin, MinGW or Git, all of witch contain an SSH client. Or use your favorite SSH client with the following authentication information shown below: Host: 127.0.0.1 Port: 2222 Username: vagrant Private key: C:/path/to/project/.vagrant/machines/default/virtualbox/private_key Fine. Let's install Git , then (considering it is not already the case). Git install The key is when the \"Adjusting your PATH environment\" screen pops up: You want to select \"Use Git and optional Unix tools from the Windows Command Prompt\" . Now I know the message in red looks quite scary, but honestly, unless you are a hardcore user of the Windows console, there is not much to worry about. Basically it will override some of the commands and add a few others. Personally, it never caused me any trouble. If you are still a bit worried tho, be reassured: none of this is irreversible. All you would need to do is uninstall Git, or update the PATH variable removing the incriminated part. More on that in a minute. Try to vagrant ssh your VM again, this time it should do it (you might need to open a new terminal for the update to take effect, tho). What if Git is installed already? Well, it was the case for me as well. You could always remove it and install it again, but there is another way. You will have to do manually what the installation of Git could have done for you, but fortunately it is quite trivial: Open the Control Panel Go to System and Security Click on System , then on the Change Settings button Display the Advanced tab and click on Environment Variables... Look for the Path variable in the System variables list, select it then Edit... At the end of the string, add the path to Git's bin (something like \"C:\\Program Files\\Git\\bin\" ) (don't forget to add a semicolon first to separate it from the previous path): Validate and close the different menus. Try to vagrant ssh your box again, it should work (again, you might need to open a new terminal first). You probably guessed it already, but if you don't want Git's commands to override the Windows ones anymore, all you need to do is to remove that bit. You will need to find another way for ssh to work though! Ah, but wait. There is another way. PuTTY Remember that error message we initially got trying to ssh the box? Let's have a look at the second part of it: Or use your favorite SSH client with the following authentication information shown below: Host: 127.0.0.1 Port: 2222 Username: vagrant Private key: C:/path/to/project/.vagrant/machines/default/virtualbox/private_key I wasn't joking when I said it was constructive, because it really tells you what to do. The Windows console works ok but let's be honest, in the long run it is a real pain to use. It does the trick for a quick vagrant ssh but when the time comes to actually do some work on an Ubuntu server for example, a better shell is desirable. Enter PuTTY PuTTY is a very lightweight tool that allows to do a lot of cool stuff. Some of you are probably familiar with it already, and using it jointly with Vagrant is quite nice. We will use it to ssh our boxes, and rely on the info given by the message above to that purpose. First, download it if that is not the case already (the first putty.exe link will do). Download puttygen.exe as well, we are going to need it. PuTTY and PuTTYGen are stand-alone applications (no need to install them), so just double click on the .exe files. Let's open PuTTYGen first: PuTTY uses its own key format, and we need to convert Vagrant's one first. Click on File then Load private key , select the file indicated by the error message earlier (e.g. \"C:/path/to/project/.vagrant/machines/default/virtualbox/private_key\" ). Once selected, PuTTY is kind enough to tell us what to do with it: Ensure SSH-2 RSA is selected, and that the number in Number of bits in a generated key is 2048. Then click on Save private key (don't set a passphrase) and save it under your own user directory's .ssh folder, as \"vagrant_private_key\" . From now on, we will use this key for all the Vagrant boxes. Close PuTTYGen and open PuTTY. In the Hostname field, type 127.0.0.1 . In the Port one, 2222 . Ensure SSH is selected and, in the Saved Sessions field, type vagrant and click Save : Go to Connection then Data , and in the Auto-login username field, enter \"vagrant\" : Next, still under Connection , go to SSH then Auth . Browse for the key you generated earlier in the Private key file for authentication field. Now head back to the Session menu, save again the \"vagrant\" one. Now click on Open : if everything went alright, you should now be in your Vagrant box ＼(&#94;o&#94;)／ Using multiple Vagrant boxes simultaneously Now let's say you already have a box running, and you need to start a second one. You vagrant up it, the Virtual Machine boots and you want to SSH it. But all the boxes cannot use the same SSH port! It all happens when the box is being booted: See the highlighted line? Seeing port 2222 was busy already, Vagrant picked the port 2200 instead. Now to SSH it using PuTTY, open it, load the \"vagrant\" session, and in the Port field, replace \"2222\" with \"2200\" . Click Open : there you are, connected to the second box. Known limitations Shared folders and symlinks One of the fairly known limitations of using Vagrant on Windows with VirtualBox is that the latter won't let you create symlinks on the shared folders for security reasons. This quickly becomes problematic when dealing with npm packages, for example. One of the workarounds is to use the \"no bin link\" parameter (e.g. npm install --no-bin-link ), but this is not always enough. Fortunately, there is a way to bypass this restriction. In your Vagrantfile, add the following piece of config: config.vm.provider \"virtualbox\" do |v| v.customize [\"setextradata\", :id, \"VBoxInternal2/SharedFoldersEnableSymlinksCreate/v-root\", \"1\"] end As Windows won't let standard users create symlinks, you now need to start your Vagrant box in administrator mode (open a Windows terminal in admin mode before running vagrant up , for example). Make sure no other box is already running though, as it won't start if VirtualBox is already running in standard mode. Maximum path length Another recurring problem comes from the fact that Windows' maximum length for a path is 255 characters. Again, this is quickly an issue when dealing with npm packages, especially when they have dependencies, themselves having dependencies, etc. The solution in that case is to create a symbolic link between the \"node_modules\" directory and another directory outside of the shared folders. Which brings us to our practical example. Practical example: npm packages So you have this project relying on npm packages. You tried to install them using --no-bin-link but no luck, looks like some of the paths are too long. Fear not, Macless: update your Vagrant config as shown above to allow the creation of symlinks, boot your VM in admin mode, create a destination directory for your npm packages somewhere outside of the shared folder and create a symlink between it and the \"node_modules\" one: mkdir ~/node_modules ln -s /home/vagrant/node_modules /vagrant/node_modules cd /vagrant npm install Et voilà. Note: this implies preventing the \"node_modules\" directory from being versionned. Conclusion Here you go, now using Vagrant on Windows in decent conditions. The process can look a bit convoluted, and really it is. It took me quite a while to put everything together, and if today I am rather satisfied, I am still a bit bugged about the multiple Vagrant boxes part. Having to check the SSH port and update it in the PuTTY session everytime is a bit annoying, even though dealing with several instances at the same time might be an edge case. Anyways, if you have any suggestions about that, don't hesitate to leave a comment.","tags":"Vagrant","url":"https://tech.osteel.me/posts/how-to-use-vagrant-on-windows"},{"title":"I've got a blog","text":"Yeah, finally. It should have been started 8 years ago but well, better late than never, eh? Let's say that's my very own Duke Nukem Forever. So what will be in there? Posts about dev, surely. There should be one about launching a blog like this one very soon. A blog using Pelican , that is. For those of you who happen to speak French and want to read about a random dude's life, go check out the other part of this blog . Enjoy the ride!","tags":"Misc","url":"https://tech.osteel.me/posts/ive-got-a-blog"}]}